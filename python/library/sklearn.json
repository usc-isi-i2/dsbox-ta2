{
  "search_primitives": [
    {
      "common_name": "sklearn.metrics.scorer.get_scorer",
      "description": "None",
      "id": "sklearn.metrics.scorer.get_scorer",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.scorer.get_scorer",
      "parameters": [],
      "tags": [
        "metrics",
        "scorer"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "multiprocessing.forking.assert_spawning",
      "description": "None",
      "id": "multiprocessing.forking.assert_spawning",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "multiprocessing.forking.assert_spawning",
      "parameters": [],
      "tags": [
        "forking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.byte2int",
      "description": "None",
      "id": "sklearn.externals.six.byte2int",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.byte2int",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.create_bound_method",
      "description": "None",
      "id": "sklearn.externals.six.create_bound_method",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.create_bound_method",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.fixes.safe_copy",
      "description": "None",
      "id": "sklearn.utils.fixes.safe_copy",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.fixes.safe_copy",
      "parameters": [],
      "tags": [
        "utils",
        "fixes"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.iterbytes",
      "description": "None",
      "id": "sklearn.externals.six.iterbytes",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.iterbytes",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.csr.isspmatrix_csr",
      "description": "None",
      "id": "scipy.sparse.csr.isspmatrix_csr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.csr.isspmatrix_csr",
      "parameters": [],
      "tags": [
        "sparse",
        "csr"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.base.isspmatrix",
      "description": "None",
      "id": "scipy.sparse.base.isspmatrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.base.isspmatrix",
      "parameters": [],
      "tags": [
        "sparse",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.fixes.sparse_min_max",
      "description": "None",
      "id": "sklearn.utils.fixes.sparse_min_max",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.fixes.sparse_min_max",
      "parameters": [],
      "tags": [
        "utils",
        "fixes"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.indexbytes",
      "description": "None",
      "id": "sklearn.externals.six.indexbytes",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.indexbytes",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.nmf.safe_vstack",
      "description": "None",
      "id": "sklearn.decomposition.nmf.safe_vstack",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.nmf.safe_vstack",
      "parameters": [],
      "tags": [
        "decomposition",
        "nmf"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.sputils.isdense",
      "description": "None",
      "id": "scipy.sparse.sputils.isdense",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.sputils.isdense",
      "parameters": [],
      "tags": [
        "sparse",
        "sputils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.mldata.setup_module",
      "description": "None",
      "id": "sklearn.datasets.mldata.setup_module",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.mldata.setup_module",
      "parameters": [],
      "tags": [
        "datasets",
        "mldata"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.arpack.get_inv_matvec",
      "description": "None",
      "id": "sklearn.utils.arpack.get_inv_matvec",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.arpack.get_inv_matvec",
      "parameters": [],
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.mldata.teardown_module",
      "description": "None",
      "id": "sklearn.datasets.mldata.teardown_module",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.mldata.teardown_module",
      "parameters": [],
      "tags": [
        "datasets",
        "mldata"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.funcsigs.formatannotation",
      "description": "None",
      "id": "sklearn.externals.funcsigs.formatannotation",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.funcsigs.formatannotation",
      "parameters": [],
      "tags": [
        "externals",
        "funcsigs"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Base Shuffle Split",
      "description": "'Base class for ShuffleSplit and StratifiedShuffleSplit'",
      "id": "sklearn.cross_validation.BaseShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.BaseShuffleSplit",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:767",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "metrics.base",
      "common_name": "Undefined Metric Warning",
      "description": "None",
      "id": "sklearn.metrics.base.UndefinedMetricWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.metrics.base.UndefinedMetricWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/metrics/base.pyc:28",
      "tags": [
        "metrics",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.validation",
      "common_name": "Data Conversion Warning",
      "description": "None",
      "id": "sklearn.utils.validation.DataConversionWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.validation.DataConversionWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc:27",
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib_error",
      "description": "'Lazy loading of moved objects in six.moves.urllib_error'",
      "id": "sklearn.externals.six.Module_six_moves_urllib_error",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib_error",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:230",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib_robotparser",
      "description": "'Lazy loading of moved objects in six.moves.urllib_robotparser'",
      "id": "sklearn.externals.six.Module_six_moves_urllib_robotparser",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib_robotparser",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:311",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Iterator",
      "description": "None",
      "id": "sklearn.externals.six.Iterator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.six.Iterator.next",
          "name": "next",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.six.Iterator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:411",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib_response",
      "description": "'Lazy loading of moved objects in six.moves.urllib_response'",
      "id": "sklearn.externals.six.Module_six_moves_urllib_response",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib_response",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:293",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "tree.export",
      "common_name": "Sentinel",
      "description": "None",
      "id": "sklearn.tree.export.Sentinel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree.export.Sentinel",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/export.pyc:64",
      "tags": [
        "tree",
        "export"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Moved Attribute",
      "description": "None",
      "id": "sklearn.externals.six.MovedAttribute",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.MovedAttribute",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:108",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Changed Behavior Warning",
      "description": "None",
      "id": "sklearn.base.ChangedBehaviorWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.base.ChangedBehaviorWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:20",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils",
      "common_name": "Convergence Warning",
      "description": "None",
      "id": "sklearn.utils.ConvergenceWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.ConvergenceWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/__init__.pyc:25",
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.validation",
      "common_name": "Non BLAS Dot Warning",
      "description": "None",
      "id": "sklearn.utils.validation.NonBLASDotWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.validation.NonBLASDotWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc:33",
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.validation",
      "common_name": "Not Fitted Error",
      "description": "None",
      "id": "sklearn.utils.validation.NotFittedError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.validation.NotFittedError",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc:39",
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Moved Module",
      "description": "None",
      "id": "sklearn.externals.six.MovedModule",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.MovedModule",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:93",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.arpack",
      "common_name": "Arpack Error",
      "description": "'\nARPACK error\n'",
      "id": "sklearn.utils.arpack.ArpackError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.arpack.ArpackError",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:312",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib_parse",
      "description": "'Lazy loading of moved objects in six.moves.urllib_parse'",
      "id": "sklearn.externals.six.Module_six_moves_urllib_parse",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib_parse",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:202",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib_request",
      "description": "'Lazy loading of moved objects in six.moves.urllib_request'",
      "id": "sklearn.externals.six.Module_six_moves_urllib_request",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib_request",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:247",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.safe_repr",
      "description": "'Hopefully pretty robust repr equivalent.'",
      "id": "sklearn.externals.joblib.format_stack.safe_repr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.safe_repr",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Arithmetic Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibArithmeticError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibArithmeticError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Assertion Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibAssertionError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibAssertionError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Import Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibImportError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibImportError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Memory Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibMemoryError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibMemoryError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Overflow Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibOverflowError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibOverflowError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Runtime Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibRuntimeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibRuntimeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Standard Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibStandardError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibStandardError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Stop Iteration",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibStopIteration",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibStopIteration",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib System Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibSystemError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibSystemError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib System Exit",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibSystemExit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibSystemExit",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Tab Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibTabError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibTabError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Type Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibTypeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibTypeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib User Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUserWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUserWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.get_active_backend",
      "description": "'Return the active default backend'",
      "id": "sklearn.externals.joblib.parallel.get_active_backend",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.get_active_backend",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.nmf.trace_dot",
      "description": "'Trace of np.dot(X, Y.T).'",
      "id": "sklearn.decomposition.nmf.trace_dot",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.nmf.trace_dot",
      "parameters": [],
      "tags": [
        "decomposition",
        "nmf"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.exec_",
      "description": "'Execute code in a namespace.'",
      "id": "sklearn.externals.six.exec_",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.exec_",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.remove_move",
      "description": "'Remove item from six.moves.'",
      "id": "sklearn.externals.six.remove_move",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.remove_move",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.format_signature",
      "description": "None",
      "id": "sklearn.externals.joblib.func_inspect.format_signature",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.format_signature",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Buffer Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibBufferError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibBufferError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Environment Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibEnvironmentError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibEnvironmentError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Generator Exit",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibGeneratorExit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibGeneratorExit",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Import Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibImportWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibImportWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Index Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibIndexError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibIndexError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib OS Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibOSError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibOSError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.eq_repr",
      "description": "None",
      "id": "sklearn.externals.joblib.format_stack.eq_repr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.eq_repr",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.logger.pformat",
      "description": "None",
      "id": "sklearn.externals.joblib.logger.pformat",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.logger.pformat",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "logger"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Attribute Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibAttributeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibAttributeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib EOF Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibEOFError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibEOFError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unicode Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.logger.short_format_time",
      "description": "None",
      "id": "sklearn.externals.joblib.logger.short_format_time",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.logger.short_format_time",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "logger"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.cpu_count",
      "description": "'Return the number of CPUs.'",
      "id": "sklearn.externals.joblib.parallel.cpu_count",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.cpu_count",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.make_nonnegative",
      "description": "'Ensure `X.min()` >= `min_value`.'",
      "id": "sklearn.utils.extmath.make_nonnegative",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.make_nonnegative",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.u",
      "description": "'Text literal'",
      "id": "sklearn.externals.six.u",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.u",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.comb2",
      "description": "None",
      "id": "sklearn.metrics.cluster.supervised.comb2",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.comb2",
      "parameters": [],
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.entropy",
      "description": "'Calculates the entropy for a labeling.'",
      "id": "sklearn.metrics.cluster.supervised.entropy",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.entropy",
      "parameters": [],
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.sag_fast",
      "common_name": "Multinomial Log Loss",
      "description": "None",
      "id": "sklearn.linear_model.sag_fast.MultinomialLogLoss",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sag_fast.MultinomialLogLoss",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sag_fast"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "urllib2.urlopen",
      "description": "None",
      "id": "urllib2.urlopen",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "urllib2.urlopen",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.format_records",
      "description": "None",
      "id": "sklearn.externals.joblib.format_stack.format_records",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.format_records",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.logger.format_time",
      "description": "None",
      "id": "sklearn.externals.joblib.logger.format_time",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.logger.format_time",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "logger"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Bytes Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibBytesWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibBytesWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Deprecation Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibDeprecationWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibDeprecationWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Future Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibFutureWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibFutureWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib IO Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibIOError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibIOError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Keyboard Interrupt",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibKeyboardInterrupt",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibKeyboardInterrupt",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Lookup Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibLookupError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibLookupError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Name Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibNameError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibNameError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Runtime Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibRuntimeWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibRuntimeWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Syntax Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibSyntaxWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibSyntaxWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Value Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibValueError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibValueError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.format_outer_frames",
      "description": "None",
      "id": "sklearn.externals.joblib.format_stack.format_outer_frames",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.format_outer_frames",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.add_move",
      "description": "'Add an item to six.moves.'",
      "id": "sklearn.externals.six.add_move",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.add_move",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.reraise",
      "description": "'Reraise an exception.'",
      "id": "sklearn.externals.six.reraise",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.reraise",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.arpack.get_O Pinv_matvec",
      "description": "None",
      "id": "sklearn.utils.arpack.get_OPinv_matvec",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.arpack.get_OPinv_matvec",
      "parameters": [],
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.fast_dict",
      "common_name": "Int Float Dict",
      "description": "None",
      "id": "sklearn.utils.fast_dict.IntFloatDict",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.fast_dict.IntFloatDict",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "utils",
        "fast_dict"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Base Exception",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibBaseException",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibBaseException",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Indentation Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibIndentationError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibIndentationError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Key Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibKeyError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibKeyError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Reference Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibReferenceError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibReferenceError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Syntax Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibSyntaxError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibSyntaxError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unicode Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.unsupervised.check_number_of_labels",
      "description": "None",
      "id": "sklearn.metrics.cluster.unsupervised.check_number_of_labels",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.unsupervised.check_number_of_labels",
      "parameters": [],
      "tags": [
        "metrics",
        "cluster",
        "unsupervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "posixpath.dirname",
      "description": "'Returns the directory component of a pathname'",
      "id": "posixpath.dirname",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "posixpath.dirname",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.b",
      "description": "'Byte literal'",
      "id": "sklearn.externals.six.b",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.b",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.print_",
      "description": "'The new-style print function.'",
      "id": "sklearn.externals.six.print_",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.print_",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.with_metaclass",
      "description": "'Create a base class with a metaclass.'",
      "id": "sklearn.externals.six.with_metaclass",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.with_metaclass",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle_compat.hex_str",
      "description": "'Convert an int to an hexadecimal string.'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.hex_str",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle_compat.hex_str",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.clear_data_home",
      "description": "'Delete all the content of the data home cache.'",
      "id": "sklearn.datasets.base.clear_data_home",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.clear_data_home",
      "parameters": [],
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.tosequence",
      "description": "'Cast iterable x to a Sequence, avoiding a copy if possible.'",
      "id": "sklearn.utils.tosequence",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.tosequence",
      "parameters": [],
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.funcsigs.signature",
      "description": "'Get a signature object for the passed callable.'",
      "id": "sklearn.externals.funcsigs.signature",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.funcsigs.signature",
      "parameters": [],
      "tags": [
        "externals",
        "funcsigs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.add_metaclass",
      "description": "'Class decorator for creating a class with a metaclass.'",
      "id": "sklearn.externals.six.add_metaclass",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.add_metaclass",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.iteritems",
      "description": "'Return an iterator over the (key, value) pairs of a dictionary.'",
      "id": "sklearn.externals.six.iteritems",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.iteritems",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "genericpath.isdir",
      "description": "'Return true if the pathname refers to an existing directory.'",
      "id": "genericpath.isdir",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "genericpath.isdir",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.iterkeys",
      "description": "'Return an iterator over the keys of a dictionary.'",
      "id": "sklearn.externals.six.iterkeys",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.iterkeys",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Worker Interrupt",
      "description": "' An exception that is not KeyboardInterrupt to allow subprocesses\nto be interrupted.\n'",
      "id": "sklearn.externals.joblib.my_exceptions.WorkerInterrupt",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.WorkerInterrupt",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/my_exceptions.pyc:53",
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Fallback To Backend",
      "description": "'Raised when configuration should fallback to another backend'",
      "id": "sklearn.externals.joblib._parallel_backends.FallbackToBackend",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib._parallel_backends.FallbackToBackend",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:356",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Transportable Exception",
      "description": "'An exception containing all the info to wrap an original\nexception and recreate it.\n'",
      "id": "sklearn.externals.joblib.my_exceptions.TransportableException",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.TransportableException",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/my_exceptions.pyc:40",
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.approximate",
      "common_name": "Projection To Hash Mixin",
      "description": "'Turn a transformed real-valued array into a hash'",
      "id": "sklearn.neighbors.approximate.ProjectionToHashMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.neighbors.approximate.ProjectionToHashMixin.fit_transform",
          "name": "fit_transform",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.neighbors.approximate.ProjectionToHashMixin.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.neighbors.approximate.ProjectionToHashMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/approximate.pyc:73",
      "tags": [
        "neighbors",
        "approximate"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.joblib.logger",
      "common_name": "Logger",
      "description": "' Base class for logging messages.\n'",
      "id": "sklearn.externals.joblib.logger.Logger",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.logger.Logger.debug",
          "name": "debug",
          "parameters": []
        },
        {
          "description": "' Return the formated representation of the object.\n'",
          "id": "sklearn.externals.joblib.logger.Logger.format",
          "name": "format",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.logger.Logger.warn",
          "name": "warn",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.logger.Logger",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/logger.pyc:63",
      "tags": [
        "externals",
        "joblib",
        "logger"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Undefined Metric Warning",
      "description": "'Warning used when the metric is invalid\n\n.. versionchanged:: 0.18\nMoved from sklearn.base.\n'",
      "id": "sklearn.exceptions.UndefinedMetricWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.UndefinedMetricWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:141",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.joblib.logger",
      "common_name": "Print Time",
      "description": "' Print and log messages while keeping track of time.\n'",
      "id": "sklearn.externals.joblib.logger.PrintTime",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.logger.PrintTime",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/logger.pyc:92",
      "tags": [
        "externals",
        "joblib",
        "logger"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Stationary Kernel Mixin",
      "description": "'Mixin for kernels which are stationary: k(X, Y)= f(X-Y).\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.gaussian_process.kernels.StationaryKernelMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.StationaryKernelMixin.is_stationary",
          "name": "is_stationary",
          "parameters": []
        }
      ],
      "name": "sklearn.gaussian_process.kernels.StationaryKernelMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:383",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Cluster Mixin",
      "description": "'Mixin class for all cluster estimators in scikit-learn.'",
      "id": "sklearn.base.ClusterMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.base.ClusterMixin.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.base.ClusterMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:391",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Floating Point Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibFloatingPointError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibFloatingPointError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Not Implemented Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibNotImplementedError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibNotImplementedError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Zero Division Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibZeroDivisionError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibZeroDivisionError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.disk.memstr_to_bytes",
      "description": "' Convert a memory text to its value in bytes.\n'",
      "id": "sklearn.externals.joblib.disk.memstr_to_bytes",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.disk.memstr_to_bytes",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "disk"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.get_unbound_function",
      "description": "'Get the function out of a possibly unbound function'",
      "id": "sklearn.externals.six.get_unbound_function",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.get_unbound_function",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "posixpath.expanduser",
      "description": "'Expand ~ and ~user constructions.  If user or $HOME is unknown,\ndo nothing.'",
      "id": "posixpath.expanduser",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "posixpath.expanduser",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Not Memorized Result",
      "description": "'Class representing an arbitrary value.\n\nThis class is a replacement for MemorizedResult when there is no cache.\n'",
      "id": "sklearn.externals.joblib.memory.NotMemorizedResult",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.NotMemorizedResult.clear",
          "name": "clear",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.NotMemorizedResult.get",
          "name": "get",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.memory.NotMemorizedResult",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:225",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Exception",
      "description": "'A simple exception with an error message that you can get to.'",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibException",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibException",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/my_exceptions.pyc:12",
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Meta Estimator Mixin",
      "description": "'Mixin class for all meta estimators in scikit-learn.'",
      "id": "sklearn.base.MetaEstimatorMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.base.MetaEstimatorMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:519",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Convergence Warning",
      "description": "'Custom warning to capture convergence problems\n\n.. versionchanged:: 0.18\nMoved from sklearn.utils.\n'",
      "id": "sklearn.exceptions.ConvergenceWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.ConvergenceWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:47",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.six",
      "common_name": "Module_six_moves_urllib",
      "description": "'Create a six.moves.urllib namespace that resembles the Python 3 namespace'",
      "id": "sklearn.externals.six.Module_six_moves_urllib",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.six.Module_six_moves_urllib",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/six.pyc:326",
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.disk.disk_used",
      "description": "' Return the disk usage in a directory.'",
      "id": "sklearn.externals.joblib.disk.disk_used",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.disk.disk_used",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "disk"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unbound Local Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnboundLocalError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnboundLocalError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unicode Translate Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeTranslateError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeTranslateError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.fixes.parallel_helper",
      "description": "'Helper to workaround Python 2 limitations of pickling instance methods'",
      "id": "sklearn.utils.fixes.parallel_helper",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.fixes.parallel_helper",
      "parameters": [],
      "tags": [
        "utils",
        "fixes"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.iterlists",
      "description": "'Return an iterator over the (key, [values]) pairs of a dictionary.'",
      "id": "sklearn.externals.six.iterlists",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.iterlists",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.base.is_regressor",
      "description": "'Returns True if the given estimator is (probably) a regressor.'",
      "id": "sklearn.base.is_regressor",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.base.is_regressor",
      "parameters": [],
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.ball_tree",
      "common_name": "Binary Tree",
      "description": "None",
      "id": "sklearn.neighbors.ball_tree.BinaryTree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.ball_tree.BinaryTree",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "ball_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Loss Function",
      "description": "'Base class for convex loss functions'",
      "id": "sklearn.linear_model.sgd_fast.LossFunction",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.LossFunction",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.kd_tree",
      "common_name": "Binary Tree",
      "description": "None",
      "id": "sklearn.neighbors.kd_tree.BinaryTree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.kd_tree.BinaryTree",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "kd_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Pending Deprecation Warning",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibPendingDeprecationWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibPendingDeprecationWarning",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unicode Decode Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeDecodeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeDecodeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Joblib Unicode Encode Error",
      "description": "None",
      "id": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeEncodeError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.my_exceptions.JoblibUnicodeEncodeError",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "my_exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.check_clusterings",
      "description": "'Check that the two clusterings matching 1D integer arrays.'",
      "id": "sklearn.metrics.cluster.supervised.check_clusterings",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.check_clusterings",
      "parameters": [],
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.lfw.check_fetch_lfw",
      "description": "'Helper function to download any missing LFW data'",
      "id": "sklearn.datasets.lfw.check_fetch_lfw",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.lfw.check_fetch_lfw",
      "parameters": [],
      "tags": [
        "datasets",
        "lfw"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "shutil.copyfileobj",
      "description": "'copy data from file-like object fsrc to file-like object fdst'",
      "id": "shutil.copyfileobj",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "shutil.copyfileobj",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "genericpath.exists",
      "description": "'Test whether a path exists.  Returns False for broken symbolic links'",
      "id": "genericpath.exists",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "genericpath.exists",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.six.itervalues",
      "description": "'Return an iterator over the values of a dictionary.'",
      "id": "sklearn.externals.six.itervalues",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.six.itervalues",
      "parameters": [],
      "tags": [
        "externals",
        "six"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.base.is_classifier",
      "description": "'Returns True if the given estimator is (probably) a classifier.'",
      "id": "sklearn.base.is_classifier",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.base.is_classifier",
      "parameters": [],
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.pool.delete_folder",
      "description": "'Utility function to cleanup a temporary folder if still existing.'",
      "id": "sklearn.externals.joblib.pool.delete_folder",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.pool.delete_folder",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Changed Behavior Warning",
      "description": "'Warning class used to notify the user of any change in the behavior.\n\n.. versionchanged:: 0.18\nMoved from sklearn.base.\n'",
      "id": "sklearn.exceptions.ChangedBehaviorWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.ChangedBehaviorWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:39",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Density Mixin",
      "description": "'Mixin class for all density estimators in scikit-learn.'",
      "id": "sklearn.base.DensityMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the score of the model on the data X\n",
          "id": "sklearn.base.DensityMixin.score",
          "name": "score",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "score: float"
          }
        }
      ],
      "name": "sklearn.base.DensityMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:500",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Batched Calls",
      "description": "'Wrap a sequence of (func, args, kwargs) tuples as a single callable'",
      "id": "sklearn.externals.joblib.parallel.BatchedCalls",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.parallel.BatchedCalls",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc:123",
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Job Lib Collision Warning",
      "description": "' Warn that there might be a collision between names of functions.\n'",
      "id": "sklearn.externals.joblib.memory.JobLibCollisionWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.memory.JobLibCollisionWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:69",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Least Squares Error",
      "description": "'Loss function for least squares (LS) estimation.\nTerminal regions need not to be updated for least squares. '",
      "id": "sklearn.ensemble.gradient_boosting.LeastSquaresError",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.LeastSquaresError.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.LeastSquaresError.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Least squares does not need to update terminal regions.\n\nBut it has to update the predictions.\n'",
          "id": "sklearn.ensemble.gradient_boosting.LeastSquaresError.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": []
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.LeastSquaresError",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:274",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Transformer Mixin",
      "description": "'Mixin class for all transformers in scikit-learn.'",
      "id": "sklearn.base.TransformerMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.base.TransformerMixin.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.base.TransformerMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:467",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Verbose Reporter",
      "description": "'Reports verbose output to stdout.\n\nIf ``verbose==1`` output is printed once in a while (when iteration mod\nverbose_mod is zero).; if larger than 1 then output is printed for\neach update.\n'",
      "id": "sklearn.ensemble.gradient_boosting.VerboseReporter",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.VerboseReporter.init",
          "name": "init",
          "parameters": []
        },
        {
          "description": "'Update reporter with new iteration. '",
          "id": "sklearn.ensemble.gradient_boosting.VerboseReporter.update",
          "name": "update",
          "parameters": []
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.VerboseReporter",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:664",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Efficiency Warning",
      "description": "'Warning used to notify the user of inefficient computation.\n\nThis warning notifies the user that the efficiency may not be optimal due\nto some reason which may be included as a part of the warning message.\nThis may be subclassed into a more specific Warning class.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.exceptions.EfficiencyWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.EfficiencyWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:87",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "datasets.base",
      "common_name": "Bunch",
      "description": "\"Container object for datasets\n\nDictionary-like object that exposes its keys as attributes.\n\n>>> b = Bunch(a=1, b=2)\n>>> b['b']\n2\n>>> b.b\n2\n>>> b.a = 3\n>>> b['a']\n3\n>>> b.c = 6\n>>> b['c']\n6\n\n\"",
      "id": "sklearn.datasets.base.Bunch",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.datasets.base.Bunch",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/datasets/base.pyc:29",
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.density",
      "description": "'Compute density of a sparse vector\n\nReturn a value between 0 and 1\n'",
      "id": "sklearn.utils.extmath.density",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.density",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.lib.stride_tricks.as_strided",
      "description": "' Make an ndarray from the given array with the given shape and strides.\n'",
      "id": "numpy.lib.stride_tricks.as_strided",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.lib.stride_tricks.as_strided",
      "parameters": [],
      "tags": [
        "lib",
        "stride_tricks"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_header",
      "description": "'\nGiven text in \"news\" format, strip the headers, by removing everything\nbefore the first blank line.\n'",
      "id": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_header",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_header",
      "parameters": [],
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.mldata.mldata_filename",
      "description": "'Convert a raw name for a data set in a mldata.org filename.'",
      "id": "sklearn.datasets.mldata.mldata_filename",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.mldata.mldata_filename",
      "parameters": [],
      "tags": [
        "datasets",
        "mldata"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Log",
      "description": "'Logistic regression loss for binary classification with y in {-1, 1}'",
      "id": "sklearn.linear_model.sgd_fast.Log",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.Log",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Squared Epsilon Insensitive",
      "description": "'Epsilon-Insensitive loss.\n\nloss = max(0, |y - p| - epsilon)^2\n'",
      "id": "sklearn.linear_model.sgd_fast.SquaredEpsilonInsensitive",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.SquaredEpsilonInsensitive",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Euclidean Distance",
      "description": "'Euclidean Distance metric\n\n.. math::\nD(x, y) = \\\\sqrt{ \\\\sum_i (x_i - y_i) ^ 2 }\n'",
      "id": "sklearn.neighbors.dist_metrics.EuclideanDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.EuclideanDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.seq_dataset",
      "common_name": "Sequential Dataset",
      "description": "'Base class for datasets with sequential data access. '",
      "id": "sklearn.utils.seq_dataset.SequentialDataset",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.seq_dataset.SequentialDataset",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "utils",
        "seq_dataset"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "Unsupervised Mixin",
      "description": "None",
      "id": "sklearn.neighbors.base.UnsupervisedMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data\n",
          "id": "sklearn.neighbors.base.UnsupervisedMixin.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. \"",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            }
          ]
        }
      ],
      "name": "sklearn.neighbors.base.UnsupervisedMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:789",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.memory.extract_first_line",
      "description": "' Extract the first line information from the function code\ntext if available.\n'",
      "id": "sklearn.externals.joblib.memory.extract_first_line",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.memory.extract_first_line",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.lfw.scale_face",
      "description": "'Scale back to 0-1 range in case of normalization for plotting'",
      "id": "sklearn.datasets.lfw.scale_face",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.lfw.scale_face",
      "parameters": [],
      "tags": [
        "datasets",
        "lfw"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.nmf.norm",
      "description": "'Dot product-based Euclidean norm implementation\n\nSee: http://fseoane.net/blog/2011/computing-the-vector-norm/\n'",
      "id": "sklearn.decomposition.nmf.norm",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.nmf.norm",
      "parameters": [],
      "tags": [
        "decomposition",
        "nmf"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.stochastic_gradient.fit_binary",
      "description": "'Fit a single binary classifier.\n\nThe i\\'th class is considered the \"positive\" class.\n'",
      "id": "sklearn.linear_model.stochastic_gradient.fit_binary",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.stochastic_gradient.fit_binary",
      "parameters": [],
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.gammaln",
      "description": "'DEPRECATED: The function gammaln is deprecated in 0.18 and will be removed in 0.20. Use scipy.special.gammaln instead.'",
      "id": "sklearn.utils.deprecation.gammaln",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.gammaln",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Classification",
      "description": "'Base class for loss functions for classification'",
      "id": "sklearn.linear_model.sgd_fast.Classification",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.Classification",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Regression",
      "description": "'Base class for loss functions for regression'",
      "id": "sklearn.linear_model.sgd_fast.Regression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.Regression",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Epsilon Insensitive",
      "description": "'Epsilon-Insensitive loss (used by SVR).\n\nloss = max(0, |y - p| - epsilon)\n'",
      "id": "sklearn.linear_model.sgd_fast.EpsilonInsensitive",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.EpsilonInsensitive",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.seq_dataset",
      "common_name": "CSR Dataset",
      "description": "'A ``SequentialDataset`` backed by a scipy sparse CSR matrix. '",
      "id": "sklearn.utils.seq_dataset.CSRDataset",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.seq_dataset.CSRDataset",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "utils",
        "seq_dataset"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Original undecorated function.",
          "name": "func",
          "type": "callable"
        }
      ],
      "common_name": "Not Memorized Func",
      "description": "'No-op object decorating a function.\n\nThis class replaces MemorizedFunc when there is no cache. It provides an\nidentical API but does not write anything on disk.\n\nAttributes\n----------\nfunc: callable\nOriginal undecorated function.\n'",
      "id": "sklearn.externals.joblib.memory.NotMemorizedFunc",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.NotMemorizedFunc.call_and_shelve",
          "name": "call_and_shelve",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.NotMemorizedFunc.clear",
          "name": "clear",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.memory.NotMemorizedFunc",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:267",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Partial result. Converged eigenvalues.",
          "name": "eigenvalues",
          "type": "ndarray"
        },
        {
          "description": "Partial result. Converged eigenvectors. ",
          "name": "eigenvectors",
          "type": "ndarray"
        }
      ],
      "category": "utils.arpack",
      "common_name": "Arpack No Convergence",
      "description": "'\nARPACK iteration did not converge\n\nAttributes\n----------\neigenvalues : ndarray\nPartial result. Converged eigenvalues.\neigenvectors : ndarray\nPartial result. Converged eigenvectors.\n\n'",
      "id": "sklearn.utils.arpack.ArpackNoConvergence",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.arpack.ArpackNoConvergence",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:321",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Non BLAS Dot Warning",
      "description": "'Warning used when the dot operation does not use BLAS.\n\nThis warning is used to notify the user that BLAS was not used for dot\noperation and hence the efficiency may be affected.\n\n.. versionchanged:: 0.18\nMoved from sklearn.utils.validation, extends EfficiencyWarning.\n'",
      "id": "sklearn.exceptions.NonBLASDotWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.NonBLASDotWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:130",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "ND Array Wrapper",
      "description": "'An object to be persisted instead of numpy arrays.\n\nThe only thing this object does, is to carry the filename in which\nthe array has been persisted, and the array subclass.\n'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Reconstruct the array.'",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper.read",
          "name": "read",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle_compat.NDArrayWrapper",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_compat.pyc:79",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Auto Batching Mixin",
      "description": "'A helper class for automagically batching jobs.'",
      "id": "sklearn.externals.joblib._parallel_backends.AutoBatchingMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Callback indicate how long it took to run a batch'",
          "id": "sklearn.externals.joblib._parallel_backends.AutoBatchingMixin.batch_completed",
          "name": "batch_completed",
          "parameters": []
        },
        {
          "description": "'Determine the optimal batch size'",
          "id": "sklearn.externals.joblib._parallel_backends.AutoBatchingMixin.compute_batch_size",
          "name": "compute_batch_size",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib._parallel_backends.AutoBatchingMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:149",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.download_20newsgroups",
      "description": "'Download the 20 newsgroups data and stored it as a zipped pickle.'",
      "id": "sklearn.datasets.twenty_newsgroups.download_20newsgroups",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.download_20newsgroups",
      "parameters": [],
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.disk.mkdirp",
      "description": "'Ensure directory d exists (like mkdir -p on Unix)\nNo guarantee that the directory is writable.\n'",
      "id": "sklearn.externals.joblib.disk.mkdirp",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.disk.mkdirp",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "disk"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.safe_min",
      "description": "'Returns the minimum value of a dense or a CSR/CSC matrix.\n\nAdapated from http://stackoverflow.com/q/13426580\n\n'",
      "id": "sklearn.utils.extmath.safe_min",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.safe_min",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.pool.has_shareable_memory",
      "description": "'Return True if a is backed by some mmap buffer directly or not.'",
      "id": "sklearn.externals.joblib.pool.has_shareable_memory",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.pool.has_shareable_memory",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.pool.reduce_memmap",
      "description": "'Pickle the descriptors of a memmap instance to reopen on same file.'",
      "id": "sklearn.externals.joblib.pool.reduce_memmap",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.pool.reduce_memmap",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.digamma",
      "description": "'DEPRECATED: The function digamma is deprecated in 0.18 and will be removed in 0.20. Use scipy.special.digamma instead.'",
      "id": "sklearn.utils.deprecation.digamma",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.digamma",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Manhattan Distance",
      "description": "'Manhattan/City-block Distance metric\n\n.. math::\nD(x, y) = \\\\sum_i |x_i - y_i|\n'",
      "id": "sklearn.neighbors.dist_metrics.ManhattanDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.ManhattanDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Squared Loss",
      "description": "'Squared loss traditional used in linear regression.'",
      "id": "sklearn.linear_model.sgd_fast.SquaredLoss",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.SquaredLoss",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Chebyshev Distance",
      "description": "'Chebyshev/Infinity Distance\n\n.. math::\nD(x, y) = max_i (|x_i - y_i|)\n'",
      "id": "sklearn.neighbors.dist_metrics.ChebyshevDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.ChebyshevDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib._memory_helpers.open_py_source",
      "description": "'Open a file in read only mode using the encoding detected by\ndetect_encoding().\n'",
      "id": "sklearn.externals.joblib._memory_helpers.open_py_source",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib._memory_helpers.open_py_source",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "_memory_helpers"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.base.make_dataset",
      "description": "'Create ``Dataset`` abstraction for sparse and dense inputs.\n\nThis also returns the ``intercept_decay`` which is different\nfor sparse datasets.\n'",
      "id": "sklearn.linear_model.base.make_dataset",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.base.make_dataset",
      "parameters": [],
      "tags": [
        "linear_model",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.assert_all_finite",
      "description": "'Throw a ValueError if X contains NaN or infinity.\n\nInput MUST be an np.ndarray instance or a scipy.sparse matrix.'",
      "id": "sklearn.utils.validation.assert_all_finite",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.assert_all_finite",
      "parameters": [],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.format_call",
      "description": "' Returns a nicely formatted statement displaying the function\ncall with the given arguments.\n'",
      "id": "sklearn.externals.joblib.func_inspect.format_call",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.format_call",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "Supervised Float Mixin",
      "description": "None",
      "id": "sklearn.neighbors.base.SupervisedFloatMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.base.SupervisedFloatMixin.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values, array of float values, shape = [n_samples] or [n_samples, n_outputs] \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.neighbors.base.SupervisedFloatMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:726",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "feature_extraction.text",
      "common_name": "Vectorizer Mixin",
      "description": "u'Provides common code for text vectorizers (tokenization logic).'",
      "id": "sklearn.feature_extraction.text.VectorizerMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "u'Return a callable that handles preprocessing and tokenization'",
          "id": "sklearn.feature_extraction.text.VectorizerMixin.build_analyzer",
          "name": "build_analyzer",
          "parameters": []
        },
        {
          "description": "u'Return a function to preprocess the text before tokenization'",
          "id": "sklearn.feature_extraction.text.VectorizerMixin.build_preprocessor",
          "name": "build_preprocessor",
          "parameters": []
        },
        {
          "description": "u'Return a function that splits a string into a sequence of tokens'",
          "id": "sklearn.feature_extraction.text.VectorizerMixin.build_tokenizer",
          "name": "build_tokenizer",
          "parameters": []
        },
        {
          "description": "u'Decode the input into a string of unicode symbols\n\nThe decoding strategy depends on the vectorizer parameters.\n'",
          "id": "sklearn.feature_extraction.text.VectorizerMixin.decode",
          "name": "decode",
          "parameters": []
        },
        {
          "description": "u'Build or fetch the effective stop words list'",
          "id": "sklearn.feature_extraction.text.VectorizerMixin.get_stop_words",
          "name": "get_stop_words",
          "parameters": []
        }
      ],
      "name": "sklearn.feature_extraction.text.VectorizerMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc:100",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.funcsigs",
      "common_name": "Bound Arguments",
      "description": "\"Result of `Signature.bind` call.  Holds the mapping of arguments\nto the function's parameters.\n\nHas the following public attributes:\n\n* arguments : OrderedDict\nAn ordered mutable mapping of parameters' names to arguments' values.\nDoes not contain arguments' default values.\n* signature : Signature\nThe Signature object that created this instance.\n* args : tuple\nTuple of positional arguments values.\n* kwargs : dict\nDict of keyword arguments values.\n\"",
      "id": "sklearn.externals.funcsigs.BoundArguments",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.funcsigs.BoundArguments",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc:347",
      "tags": [
        "externals",
        "funcsigs"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "Supervised Integer Mixin",
      "description": "None",
      "id": "sklearn.neighbors.base.SupervisedIntegerMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.base.SupervisedIntegerMixin.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values of shape = [n_samples] or [n_samples, n_outputs]  \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.neighbors.base.SupervisedIntegerMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:746",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Batch Completion Call Back",
      "description": "\"Callback used by joblib.Parallel's multiprocessing backend.\n\nThis callable is executed by the parent process whenever a worker process\nhas returned the results of a batch of tasks.\n\nIt is used for progress reporting, to update estimate of the batch\nprocessing duration and to schedule the next batch of tasks to be\nprocessed.\n\n\"",
      "id": "sklearn.externals.joblib.parallel.BatchCompletionCallBack",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.parallel.BatchCompletionCallBack",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc:195",
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Normalized Kernel Mixin",
      "description": "'Mixin for kernels which are normalized: k(X, X)=1.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.gaussian_process.kernels.NormalizedKernelMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.NormalizedKernelMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:357",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Data Dimensionality Warning",
      "description": "'Custom warning to notify potential issues with data dimensionality.\n\nFor example, in random projection, this warning is raised when the\nnumber of components, which quantifies the dimensionality of the target\nprojection space, is higher than the number of features, which quantifies\nthe dimensionality of the original source space, to imply that the\ndimensionality of the problem will not be reduced.\n\n.. versionchanged:: 0.18\nMoved from sklearn.utils.\n'",
      "id": "sklearn.exceptions.DataDimensionalityWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.DataDimensionalityWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:73",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "exceptions",
      "common_name": "Not Fitted Error",
      "description": "\"Exception class to raise if estimator is used before fitting.\n\nThis class inherits from both ValueError and AttributeError to help with\nexception handling and backward compatibility.\n\nExamples\n--------\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.exceptions import NotFittedError\n>>> try:\n...     LinearSVC().predict([[1, 2], [2, 3], [3, 4]])\n... except NotFittedError as e:\n...     print(repr(e))\n...                        # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\nNotFittedError('This LinearSVC instance is not fitted yet',)\n\n.. versionchanged:: 0.18\nMoved from sklearn.utils.validation.\n\"",
      "id": "sklearn.exceptions.NotFittedError",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.NotFittedError",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:17",
      "tags": [
        "exceptions"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.paired_euclidean_distances",
      "description": "'\nComputes the paired euclidean distances between X and Y\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.paired_euclidean_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.paired_euclidean_distances",
      "parameters": [
        {
          "description": " Y : array-like, shape (n_samples, n_features) ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "'",
        "name": "distances",
        "type": "ndarray"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.species_distributions.construct_grids",
      "description": "'Construct the map grid from the batch object\n",
      "id": "sklearn.datasets.species_distributions.construct_grids",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.species_distributions.construct_grids",
      "parameters": [
        {
          "description": "The object returned by :func:`fetch_species_distributions` ",
          "name": "batch",
          "type": ""
        }
      ],
      "returns": {
        "description": "The grid corresponding to the values in batch.coverages '",
        "name": "(xgrid, ygrid)",
        "type": ""
      },
      "tags": [
        "datasets",
        "species_distributions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Py Func Distance",
      "description": "'PyFunc Distance\n\nA user-defined distance\n",
      "id": "sklearn.neighbors.dist_metrics.PyFuncDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.PyFuncDistance",
      "parameters": [
        {
          "description": "func should take two numpy arrays as input, and return a distance. '",
          "name": "func",
          "type": "function"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.column_or_1d",
      "description": "' Ravel column or 1d numpy array, else raises an error\n",
      "id": "sklearn.utils.validation.column_or_1d",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.column_or_1d",
      "parameters": [
        {
          "description": "",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "To control display of warnings. ",
          "name": "warn",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": " '",
        "name": "y",
        "type": "array"
      },
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "posixpath.splitext",
      "description": "'Split the extension from a pathname.\n\nExtension is everything from the last dot to the end, ignoring\nleading dots.  Returns \"(root, ext)\"; ext may be empty.'",
      "id": "posixpath.splitext",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "posixpath.splitext",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_quoting",
      "description": "'\nGiven text in \"news\" format, strip lines beginning with the quote\ncharacters > or |, plus lines that often introduce a quoted section\n(for example, because they contain the string \\'writes:\\'.)\n'",
      "id": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_quoting",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_quoting",
      "parameters": [],
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.fast_logdet",
      "description": "'Compute log(det(A)) for A symmetric\n\nEquivalent to : np.log(nl.det(A)) but more robust.\nIt returns -Inf if det(A) is non positive or is not defined.\n'",
      "id": "sklearn.utils.extmath.fast_logdet",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.fast_logdet",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.csc_median_axis_0",
      "description": "'Find the median across axis 0 of a CSC matrix.\nIt is equivalent to doing np.median(X, axis=0).\n",
      "id": "sklearn.utils.sparsefuncs.csc_median_axis_0",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.csc_median_axis_0",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "returns": {
        "description": "Median.  '",
        "name": "median",
        "shape": "n_features,",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.has_fit_parameter",
      "description": "'Checks whether the estimator\\'s fit method supports the given parameter.\n\nExamples\n--------\n>>> from sklearn.svm import SVC\n>>> has_fit_parameter(SVC(), \"sample_weight\")\nTrue\n\n'",
      "id": "sklearn.utils.validation.has_fit_parameter",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.has_fit_parameter",
      "parameters": [],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "tree._criterion",
      "common_name": "Criterion",
      "description": "'Interface for impurity criteria.\n\nThis object stores methods on how to calculate how good a split is using\ndifferent metrics.\n'",
      "id": "sklearn.tree._criterion.Criterion",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree._criterion.Criterion",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "tree",
        "_criterion"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "tree._tree",
      "common_name": "Depth First Tree Builder",
      "description": "'Build a decision tree in depth-first fashion.'",
      "id": "sklearn.tree._tree.DepthFirstTreeBuilder",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree._tree.DepthFirstTreeBuilder",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "tree",
        "_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.deprecation",
      "common_name": "deprecated",
      "description": "'Decorator to mark a function or class as deprecated.\n\nIssue a warning when the function is called/the class is instantiated and\nadds a warning to the docstring.\n\nThe optional extra argument will be appended to the deprecation message\nand the docstring. Note: to use this with the default value for extra, put\nin an empty of parentheses:\n\n>>> from sklearn.utils import deprecated\n>>> deprecated() # doctest: +ELLIPSIS\n<sklearn.utils.deprecation.deprecated object at ...>\n\n>>> @deprecated()\n... def some_function(): pass\n'",
      "id": "sklearn.utils.deprecation.deprecated",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.deprecation.deprecated",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/deprecation.pyc:6",
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Classifier Mixin",
      "description": "'Mixin class for all classifiers in scikit-learn.'",
      "id": "sklearn.base.ClassifierMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.base.ClassifierMixin.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.base.ClassifierMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:320",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.safe_sparse_dot",
      "description": "'Dot product that handle the sparse matrix case correctly\n\nUses BLAS GEMM as replacement for numpy.dot where possible\nto avoid unnecessary copies.\n'",
      "id": "sklearn.utils.extmath.safe_sparse_dot",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.safe_sparse_dot",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_non_negative",
      "description": "'\nCheck if there is any negative value in an array.\n",
      "id": "sklearn.utils.validation.check_non_negative",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_non_negative",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "Who passed X to this function. '",
          "name": "whom",
          "type": "string"
        }
      ],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.fix_frame_records_filenames",
      "description": "'Try to fix the filenames in each record from inspect.getinnerframes().\n\nParticularly, modules loaded from within zip files have useless filenames\nattached to their code object, and inspect.getinnerframes() just uses it.\n'",
      "id": "sklearn.externals.joblib.format_stack.fix_frame_records_filenames",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.fix_frame_records_filenames",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.safe_mask",
      "description": "'Return a mask which is safe to use on X.\n",
      "id": "sklearn.utils.safe_mask",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.safe_mask",
      "parameters": [
        {
          "description": "Data on which to apply mask.  mask: array Mask to be used on X. ",
          "name": "X",
          "type": "array-like, sparse matrix"
        }
      ],
      "returns": {
        "description": "'",
        "name": "mask"
      },
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "pickle.whichmodule",
      "description": "'Figure out the module in which a function occurs.\n\nSearch sys.modules for the module.\nCache in classmap.\nReturn a module name.\nIf the function cannot be found, return \"__main__\".\n'",
      "id": "pickle.whichmodule",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "pickle.whichmodule",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.text.strip_tags",
      "description": "u'Basic regexp based HTML / XML tag stripper function\n\nFor serious HTML/XML preprocessing you should rather use an external\nlibrary such as lxml or BeautifulSoup.\n'",
      "id": "sklearn.feature_extraction.text.strip_tags",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.text.strip_tags",
      "parameters": [],
      "tags": [
        "feature_extraction",
        "text"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.wishart_logz",
      "description": "'DEPRECATED: The function wishart_logz is deprecated in 0.18 and will be removed in 0.20.\n\nThe logarithm of the normalization constant for the wishart distribution'",
      "id": "sklearn.utils.deprecation.wishart_logz",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.wishart_logz",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Huber",
      "description": "'Huber regression loss\n\nVariant of the SquaredLoss that is robust to outliers (quadratic near zero,\nlinear in for large errors).\n\nhttps://en.wikipedia.org/wiki/Huber_Loss_Function\n'",
      "id": "sklearn.linear_model.sgd_fast.Huber",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.Huber",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "S Euclidean Distance",
      "description": "'Standardized Euclidean Distance metric\n\n.. math::\nD(x, y) = \\\\sqrt{ \\\\sum_i \\x0crac{ (x_i - y_i) ^ 2}{V_i} }\n'",
      "id": "sklearn.neighbors.dist_metrics.SEuclideanDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.SEuclideanDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.seq_dataset",
      "common_name": "Array Dataset",
      "description": "'Dataset backed by a two-dimensional numpy array.\n\nThe dtype of the numpy array is expected to be ``np.float64`` (double)\nand C-style memory layout.\n'",
      "id": "sklearn.utils.seq_dataset.ArrayDataset",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.seq_dataset.ArrayDataset",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "utils",
        "seq_dataset"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.linear_kernel",
      "description": "'\nCompute the linear kernel between X and Y.\n\nRead more in the :ref:`User Guide <linear_kernel>`.\n",
      "id": "sklearn.metrics.pairwise.linear_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.linear_kernel",
      "parameters": [
        {
          "description": " Y : array of shape (n_samples_2, n_features) ",
          "name": "X",
          "shape": "n_samples_1, n_features",
          "type": "array"
        }
      ],
      "returns": {
        "description": "'",
        "name": "Gram matrix",
        "shape": "n_samples_1, n_samples_2",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_consistent_length",
      "description": "'Check that all arrays have consistent first dimensions.\n\nChecks whether all objects in arrays have the same shape or length.\n",
      "id": "sklearn.utils.validation.check_consistent_length",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_consistent_length",
      "parameters": [
        {
          "description": "Objects that will be checked for consistent length. '",
          "name": "*arrays",
          "type": "list"
        }
      ],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.distribute_covar_matrix_to_match_covariance_type",
      "description": "'DEPRECATED: The functon distribute_covar_matrix_to_match_covariance_typeis deprecated in 0.18 and will be removed in 0.20.\n\nCreate all the covariance matrices from a given template.'",
      "id": "sklearn.utils.deprecation.distribute_covar_matrix_to_match_covariance_type",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.distribute_covar_matrix_to_match_covariance_type",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.log_normalize",
      "description": "'DEPRECATED: The function log_normalize is deprecated in 0.18 and will be removed in 0.20.\n\nNormalized probabilities from unnormalized log-probabilites'",
      "id": "sklearn.utils.deprecation.log_normalize",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.log_normalize",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "contextlib.contextmanager",
      "description": "'@contextmanager decorator.\n\nTypical usage:\n\n@contextmanager\ndef some_generator(<arguments>):\n<setup>\ntry:\nyield <value>\nfinally:\n<cleanup>\n\nThis makes this:\n\nwith some_generator(<arguments>) as <variable>:\n<body>\n\nequivalent to this:\n\n<setup>\ntry:\n<variable> = <value>\n<body>\nfinally:\n<cleanup>\n\n'",
      "id": "contextlib.contextmanager",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "contextlib.contextmanager",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "ZND Array Wrapper",
      "description": "'An object to be persisted instead of numpy arrays.\n\nThis object store the Zfile filename in which\nthe data array has been persisted, and the meta information to\nretrieve it.\nThe reason that we store the raw buffer data of the array and\nthe meta information, rather than array representation routine\n(tostring) is that it enables us to use completely the strided\nmodel to avoid memory copies (a and a.T store as fast). In\naddition saving the heavy information separately can avoid\ncreating large temporary buffers when unpickling data with\nlarge arrays.\n'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Reconstruct the array from the meta-information and the z-file.'",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper.read",
          "name": "read",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle_compat.ZNDArrayWrapper",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_compat.pyc:115",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Label K Fold",
      "description": "'K-fold iterator variant with non-overlapping labels.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.GroupKFold` instead.\n\nThe same label will not appear in two different folds (the number of\ndistinct labels has to be at least equal to the number of folds).\n\nThe folds are approximately balanced in the sense that the number of\ndistinct labels is approximately the same in each fold.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.cross_validation.LabelKFold",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LabelKFold",
      "parameters": [
        {
          "description": "Contains a label for each sample. The folds are built so that the same label does not appear in two different folds. ",
          "name": "labels",
          "shape": "n_samples, ",
          "type": "array-like"
        },
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_folds",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:369",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Leave One Out",
      "description": "'Leave-One-Out cross validation iterator.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.LeaveOneOut` instead.\n\nProvides train/test indices to split data in train test sets. Each\nsample is used once as a test set (singleton) while the remaining\nsamples form the training set.\n\nNote: ``LeaveOneOut(n)`` is equivalent to ``KFold(n, n_folds=n)`` and\n``LeavePOut(n, p=1)``.\n\nDue to the high number of test sets (which is the same as the\nnumber of samples) this cross validation method can be very costly.\nFor large datasets one should favor KFold, StratifiedKFold or\nShuffleSplit.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.LeaveOneOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LeaveOneOut",
      "parameters": [
        {
          "description": "Total number of elements in dataset. ",
          "name": "n",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:109",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Mean Estimator",
      "description": "'An estimator predicting the mean of the training targets.'",
      "id": "sklearn.ensemble.gradient_boosting.MeanEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.MeanEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.MeanEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.MeanEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.MeanEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.MeanEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:90",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Quantile Estimator",
      "description": "'An estimator predicting the alpha-quantile of the training targets.'",
      "id": "sklearn.ensemble.gradient_boosting.QuantileEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.QuantileEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.QuantileEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.QuantileEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.QuantileEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.QuantileEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:68",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Quantile Loss Function",
      "description": "'Loss function for quantile regression.\n\nQuantile regression allows to estimate the percentiles\nof the conditional distribution of the target.\n'",
      "id": "sklearn.ensemble.gradient_boosting.QuantileLossFunction",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.QuantileLossFunction.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.QuantileLossFunction.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.QuantileLossFunction.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.QuantileLossFunction",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:400",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification",
        "decision tree"
      ],
      "attributes": [],
      "category": "cross_validation",
      "common_name": "K Fold",
      "description": "'K-Folds cross validation iterator.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.KFold` instead.\n\nProvides train/test indices to split data in train test sets. Split\ndataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used as a validation set once while the k - 1 remaining\nfold(s) form the training set.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.KFold",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.KFold",
      "parameters": [
        {
          "description": "Total number of elements. ",
          "name": "n",
          "type": "int"
        },
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_folds",
          "type": "int"
        },
        {
          "description": "Whether to shuffle the data before splitting into batches. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "When shuffle=True, pseudo-random number generator state used for shuffling. If None, use default numpy RNG for shuffling. ",
          "name": "random_state",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:274",
      "tags": [
        "cross_validation"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Prior Probability Estimator",
      "description": "'An estimator predicting the probability of each\nclass in the training data.\n'",
      "id": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.PriorProbabilityEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:136",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Stratified K Fold",
      "description": "'Stratified K-Folds cross validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.StratifiedKFold` instead.\n\nProvides train/test indices to split data in train test sets.\n\nThis cross-validation object is a variation of KFold that\nreturns stratified folds. The folds are made by preserving\nthe percentage of samples for each class.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.StratifiedKFold",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.StratifiedKFold",
      "parameters": [
        {
          "description": "Samples to split in K folds. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_folds",
          "type": "int"
        },
        {
          "description": "Whether to shuffle each stratification of the data before splitting into batches. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "When shuffle=True, pseudo-random number generator state used for shuffling. If None, use default numpy RNG for shuffling. ",
          "name": "random_state",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:475",
      "tags": [
        "cross_validation"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "grid_search",
      "common_name": "Parameter Grid",
      "description": "\"Grid of parameters with a discrete number of values for each.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.ParameterGrid` instead.\n\nCan be used to iterate over parameter value combinations with the\nPython built-in function iter.\n\nRead more in the :ref:`User Guide <grid_search>`.\n",
      "id": "sklearn.grid_search.ParameterGrid",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.grid_search.ParameterGrid",
      "parameters": [
        {
          "description": "The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.  An empty dict signifies default parameters.  A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below. ",
          "name": "param_grid",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc:46",
      "tags": [
        "grid_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Log Odds Estimator",
      "description": "'An estimator predicting the log odds ratio.'",
      "id": "sklearn.ensemble.gradient_boosting.LogOddsEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.LogOddsEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.LogOddsEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.LogOddsEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.LogOddsEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.LogOddsEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:106",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Zero Estimator",
      "description": "'An estimator that simply predicts zero. '",
      "id": "sklearn.ensemble.gradient_boosting.ZeroEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ZeroEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.ZeroEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ZeroEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.ZeroEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.ZeroEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:154",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Base Estimator",
      "description": "'Base class for all estimators in scikit-learn\n\nNotes\n-----\nAll estimators should specify all the parameters that can be set\nat the class level in their ``__init__`` as explicit keyword\narguments (no ``*args`` or ``**kwargs``).\n'",
      "id": "sklearn.base.BaseEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.base.BaseEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.base.BaseEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.base.BaseEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:183",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "exceptions",
      "common_name": "Data Conversion Warning",
      "description": "\"Warning used to notify implicit data conversions happening in the code.\n\nThis warning occurs when some input data needs to be converted or\ninterpreted in a way that may not match the user's expectations.\n\nFor example, this warning may occur when the user\n- passes an integer array to a function which expects float input and\nwill convert the input\n- requests a non-copying operation, but a copy is required to meet the\nimplementation's data-type expectations;\n- passes an input whose shape can be interpreted ambiguously.\n\n.. versionchanged:: 0.18\nMoved from sklearn.utils.validation.\n\"",
      "id": "sklearn.exceptions.DataConversionWarning",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.DataConversionWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:55",
      "tags": [
        "exceptions"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Leave One Label Out",
      "description": "'Leave-One-Label_Out cross-validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.LeaveOneGroupOut` instead.\n\nProvides train/test indices to split data according to a third-party\nprovided label. This label information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the labels could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.LeaveOneLabelOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LeaveOneLabelOut",
      "parameters": [
        {
          "description": "Arbitrary domain-specific stratification of the data to be used to draw the splits. ",
          "name": "labels",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:601",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Leave P Out",
      "description": "'Leave-P-Out cross validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.LeavePOut` instead.\n\nProvides train/test indices to split data in train test sets. This results\nin testing on all distinct samples of size p, while the remaining n - p\nsamples form the training set in each iteration.\n\nNote: ``LeavePOut(n, p)`` is NOT equivalent to ``KFold(n, n_folds=n // p)``\nwhich creates non-overlapping test sets.\n\nDue to the high number of iterations which grows combinatorically with the\nnumber of samples this cross validation method can be very costly. For\nlarge datasets one should favor KFold, StratifiedKFold or ShuffleSplit.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.LeavePOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LeavePOut",
      "parameters": [
        {
          "description": "Total number of elements in dataset. ",
          "name": "n",
          "type": "int"
        },
        {
          "description": "Size of the test sets. ",
          "name": "p",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:175",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Customizable Pickling Queue",
      "description": "'Locked Pipe implementation that uses a customizable pickler.\n\nThis class is an alternative to the multiprocessing implementation\nof SimpleQueue in order to make it possible to pass custom\npickling reducers, for instance to avoid memory copy when passing\nmemory mapped datastructures.\n\n`reducers` is expected to be a dict with key / values being\n`(type, callable)` pairs where `callable` is a function that, given an\ninstance of `type`, will return a tuple `(constructor, tuple_of_objects)`\nto rebuild an instance out of the pickled `tuple_of_objects` as would\nreturn a `__reduce__` method.\n\nSee the standard library documentation on pickling for more details.\n'",
      "id": "sklearn.externals.joblib.pool.CustomizablePicklingQueue",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePicklingQueue.empty",
          "name": "empty",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.pool.CustomizablePicklingQueue",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc:315",
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.funcsigs",
      "common_name": "Parameter",
      "description": "'Represents a parameter in a function signature.\n\nHas the following public attributes:\n\n* name : str\nThe name of the parameter as a string.\n* default : object\nThe default value for the parameter if specified.  If the\nparameter has no default value, this attribute is not set.\n* annotation\nThe annotation for the parameter if specified.  If the\nparameter has no annotation, this attribute is not set.\n* kind : str\nDescribes how argument values are bound to the parameter.\nPossible values: `Parameter.POSITIONAL_ONLY`,\n`Parameter.POSITIONAL_OR_KEYWORD`, `Parameter.VAR_POSITIONAL`,\n`Parameter.KEYWORD_ONLY`, `Parameter.VAR_KEYWORD`.\n'",
      "id": "sklearn.externals.funcsigs.Parameter",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Creates a customized copy of the Parameter.'",
          "id": "sklearn.externals.funcsigs.Parameter.replace",
          "name": "replace",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.funcsigs.Parameter",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc:207",
      "tags": [
        "externals",
        "funcsigs"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Least Absolute Error",
      "description": "'Loss function for least absolute deviation (LAD) regression. '",
      "id": "sklearn.ensemble.gradient_boosting.LeastAbsoluteError",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.LeastAbsoluteError.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'1.0 if y - pred > 0.0 else -1.0'",
          "id": "sklearn.ensemble.gradient_boosting.LeastAbsoluteError.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.LeastAbsoluteError.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.LeastAbsoluteError",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:305",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Base Cross Validator",
      "description": "'Base class for all cross-validators\n\nImplementations must define `_iter_test_masks` or `_iter_test_indices`.\n'",
      "id": "sklearn.model_selection._split.BaseCrossValidator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator'",
          "id": "sklearn.model_selection._split.BaseCrossValidator.get_n_splits",
          "name": "get_n_splits",
          "parameters": []
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.BaseCrossValidator.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.BaseCrossValidator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:54",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "Neighbors Base",
      "description": "'Base class for nearest neighbors estimators.'",
      "id": "sklearn.neighbors.base.NeighborsBase",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.base.NeighborsBase.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.base.NeighborsBase.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.base.NeighborsBase",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:102",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Scaled Log Odds Estimator",
      "description": "'Log odds ratio scaled by 0.5 -- for exponential loss. '",
      "id": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator.predict",
          "name": "predict",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.ScaledLogOddsEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:131",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Predefined Split",
      "description": "'Predefined split cross validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.PredefinedSplit` instead.\n\nSplits the data into training/test set folds according to a predefined\nscheme. Each sample can be assigned to at most one test set fold, as\nspecified by the user through the ``test_fold`` parameter.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.PredefinedSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.PredefinedSplit",
      "parameters": [
        {
          "description": "test_fold[i] gives the test set fold of sample i. A value of -1 indicates that the corresponding sample is not part of any test set folds, but will instead always be put into the training fold. ",
          "name": "test_fold",
          "shape": "n_samples,",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:1121",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "base",
      "common_name": "Regressor Mixin",
      "description": "'Mixin class for all regression estimators in scikit-learn.'",
      "id": "sklearn.base.RegressorMixin",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.base.RegressorMixin.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.base.RegressorMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:353",
      "tags": [
        "base"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "model_selection._search",
      "common_name": "Parameter Grid",
      "description": "\"Grid of parameters with a discrete number of values for each.\n\nCan be used to iterate over parameter value combinations with the\nPython built-in function iter.\n\nRead more in the :ref:`User Guide <search>`.\n",
      "id": "sklearn.model_selection._search.ParameterGrid",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.model_selection._search.ParameterGrid",
      "parameters": [
        {
          "description": "The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values.  An empty dict signifies default parameters.  A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below. ",
          "name": "param_grid",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc:44",
      "tags": [
        "model_selection",
        "_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "base",
      "common_name": "Bicluster Mixin",
      "description": "'Mixin class for all bicluster estimators in scikit-learn'",
      "id": "sklearn.base.BiclusterMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Row and column indices of the i'th bicluster.\n\nOnly works if ``rows_`` and ``columns_`` attributes exist.\n",
          "id": "sklearn.base.BiclusterMixin.get_indices",
          "name": "get_indices",
          "parameters": [],
          "returns": {
            "description": "Indices of rows in the dataset that belong to the bicluster. col_ind : np.array, dtype=np.intp Indices of columns in the dataset that belong to the bicluster.  \"",
            "name": "row_ind",
            "type": "np"
          }
        },
        {
          "description": "\"Shape of the i'th bicluster.\n",
          "id": "sklearn.base.BiclusterMixin.get_shape",
          "name": "get_shape",
          "parameters": [],
          "returns": {
            "description": "Number of rows and columns (resp.) in the bicluster. \"",
            "name": "shape",
            "type": ""
          }
        },
        {
          "description": "'Returns the submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if ``rows_`` and\n``columns_`` attributes exist.\n\n'",
          "id": "sklearn.base.BiclusterMixin.get_submatrix",
          "name": "get_submatrix",
          "parameters": []
        }
      ],
      "name": "sklearn.base.BiclusterMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/base.pyc:414",
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.sigmoid_kernel",
      "description": "'\nCompute the sigmoid kernel between X and Y::\n\nK(X, Y) = tanh(gamma <X, Y> + coef0)\n\nRead more in the :ref:`User Guide <sigmoid_kernel>`.\n",
      "id": "sklearn.metrics.pairwise.sigmoid_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.sigmoid_kernel",
      "parameters": [
        {
          "description": " Y : ndarray of shape (n_samples_2, n_features) ",
          "name": "X",
          "shape": "n_samples_1, n_features",
          "type": "ndarray"
        },
        {
          "description": "If None, defaults to 1.0 / n_samples_1  coef0 : int, default 1 ",
          "name": "gamma",
          "type": "float"
        }
      ],
      "returns": {
        "description": "'",
        "name": "Gram matrix",
        "shape": "n_samples_1, n_samples_2",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "os.makedirs",
      "description": "'makedirs(path [, mode=0777])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.\nWorks like mkdir, except that any intermediate path segment (not\njust the rightmost) will be created if it does not exist.  This is\nrecursive.\n\n'",
      "id": "os.makedirs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "os.makedirs",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_footer",
      "description": "'\nGiven text in \"news\" format, attempt to remove a signature block.\n\nAs a rough heuristic, we assume that signatures are set apart by either\na blank line or a line made of hyphens, and that it is the last such line\nin the file (disregarding blank lines at the end).\n'",
      "id": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_footer",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.strip_newsgroup_footer",
      "parameters": [],
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "posixpath.join",
      "description": "\"Join two or more pathname components, inserting '/' as needed.\nIf any component is an absolute path, all previous path components\nwill be discarded.  An empty last part will result in a path that\nends with a separator.\"",
      "id": "posixpath.join",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "posixpath.join",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Matching Distance",
      "description": "'Matching Distance\n\nMatching Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{N_{TF} + N_{FT}}{N}\n'",
      "id": "sklearn.neighbors.dist_metrics.MatchingDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.MatchingDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_row_scale",
      "description": "' Inplace row scaling of a CSR or CSC matrix.\n\nScale each row of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_row_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_row_scale",
      "parameters": [
        {
          "description": "Matrix to be scaled. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        },
        {
          "description": "Array of precomputed sample-wise values to use for scaling. '",
          "name": "scale",
          "shape": "n_features,",
          "type": "float"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_swap_column",
      "description": "'\nSwaps two columns of a CSC/CSR matrix in-place.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_swap_column",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_swap_column",
      "parameters": [
        {
          "description": "Matrix whose two columns are to be swapped.  m: int Index of the column of X to be swapped. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        },
        {
          "description": "Index of the column of X to be swapped. '",
          "name": "n",
          "type": "int"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_swap_row_csc",
      "description": "'\nSwaps two rows of a CSC matrix in-place.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_swap_row_csc",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_swap_row_csc",
      "parameters": [
        {
          "description": "Matrix whose two rows are to be swapped.  m: int Index of the row of X to be swapped.  n: int Index of the row of X to be swapped. '",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "scipy"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_swap_row_csr",
      "description": "'\nSwaps two rows of a CSR matrix in-place.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_swap_row_csr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_swap_row_csr",
      "parameters": [
        {
          "description": "Matrix whose two rows are to be swapped.  m: int Index of the row of X to be swapped.  n: int Index of the row of X to be swapped. '",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "scipy"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_random_state",
      "description": "'Turn seed into a np.random.RandomState instance\n\nIf seed is None, return the RandomState singleton used by np.random.\nIf seed is an int, return a new RandomState instance seeded with seed.\nIf seed is already a RandomState instance, return it.\nOtherwise raise ValueError.\n'",
      "id": "sklearn.utils.validation.check_random_state",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_random_state",
      "parameters": [],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.text.strip_accents_ascii",
      "description": "u'Transform accentuated unicode symbols into ascii or nothing\n\nWarning: this solution is only suited for languages that have a direct\ntransliteration to ASCII symbols.\n\nSee also\n--------\nstrip_accents_unicode\nRemove accentuated char for any unicode symbol.\n'",
      "id": "sklearn.feature_extraction.text.strip_accents_ascii",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.text.strip_accents_ascii",
      "parameters": [],
      "tags": [
        "feature_extraction",
        "text"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.polynomial_kernel",
      "description": "'\nCompute the polynomial kernel between X and Y::\n\nK(X, Y) = (gamma <X, Y> + coef0)^degree\n\nRead more in the :ref:`User Guide <polynomial_kernel>`.\n",
      "id": "sklearn.metrics.pairwise.polynomial_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.polynomial_kernel",
      "parameters": [
        {
          "description": " Y : ndarray of shape (n_samples_2, n_features) ",
          "name": "X",
          "shape": "n_samples_1, n_features",
          "type": "ndarray"
        },
        {
          "description": "",
          "name": "degree",
          "type": "int"
        },
        {
          "description": "if None, defaults to 1.0 / n_samples_1  coef0 : int, default 1 ",
          "name": "gamma",
          "type": "float"
        }
      ],
      "returns": {
        "description": "'",
        "name": "Gram matrix",
        "shape": "n_samples_1, n_samples_2",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.regression_models.constant",
      "description": "'\nZero order polynomial (constant, p = 1) regression model.\n\nx --> f(x) = 1\n",
      "id": "sklearn.gaussian_process.regression_models.constant",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.regression_models.constant",
      "parameters": [
        {
          "description": "An array with shape (n_eval, n_features) giving the locations x at which the regression model should be evaluated. ",
          "name": "x",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, p) with the values of the regression model. '",
        "name": "f",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "regression_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.sparse_center_data",
      "description": "'DEPRECATED: sparse_center_data was deprecated in version 0.18 and will be removed in 0.20. Use utilities in preprocessing.data instead\n\n\nCompute information needed to center data to have mean zero along\naxis 0. Be aware that X will not be centered since it would break\nthe sparsity, but will be normalized if asked so.\n'",
      "id": "sklearn.utils.deprecation.sparse_center_data",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.sparse_center_data",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.norm",
      "description": "'Compute the Euclidean or Frobenius norm of x.\n\nReturns the Euclidean norm when x is a vector, the Frobenius norm when x\nis a matrix (2-d array). More precise than sqrt(squared_norm(x)).\n'",
      "id": "sklearn.utils.extmath.norm",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.norm",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.stable_cumsum",
      "description": "'Use high precision for cumsum and check that final value matches sum\n",
      "id": "sklearn.utils.extmath.stable_cumsum",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.stable_cumsum",
      "parameters": [
        {
          "description": "To be cumulatively summed as flat",
          "name": "arr",
          "type": "array-like"
        },
        {
          "description": "Relative tolerance, see ``np.allclose``",
          "name": "rtol",
          "type": "float"
        },
        {
          "description": "Absolute tolerance, see ``np.allclose`` '",
          "name": "atol",
          "type": "float"
        }
      ],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.indices_to_mask",
      "description": "'Convert list of indices to boolean mask.\n",
      "id": "sklearn.utils.indices_to_mask",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.indices_to_mask",
      "parameters": [
        {
          "description": "List of integers treated as indices.",
          "name": "indices",
          "type": "list-like"
        },
        {
          "description": "Length of boolean mask to be generated. ",
          "name": "mask_length",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Boolean array that is True where indices are present, else False. '",
        "name": "mask",
        "type": ""
      },
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.safe_indexing",
      "description": "'Return items or rows from X using indices.\n\nAllows simple indexing of lists or arrays.\n",
      "id": "sklearn.utils.safe_indexing",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.safe_indexing",
      "parameters": [
        {
          "description": "Data from which to sample rows or items. ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "Indices according to which X will be subsampled. '",
          "name": "indices",
          "type": "array-like"
        }
      ],
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_csr_row_scale",
      "description": "' Inplace row scaling of a CSR matrix.\n\nScale each sample of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_csr_row_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_csr_row_scale",
      "parameters": [
        {
          "description": "Matrix to be scaled. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        },
        {
          "description": "Array of precomputed sample-wise values to use for scaling. '",
          "name": "scale",
          "shape": "n_samples,",
          "type": "float"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.mean_variance_axis",
      "description": "'Compute mean and variance along an axix on a CSR or CSC matrix\n",
      "id": "sklearn.utils.sparsefuncs.mean_variance_axis",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.mean_variance_axis",
      "parameters": [
        {
          "description": "Input data.  axis: int (either 0 or 1) Axis along which the axis should be computed. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "returns": {
        "description": "means: float array with shape (n_features,) Feature-wise means  variances: float array with shape (n_features,) Feature-wise variances  '",
        "name": ""
      },
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.indexable",
      "description": "'Make arrays indexable for cross-validation.\n\nChecks consistent length, passes through None, and ensures that everything\ncan be indexed by converting sparse matrices to csr and converting\nnon-interable objects to arrays.\n",
      "id": "sklearn.utils.validation.indexable",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.indexable",
      "parameters": [
        {
          "description": "List of objects to ensure sliceability. '",
          "name": "*iterables",
          "type": "lists"
        }
      ],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.delayed",
      "description": "\"Decorator used to capture the arguments of a function.\n\nPass `check_pickle=False` when:\n\n- performing a possibly repeated check is too costly and has been done\nalready once outside of the call to delayed.\n\n- when used in conjunction `Parallel(backend='threading')`.\n\n\"",
      "id": "sklearn.externals.joblib.parallel.delayed",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.delayed",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Squared Hinge",
      "description": "'Squared Hinge loss for binary classification tasks with y in {-1,1}\n",
      "id": "sklearn.linear_model.sgd_fast.SquaredHinge",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.SquaredHinge",
      "parameters": [
        {
          "description": "Margin threshold. When threshold=1.0, one gets the loss used by (quadratically penalized) SVM. '",
          "name": "threshold",
          "type": "float"
        }
      ],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Bray Curtis Distance",
      "description": "'Bray-Curtis Distance\n\nBray-Curtis distance is meant for discrete-valued vectors, though it is\na valid metric for real-valued vectors.\n\n.. math::\nD(x, y) = \\x0crac{\\\\sum_i |x_i - y_i|}{\\\\sum_i(|x_i| + |y_i|)}\n'",
      "id": "sklearn.neighbors.dist_metrics.BrayCurtisDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.BrayCurtisDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Hamming Distance",
      "description": "'Hamming Distance\n\nHamming distance is meant for discrete-valued vectors, though it is\na valid metric for real-valued vectors.\n\n.. math::\nD(x, y) = \\x0crac{1}{N} \\\\sum_i \\\\delta_{x_i, y_i}\n'",
      "id": "sklearn.neighbors.dist_metrics.HammingDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.HammingDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "tree._splitter",
      "common_name": "Splitter",
      "description": "'Abstract splitter class.\n\nSplitters are called by tree builders to find the best splits on both\nsparse and dense data, one split at a time.\n'",
      "id": "sklearn.tree._splitter.Splitter",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree._splitter.Splitter",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "tree",
        "_splitter"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.format_exc",
      "description": "' Return a nice text document describing the traceback.\n",
      "id": "sklearn.externals.joblib.format_stack.format_exc",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.format_exc",
      "parameters": [
        {
          "description": "context: number of lines of the source file to plot tb_offset: the number of stack frame not to use (0 = use all)  '",
          "name": "etype, evalue, etb",
          "type": "as"
        }
      ],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.interface.aslinearoperator",
      "description": "\"Return A as a LinearOperator.\n\n'A' may be any of the following types:\n- ndarray\n- matrix\n- sparse matrix (e.g. csr_matrix, lil_matrix, etc.)\n- LinearOperator\n- An object with .shape and .matvec attributes\n\nSee the LinearOperator documentation for additional information.\n\nExamples\n--------\n>>> from scipy.sparse.linalg import aslinearoperator\n>>> M = np.array([[1,2,3],[4,5,6]], dtype=np.int32)\n>>> aslinearoperator(M)\n<2x3 MatrixLinearOperator with dtype=int32>\n\n\"",
      "id": "scipy.sparse.linalg.interface.aslinearoperator",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.interface.aslinearoperator",
      "parameters": [],
      "tags": [
        "sparse",
        "linalg",
        "interface"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_csr_column_scale",
      "description": "'Inplace column scaling of a CSR matrix.\n\nScale each feature of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_csr_column_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_csr_column_scale",
      "parameters": [
        {
          "description": "Matrix to normalize using the variance of the features. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        },
        {
          "description": "Array of precomputed feature-wise values to use for scaling. '",
          "name": "scale",
          "shape": "n_features,",
          "type": "float"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.text.strip_accents_unicode",
      "description": "u'Transform accentuated unicode symbols into their simple counterpart\n\nWarning: the python-level loop and join operations make this\nimplementation 20 times slower than the strip_accents_ascii basic\nnormalization.\n\nSee also\n--------\nstrip_accents_ascii\nRemove accentuated char for any unicode symbol that has a direct\nASCII equivalent.\n'",
      "id": "sklearn.feature_extraction.text.strip_accents_unicode",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.text.strip_accents_unicode",
      "parameters": [],
      "tags": [
        "feature_extraction",
        "text"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.paired_cosine_distances",
      "description": "'\nComputes the paired cosine distances between X and Y\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.paired_cosine_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.paired_cosine_distances",
      "parameters": [
        {
          "description": " Y : array-like, shape (n_samples, n_features) ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " Notes ------ The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm '",
        "name": "distances",
        "shape": "n_samples, ",
        "type": "ndarray"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Minkowski Distance",
      "description": "'Minkowski Distance\n\n.. math::\nD(x, y) = [\\\\sum_i (x_i - y_i)^p] ^ (1/p)\n\nMinkowski Distance requires p >= 1 and finite. For p = infinity,\nuse ChebyshevDistance.\nNote that for p=1, ManhattanDistance is more efficient, and for\np=2, EuclideanDistance is more efficient.\n'",
      "id": "sklearn.neighbors.dist_metrics.MinkowskiDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.MinkowskiDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Sokal Michener Distance",
      "description": "'Sokal-Michener Distance\n\nSokal-Michener Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{2 (N_{TF} + N_{FT})}{N + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.SokalMichenerDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.SokalMichenerDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Sokal Sneath Distance",
      "description": "'Sokal-Sneath Distance\n\nSokal-Sneath Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{N_{TF} + N_{FT}}{N_{TT} / 2 + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.SokalSneathDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.SokalSneathDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "threading.R Lock",
      "description": "'Factory function that returns a new reentrant lock.\n\nA reentrant lock must be released by the thread that acquired it. Once a\nthread has acquired a reentrant lock, the same thread may acquire it again\nwithout blocking; the thread must release it once for each time it has\nacquired it.\n\n'",
      "id": "threading.RLock",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "threading.RLock",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.logsumexp",
      "description": "'Computes the sum of arr assuming arr is in the log domain.\n\nReturns log(sum(exp(arr))) while minimizing the possibility of\nover/underflow.\n\nExamples\n--------\n\n>>> import numpy as np\n>>> from sklearn.utils.extmath import logsumexp\n>>> a = np.arange(10)\n>>> np.log(np.sum(np.exp(a)))\n9.4586297444267107\n>>> logsumexp(a)\n9.4586297444267107\n'",
      "id": "sklearn.utils.extmath.logsumexp",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.logsumexp",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.row_norms",
      "description": "'Row-wise (squared) Euclidean norm of X.\n\nEquivalent to np.sqrt((X * X).sum(axis=1)), but also supports sparse\nmatrices and does not create an X.shape-sized temporary.\n\nPerforms no input validation.\n'",
      "id": "sklearn.utils.extmath.row_norms",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.row_norms",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.safe_sqr",
      "description": "'Element wise squaring of array-likes and sparse matrices.\n",
      "id": "sklearn.utils.safe_sqr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.safe_sqr",
      "parameters": [
        {
          "description": "",
          "name": "X",
          "type": "array"
        },
        {
          "description": "Whether to create a copy of X and operate on it or to perform inplace computation (default behaviour). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "'",
        "name": "X ** 2",
        "type": "element"
      },
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.count_nonzero",
      "description": "'A variant of X.getnnz() with extension to weighting on axis 0\n\nUseful in efficiently calculating multilabel metrics.\n",
      "id": "sklearn.utils.sparsefuncs.count_nonzero",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.count_nonzero",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "shape": "n_samples, n_labels",
          "type": ""
        },
        {
          "description": "The axis on which the data is aggregated. ",
          "name": "axis",
          "type": ""
        },
        {
          "description": "Weight for each row of X. '",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array"
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.min_max_axis",
      "description": "'Compute minimum and maximum along an axis on a CSR or CSC matrix\n",
      "id": "sklearn.utils.sparsefuncs.min_max_axis",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.min_max_axis",
      "parameters": [
        {
          "description": "Input data.  axis: int (either 0 or 1) Axis along which the axis should be computed. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "returns": {
        "description": "mins: float array with shape (n_features,) Feature-wise minima  maxs: float array with shape (n_features,) Feature-wise maxima '",
        "name": ""
      },
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.paired_manhattan_distances",
      "description": "'Compute the L1 distances between the vectors in X and Y.\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.paired_manhattan_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.paired_manhattan_distances",
      "parameters": [
        {
          "description": " Y : array-like, shape (n_samples, n_features) ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "'",
        "name": "distances",
        "type": "ndarray"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Canberra Distance",
      "description": "'Canberra Distance\n\nCanberra distance is meant for discrete-valued vectors, though it is\na valid metric for real-valued vectors.\n\n.. math::\nD(x, y) = \\\\sum_i \\x0crac{|x_i - y_i|}{|x_i| + |y_i|}\n'",
      "id": "sklearn.neighbors.dist_metrics.CanberraDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.CanberraDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Rogers Tanimoto Distance",
      "description": "'Rogers-Tanimoto Distance\n\nRogers-Tanimoto Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{2 (N_{TF} + N_{FT})}{N + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.RogersTanimotoDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.RogersTanimotoDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "tree._tree",
      "common_name": "Best First Tree Builder",
      "description": "'Build a decision tree in best-first fashion.\n\nThe best node to expand is given by the node at the frontier that has the\nhighest impurity improvement.\n'",
      "id": "sklearn.tree._tree.BestFirstTreeBuilder",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree._tree.BestFirstTreeBuilder",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "tree",
        "_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Hinge",
      "description": "'Hinge loss for binary classification tasks with y in {-1,1}\n",
      "id": "sklearn.linear_model.sgd_fast.Hinge",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.Hinge",
      "parameters": [
        {
          "description": "Margin threshold. When threshold=1.0, one gets the loss used by SVM. When threshold=0.0, one gets the loss used by the Perceptron. '",
          "name": "threshold",
          "type": "float"
        }
      ],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "linear_model.sgd_fast",
      "common_name": "Modified Huber",
      "description": "\"Modified Huber loss for binary classification with y in {-1, 1}\n\nThis is equivalent to quadratically smoothed SVM with gamma = 2.\n\nSee T. Zhang 'Solving Large Scale Linear Prediction Problems Using\nStochastic Gradient Descent', ICML'04.\n\"",
      "id": "sklearn.linear_model.sgd_fast.ModifiedHuber",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.linear_model.sgd_fast.ModifiedHuber",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "linear_model",
        "sgd_fast"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Jaccard Distance",
      "description": "'Jaccard Distance\n\nJaccard Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{N_{TF} + N_{FT}}{N_{TT} + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.JaccardDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.JaccardDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Kulsinski Distance",
      "description": "'Kulsinski Distance\n\nKulsinski Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = 1 - \\x0crac{N_{TT}}{N + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.KulsinskiDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.KulsinskiDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Mahalanobis Distance",
      "description": "'Mahalanobis Distance\n\n.. math::\nD(x, y) = \\\\sqrt{ (x - y)^T V^{-1} (x - y) }\n",
      "id": "sklearn.neighbors.dist_metrics.MahalanobisDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.MahalanobisDistance",
      "parameters": [
        {
          "description": "Symmetric positive-definite covariance matrix. The inverse of this matrix will be explicitly computed. VI : array_like optionally specify the inverse directly.  If VI is passed, then V is not referenced. '",
          "name": "V",
          "type": "array"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Russell Rao Distance",
      "description": "'Russell-Rao Distance\n\nRussell-Rao Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{N - N_{TT}}{N}\n'",
      "id": "sklearn.neighbors.dist_metrics.RussellRaoDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.RussellRaoDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "W Minkowski Distance",
      "description": "'Weighted Minkowski Distance\n\n.. math::\nD(x, y) = [\\\\sum_i w_i (x_i - y_i)^p] ^ (1/p)\n\nWeighted Minkowski Distance requires p >= 1 and finite.\n",
      "id": "sklearn.neighbors.dist_metrics.WMinkowskiDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.WMinkowskiDistance",
      "parameters": [
        {
          "description": "The order of the norm of the difference :math:`{||u-v||}_p`.",
          "name": "p",
          "type": "int"
        },
        {
          "description": "The weight vector.  '",
          "name": "w",
          "type": ""
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Dice Distance",
      "description": "'Dice Distance\n\nDice Distance is a dissimilarity measure for boolean-valued\nvectors. All nonzero entries will be treated as True, zero entries will\nbe treated as False.\n\n.. math::\nD(x, y) = \\x0crac{N_{TF} + N_{FT}}{2 * N_{TT} + N_{TF} + N_{FT}}\n'",
      "id": "sklearn.neighbors.dist_metrics.DiceDistance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.DiceDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle_compat.read_zfile",
      "description": "'Read the z-file and return the content as a string.\n\nZ-files are raw data compressed with zlib used internally by joblib\nfor persistence. Backward compatibility is not guaranteed. Do not\nuse for external purposes.\n'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.read_zfile",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle_compat.read_zfile",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle_compat.write_zfile",
      "description": "'Write the data in the given file as a Z-file.\n\nZ-files are raw data compressed with zlib used internally by joblib\nfor persistence. Backward compatibility is not guarantied. Do not\nuse for external purposes.\n'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.write_zfile",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle_compat.write_zfile",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.load_lfw_pairs",
      "description": "\"DEPRECATED: Function 'load_lfw_pairs' has been deprecated in 0.17 and will be removed in 0.19.Use fetch_lfw_pairs(download_if_missing=False) instead.\n\n\nAlias for fetch_lfw_pairs(download_if_missing=False)\n\n.. deprecated:: 0.17\nThis function will be removed in 0.19.\nUse :func:`sklearn.datasets.fetch_lfw_pairs` with parameter\n``download_if_missing=False`` instead.\n\nCheck fetch_lfw_pairs.__doc__ for the documentation and parameter list.\n\"",
      "id": "sklearn.utils.deprecation.load_lfw_pairs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.load_lfw_pairs",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.load_lfw_people",
      "description": "\"DEPRECATED: Function 'load_lfw_people' has been deprecated in 0.17 and will be removed in 0.19.Use fetch_lfw_people(download_if_missing=False) instead.\n\n\nAlias for fetch_lfw_people(download_if_missing=False)\n\n.. deprecated:: 0.17\nThis function will be removed in 0.19.\nUse :func:`sklearn.datasets.fetch_lfw_people` with parameter\n``download_if_missing=False`` instead.\n\nCheck fetch_lfw_people.__doc__ for the documentation and parameter list.\n\"",
      "id": "sklearn.utils.deprecation.load_lfw_people",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.load_lfw_people",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.squared_norm",
      "description": "'Squared Euclidean or Frobenius norm of x.\n\nReturns the Euclidean norm when x is a vector, the Frobenius norm when x\nis a matrix (2-d array). Faster than norm(x) ** 2.\n'",
      "id": "sklearn.utils.extmath.squared_norm",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.squared_norm",
      "parameters": [],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.multiclass.check_classification_targets",
      "description": "\"Ensure that target y is of a non-regression type.\n\nOnly the following target types (as defined in type_of_target) are allowed:\n'binary', 'multiclass', 'multiclass-multioutput',\n'multilabel-indicator', 'multilabel-sequences'\n",
      "id": "sklearn.utils.multiclass.check_classification_targets",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.multiclass.check_classification_targets",
      "parameters": [
        {
          "description": "\"",
          "name": "y",
          "type": "array-like"
        }
      ],
      "tags": [
        "utils",
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_swap_row",
      "description": "'\nSwaps two rows of a CSC/CSR matrix in-place.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_swap_row",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_swap_row",
      "parameters": [
        {
          "description": "Matrix whose two rows are to be swapped.  m: int Index of the row of X to be swapped.  n: int Index of the row of X to be swapped. '",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.wishart_log_det",
      "description": "'DEPRECATED: The function wishart_log_det is deprecated in 0.18 and will be removed in 0.20.\n\nExpected value of the log of the determinant of a Wishart\n\nThe expected value of the logarithm of the determinant of a\nwishart-distributed random variable with the specified parameters.'",
      "id": "sklearn.utils.deprecation.wishart_log_det",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.wishart_log_det",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.regression_models.linear",
      "description": "'\nFirst order polynomial (linear, p = n+1) regression model.\n\nx --> f(x) = [ 1, x_1, ..., x_n ].T\n",
      "id": "sklearn.gaussian_process.regression_models.linear",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.regression_models.linear",
      "parameters": [
        {
          "description": "An array with shape (n_eval, n_features) giving the locations x at which the regression model should be evaluated. ",
          "name": "x",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, p) with the values of the regression model. '",
        "name": "f",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "regression_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.getfullargspec",
      "description": "'Compatibility function to provide inspect.getfullargspec in Python 2\n\nThis should be rewritten using a backport of Python 3 signature\nonce we drop support for Python 2.6. We went for a simpler\napproach at the time of writing because signature uses OrderedDict\nwhich is not available in Python 2.6.\n'",
      "id": "sklearn.externals.joblib.func_inspect.getfullargspec",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.getfullargspec",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.hashing.hash",
      "description": "\" Quick calculation of a hash to identify uniquely Python objects\ncontaining numpy arrays.\n\n",
      "id": "sklearn.externals.joblib.hashing.hash",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.hashing.hash",
      "parameters": [
        {
          "description": "Hashing algorithm used. sha1 is supposedly safer, but md5 is faster. coerce_mmap: boolean Make no difference between np.memmap and np.ndarray \"",
          "name": "hash_name",
          "type": ""
        }
      ],
      "tags": [
        "externals",
        "joblib",
        "hashing"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.get_func_name",
      "description": "' Return the function import path (as a list of module names), and\na name for the function.\n",
      "id": "sklearn.externals.joblib.func_inspect.get_func_name",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.get_func_name",
      "parameters": [
        {
          "description": "The func to inspect resolv_alias: boolean, optional If true, possible local aliases are indicated. win_characters: boolean, optional If true, substitute special characters using urllib.quote This is useful in Windows, as it cannot encode some filenames '",
          "name": "func",
          "type": "callable"
        }
      ],
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.auc",
      "description": "'Compute Area Under the Curve (AUC) using the trapezoidal rule\n\nThis is a general function, given points on a curve.  For computing the\narea under the ROC-curve, see :func:`roc_auc_score`.\n",
      "id": "sklearn.metrics.ranking.auc",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.auc",
      "parameters": [
        {
          "description": "x coordinates. ",
          "name": "x",
          "shape": "n",
          "type": "array"
        },
        {
          "description": "y coordinates. ",
          "name": "y",
          "shape": "n",
          "type": "array"
        },
        {
          "default": "False",
          "description": "If True, assume that the curve is ascending in the case of ties, as for an ROC curve. If the curve is non-ascending, the result will be wrong. ",
          "name": "reorder",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": " Examples -------- >>> import numpy as np >>> from sklearn import metrics >>> y = np.array([1, 1, 2, 2]) >>> pred = np.array([0.1, 0.4, 0.35, 0.8]) >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2) >>> metrics.auc(fpr, tpr) 0.75  See also -------- roc_auc_score : Computes the area under the ROC curve  precision_recall_curve : Compute precision-recall pairs for different probability thresholds  '",
        "name": "auc",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.rbf_kernel",
      "description": "'\nCompute the rbf (gaussian) kernel between X and Y::\n\nK(x, y) = exp(-gamma ||x-y||^2)\n\nfor each pair of rows x in X and y in Y.\n\nRead more in the :ref:`User Guide <rbf_kernel>`.\n",
      "id": "sklearn.metrics.pairwise.rbf_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.rbf_kernel",
      "parameters": [
        {
          "description": " Y : array of shape (n_samples_Y, n_features) ",
          "name": "X",
          "shape": "n_samples_X, n_features",
          "type": "array"
        },
        {
          "description": "If None, defaults to 1.0 / n_samples_X ",
          "name": "gamma",
          "type": "float"
        }
      ],
      "returns": {
        "description": "'",
        "name": "kernel_matrix",
        "shape": "n_samples_X, n_samples_Y",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.filter_args",
      "description": "\" Filters the given args and kwargs using a list of arguments to\nignore, and a function specification.\n",
      "id": "sklearn.externals.joblib.func_inspect.filter_args",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.filter_args",
      "parameters": [
        {
          "description": "Function giving the argument specification ignore_lst: list of strings List of arguments to ignore (either a name of an argument in the function spec, or '*', or '**') *args: list Positional arguments passed to the function. **kwargs: dict Keyword arguments passed to the function ",
          "name": "func",
          "type": "callable"
        }
      ],
      "returns": {
        "description": "List of filtered positional and keyword arguments. \"",
        "name": "filtered_args: list"
      },
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.register_parallel_backend",
      "description": "'Register a new Parallel backend factory.\n\nThe new backend can then be selected by passing its name as the backend\nargument to the Parallel class. Moreover, the default backend can be\noverwritten globally by setting make_default=True.\n\nThe factory can be any callable that takes no argument and return an\ninstance of ``ParallelBackendBase``.\n\nWarning: this function is experimental and subject to change in a future\nversion of joblib.\n\n.. versionadded:: 0.10\n\n'",
      "id": "sklearn.externals.joblib.parallel.register_parallel_backend",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.register_parallel_backend",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.ball_tree",
      "common_name": "Node Heap",
      "description": "'NodeHeap\n\nThis is a min-heap implementation for keeping track of nodes\nduring a breadth-first search.  Unlike the NeighborsHeap above,\nthe NodeHeap does not have a fixed size and must be able to grow\nas elements are added.\n\nInternally, the data is stored in a simple binary heap which meets\nthe min heap condition:\n\nheap[i].val < min(heap[2 * i + 1].val, heap[2 * i + 2].val)\n'",
      "id": "sklearn.neighbors.ball_tree.NodeHeap",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.ball_tree.NodeHeap",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "ball_tree"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.gen_batches",
      "description": "'Generator to create slices containing batch_size elements, from 0 to n.\n\nThe last slice may contain less than batch_size elements, when batch_size\ndoes not divide n.\n\nExamples\n--------\n>>> from sklearn.utils import gen_batches\n>>> list(gen_batches(7, 3))\n[slice(0, 3, None), slice(3, 6, None), slice(6, 7, None)]\n>>> list(gen_batches(6, 3))\n[slice(0, 3, None), slice(3, 6, None)]\n>>> list(gen_batches(2, 3))\n[slice(0, 2, None)]\n'",
      "id": "sklearn.utils.gen_batches",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.gen_batches",
      "parameters": [],
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.empirical_covariance_.empirical_covariance",
      "description": "'Computes the Maximum likelihood covariance estimator\n\n",
      "id": "sklearn.covariance.empirical_covariance_.empirical_covariance",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.empirical_covariance_.empirical_covariance",
      "parameters": [
        {
          "description": "Data from which to compute the covariance estimate ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "ndarray"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": ""
        }
      ],
      "returns": {
        "description": "Empirical covariance (Maximum Likelihood Estimator).  '",
        "name": "covariance",
        "shape": "n_features, n_features",
        "type": ""
      },
      "tags": [
        "covariance",
        "empirical_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.shrunk_covariance_.shrunk_covariance",
      "description": "'Calculates a covariance matrix shrunk on the diagonal\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n",
      "id": "sklearn.covariance.shrunk_covariance_.shrunk_covariance",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.shrunk_covariance_.shrunk_covariance",
      "parameters": [
        {
          "description": "Covariance matrix to be shrunk ",
          "name": "emp_cov",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Coefficient in the convex combination used for the computation of the shrunk estimate. ",
          "name": "shrinkage",
          "type": "float"
        }
      ],
      "returns": {
        "description": "Shrunk covariance.  Notes ----- The regularized (shrunk) covariance is given by  (1 - shrinkage)*cov + shrinkage*mu*np.identity(n_features)  where mu = trace(cov) / n_features  '",
        "name": "shrunk_cov",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle_compat.load_compatibility",
      "description": "'Reconstruct a Python object from a file persisted with joblib.dump.\n\nThis function ensures the compatibility with joblib old persistence format\n(<= 0.9.3).\n",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.load_compatibility",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle_compat.load_compatibility",
      "parameters": [
        {
          "description": "The name of the file from which to load the object ",
          "name": "filename",
          "type": "string"
        }
      ],
      "returns": {
        "description": "The object stored in the file.  See Also -------- joblib.dump : function to save an object  Notes -----  This function can load numpy array files saved separately during the dump. '",
        "name": "result: any Python object"
      },
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.cosine_distances",
      "description": "'Compute cosine distance between samples in X and Y.\n\nCosine distance is defined as 1.0 minus the cosine similarity.\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.cosine_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.cosine_distances",
      "parameters": [
        {
          "description": "with shape (n_samples_X, n_features).  Y : array_like, sparse matrix (optional) with shape (n_samples_Y, n_features). ",
          "name": "X",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_samples_X, n_samples_Y).  See also -------- sklearn.metrics.pairwise.cosine_similarity scipy.spatial.distance.cosine (dense matrices only) '",
        "name": "distance matrix",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.pure_nugget",
      "description": "\"\nSpatial independence correlation model (pure nugget).\n(Useful when one wants to solve an ordinary least squares problem!)::\n\nn\ntheta, d --> r(theta, d) = 1 if   sum |d_i| == 0\ni = 1\n0 otherwise\n",
      "id": "sklearn.gaussian_process.correlation_models.pure_nugget",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.pure_nugget",
      "parameters": [
        {
          "description": "None. ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) with the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.metaestimators.if_delegate_has_method",
      "description": "'Create a decorator for methods that are delegated to a sub-estimator\n\nThis enables ducktyping by hasattr returning True according to the\nsub-estimator.\n",
      "id": "sklearn.utils.metaestimators.if_delegate_has_method",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.metaestimators.if_delegate_has_method",
      "parameters": [
        {
          "description": "Name of the sub-estimator that can be accessed as an attribute of the base object. If a list or a tuple of names are provided, the first sub-estimator that is an attribute of the base object  will be used.  '",
          "name": "delegate",
          "type": "string"
        }
      ],
      "tags": [
        "utils",
        "metaestimators"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.inplace_column_scale",
      "description": "'Inplace column scaling of a CSC/CSR matrix.\n\nScale each feature of the data matrix by multiplying with specific scale\nprovided by the caller assuming a (n_samples, n_features) shape.\n",
      "id": "sklearn.utils.sparsefuncs.inplace_column_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.inplace_column_scale",
      "parameters": [
        {
          "description": "Matrix to normalize using the variance of the features.  scale: float array with shape (n_features,) Array of precomputed feature-wise values to use for scaling. '",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.pipeline.make_pipeline",
      "description": "\"Construct a Pipeline from the given estimators.\n\nThis is a shorthand for the Pipeline constructor; it does not require, and\ndoes not permit, naming the estimators. Instead, their names will be set\nto the lowercase of their types automatically.\n\nExamples\n--------\n>>> from sklearn.naive_bayes import GaussianNB\n>>> from sklearn.preprocessing import StandardScaler\n>>> make_pipeline(StandardScaler(), GaussianNB(priors=None))\n...     # doctest: +NORMALIZE_WHITESPACE\nPipeline(steps=[('standardscaler',\nStandardScaler(copy=True, with_mean=True, with_std=True)),\n('gaussiannb', GaussianNB(priors=None))])\n",
      "id": "sklearn.pipeline.make_pipeline",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.pipeline.make_pipeline",
      "parameters": [],
      "returns": {
        "description": "\"",
        "name": "p",
        "type": ""
      },
      "tags": [
        "pipeline"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.random_projection.gaussian_random_matrix",
      "description": "' Generate a dense Gaussian random matrix.\n\nThe components of the random matrix are drawn from\n\nN(0, 1.0 / n_components).\n\nRead more in the :ref:`User Guide <gaussian_random_matrix>`.\n",
      "id": "sklearn.random_projection.gaussian_random_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.random_projection.gaussian_random_matrix",
      "parameters": [
        {
          "description": "Dimensionality of the target projection space. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Dimensionality of the original source space. ",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "Control the pseudo random number generator used to generate the matrix at fit time. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated Gaussian random matrix.  See Also -------- GaussianRandomProjection sparse_random_matrix '",
        "name": "components",
        "shape": "n_components, n_features",
        "type": "numpy"
      },
      "tags": [
        "random_projection"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.ball_tree",
      "common_name": "Neighbors Heap",
      "description": "'A max-heap structure to keep track of distances/indices of neighbors\n\nThis implements an efficient pre-allocated set of fixed-size heaps\nfor chasing neighbors, holding both an index and a distance.\nWhen any row of the heap is full, adding an additional point will push\nthe furthest point off the heap.\n",
      "id": "sklearn.neighbors.ball_tree.NeighborsHeap",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.ball_tree.NeighborsHeap",
      "parameters": [
        {
          "description": "the number of heaps to use",
          "name": "n_pts",
          "type": "int"
        },
        {
          "description": "the size of each heap. '",
          "name": "n_nbrs",
          "type": "int"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "ball_tree"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.approximate",
      "common_name": "Gaussian Random Projection Hash",
      "description": "'Use GaussianRandomProjection to produce a cosine LSH fingerprint'",
      "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Generate a sparse random projection matrix\n",
          "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": " '",
            "name": "self"
          }
        },
        {
          "description": "None",
          "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash.fit_transform",
          "name": "fit_transform",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "None",
          "id": "sklearn.neighbors.approximate.GaussianRandomProjectionHash.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.neighbors.approximate.GaussianRandomProjectionHash",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/approximate.pyc:92",
      "tags": [
        "neighbors",
        "approximate"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.base",
      "common_name": "Sparse Coef Mixin",
      "description": "'Mixin for converting coef_ to and from CSR format.\n\nL1-regularizing estimators should inherit this.\n'",
      "id": "sklearn.linear_model.base.SparseCoefMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.base.SparseCoefMixin.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.base.SparseCoefMixin.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.base.SparseCoefMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc:363",
      "tags": [
        "linear_model",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Leave P Label Out",
      "description": "'Leave-P-Label_Out cross-validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.LeavePGroupsOut` instead.\n\nProvides train/test indices to split data according to a third-party\nprovided label. This label information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the labels could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePLabelOut and LeaveOneLabelOut is that\nthe former builds the test sets with all the samples assigned to\n``p`` different values of the labels while the latter uses samples\nall assigned the same labels.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.LeavePLabelOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LeavePLabelOut",
      "parameters": [
        {
          "description": "Arbitrary domain-specific stratification of the data to be used to draw the splits. ",
          "name": "labels",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Number of samples to leave out in the test split. ",
          "name": "p",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:675",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "covariance.outlier_detection",
      "common_name": "Outlier Detection Mixin",
      "description": "'Set of methods for outliers detection with covariance estimators.\n",
      "id": "sklearn.covariance.outlier_detection.OutlierDetectionMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the decision function of the given observations.\n",
          "id": "sklearn.covariance.outlier_detection.OutlierDetectionMixin.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools. ",
              "name": "raw_values",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "The values of the decision function for each observations. It is equal to the Mahalanobis distances if `raw_values` is True. By default (``raw_values=True``), it is equal to the cubic root of the shifted Mahalanobis distances. In that case, the threshold for being an outlier is 0, which ensures a compatibility with other outlier detection tools such as the One-Class SVM.  '",
            "name": "decision",
            "shape": "n_samples, ",
            "type": "array-like"
          }
        },
        {
          "description": "\"Outlyingness of observations in X according to the fitted model.\n",
          "id": "sklearn.covariance.outlier_detection.OutlierDetectionMixin.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each observations, tells whether or not it should be considered as an outlier according to the fitted model.  threshold : float, The values of the less outlying point's decision function.  \"",
            "name": "is_outliers",
            "shape": "n_samples, ",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.covariance.outlier_detection.OutlierDetectionMixin",
      "parameters": [
        {
          "description": "The amount of contamination of the data set, i.e. the proportion of outliers in the data set. ",
          "name": "contamination",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/outlier_detection.pyc:22",
      "tags": [
        "covariance",
        "outlier_detection"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "exceptions",
      "common_name": "Fit Failed Warning",
      "description": "'Warning class used if there is an error while fitting the estimator.\n\nThis Warning is used in meta estimators GridSearchCV and RandomizedSearchCV\nand the cross-validation helper function cross_val_score to warn when there\nis an error while fitting the estimator.\n\nExamples\n--------\n>>> from sklearn.model_selection import GridSearchCV\n>>> from sklearn.svm import LinearSVC\n>>> from sklearn.exceptions import FitFailedWarning\n>>> import warnings\n>>> warnings.simplefilter(\\'always\\', FitFailedWarning)\n>>> gs = GridSearchCV(LinearSVC(), {\\'C\\': [-1, -2]}, error_score=0)\n>>> X, y = [[1, 2], [3, 4], [5, 6], [7, 8], [8, 9]], [0, 0, 0, 1, 1]\n>>> with warnings.catch_warnings(record=True) as w:\n...     try:\n...         gs.fit(X, y)   # This will raise a ValueError since C is < 0\n...     except ValueError:\n...         pass\n...     print(repr(w[-1].message))\n... # doctest: +NORMALIZE_WHITESPACE\nFitFailedWarning(\"Classifier fit failed. The score on this train-test\npartition for these parameters will be set to 0.000000. Details:\n\\\nValueError(\\'Penalty term must be positive; got (C=-2)\\',)\",)\n\n.. versionchanged:: 0.18\nMoved from sklearn.cross_validation.\n'",
      "id": "sklearn.exceptions.FitFailedWarning",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.exceptions.FitFailedWarning",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/exceptions.pyc:98",
      "tags": [
        "exceptions"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Kernel Operator",
      "description": "'Base class for all kernel operators.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.gaussian_process.kernels.KernelOperator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.KernelOperator.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.KernelOperator.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.KernelOperator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.KernelOperator.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.KernelOperator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.KernelOperator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:530",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Huber Loss Function",
      "description": "'Huber loss function for robust regression.\n\nM-Regression proposed in Friedman 2001.\n\nReferences\n----------\nJ. Friedman, Greedy Function Approximation: A Gradient Boosting\nMachine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n'",
      "id": "sklearn.ensemble.gradient_boosting.HuberLossFunction",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.HuberLossFunction.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.HuberLossFunction.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.HuberLossFunction.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.HuberLossFunction",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:331",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Regression Loss Function",
      "description": "'Base class for regression loss functions. '",
      "id": "sklearn.ensemble.gradient_boosting.RegressionLossFunction",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Default ``init`` estimator for loss function. '",
          "id": "sklearn.ensemble.gradient_boosting.RegressionLossFunction.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'Compute the negative gradient.\n",
          "id": "sklearn.ensemble.gradient_boosting.RegressionLossFunction.negative_gradient",
          "name": "negative_gradient",
          "parameters": [
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "np"
            },
            {
              "description": "The predictions. '",
              "name": "y_pred",
              "shape": "n,",
              "type": "np"
            }
          ]
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.RegressionLossFunction.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.RegressionLossFunction",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:264",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "linear_model.stochastic_gradient",
      "common_name": "Base SGD",
      "description": "'Base class for SGD classification and regression.'",
      "id": "sklearn.linear_model.stochastic_gradient.BaseSGD",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGD.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit model.'",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGD.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGD.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGD.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGD.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.stochastic_gradient.BaseSGD",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc:46",
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Threading Backend",
      "description": "'A ParallelBackend which will use a thread pool to execute batches in.\n\nThis is a low-overhead backend but it suffers from the Python Global\nInterpreter Lock if the called function relies a lot on Python objects.\nMostly useful when the execution bottleneck is a compiled extension that\nexplicitly releases the GIL (for instance a Cython loop wrapped in a\n\"with nogil\" block or an expensive call to a library such as NumPy).\n'",
      "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Shutdown the pool and restart a new one with the same parameters'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.abort_everything",
          "name": "abort_everything",
          "parameters": []
        },
        {
          "description": "'Schedule a func to be run'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "'Callback indicate how long it took to run a batch'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.batch_completed",
          "name": "batch_completed",
          "parameters": []
        },
        {
          "description": "'Determine the optimal batch size'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.compute_batch_size",
          "name": "compute_batch_size",
          "parameters": []
        },
        {
          "description": "'Build a process or thread pool and return the number of workers'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.configure",
          "name": "configure",
          "parameters": []
        },
        {
          "description": "'Determine the number of jobs which are going to run in parallel'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.effective_n_jobs",
          "name": "effective_n_jobs",
          "parameters": []
        },
        {
          "description": "'List of exception types to be captured.'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.get_exceptions",
          "name": "get_exceptions",
          "parameters": []
        },
        {
          "description": "'Shutdown the process or thread pool'",
          "id": "sklearn.externals.joblib._parallel_backends.ThreadingBackend.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib._parallel_backends.ThreadingBackend",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:229",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Multiprocessing Backend",
      "description": "'A ParallelBackend which will use a multiprocessing.Pool.\n\nWill introduce some communication and memory overhead when exchanging\ninput and output data with the with the worker Python processes.\nHowever, does not suffer from the Python Global Interpreter Lock.\n'",
      "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Shutdown the pool and restart a new one with the same parameters'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.abort_everything",
          "name": "abort_everything",
          "parameters": []
        },
        {
          "description": "'Schedule a func to be run'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "'Callback indicate how long it took to run a batch'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.batch_completed",
          "name": "batch_completed",
          "parameters": []
        },
        {
          "description": "'Determine the optimal batch size'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.compute_batch_size",
          "name": "compute_batch_size",
          "parameters": []
        },
        {
          "description": "'Build a process or thread pool and return the number of workers'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.configure",
          "name": "configure",
          "parameters": []
        },
        {
          "description": "'Determine the number of jobs which are going to run in parallel.\n\nThis also checks if we are attempting to create a nested parallel\nloop.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.effective_n_jobs",
          "name": "effective_n_jobs",
          "parameters": []
        },
        {
          "description": "'List of exception types to be captured.'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.get_exceptions",
          "name": "get_exceptions",
          "parameters": []
        },
        {
          "description": "'Shutdown the process or thread pool'",
          "id": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib._parallel_backends.MultiprocessingBackend",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:250",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Stratified Shuffle Split",
      "description": "'Stratified ShuffleSplit cross validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.StratifiedShuffleSplit` instead.\n\nProvides train/test indices to split data in train test sets.\n\nThis cross-validation object is a merge of StratifiedKFold and\nShuffleSplit, which returns stratified randomized folds. The folds\nare made by preserving the percentage of samples for each class.\n\nNote: like the ShuffleSplit strategy, stratified random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.StratifiedShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.StratifiedShuffleSplit",
      "parameters": [
        {
          "description": "Labels of samples. ",
          "name": "y",
          "type": "array"
        },
        {
          "description": "Number of re-shuffling & splitting iterations. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:995",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "The number of regression trees to be induced; 1 for regression and binary classification; ``n_classes`` for multi-class classification.",
          "name": "K",
          "type": "int"
        }
      ],
      "category": "ensemble.gradient_boosting",
      "common_name": "Loss Function",
      "description": "'Abstract base class for various loss functions.\n\nAttributes\n----------\nK : int\nThe number of regression trees to be induced;\n1 for regression and binary classification;\n``n_classes`` for multi-class classification.\n'",
      "id": "sklearn.ensemble.gradient_boosting.LossFunction",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Default ``init`` estimator for loss function. '",
          "id": "sklearn.ensemble.gradient_boosting.LossFunction.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'Compute the negative gradient.\n",
          "id": "sklearn.ensemble.gradient_boosting.LossFunction.negative_gradient",
          "name": "negative_gradient",
          "parameters": [
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "np"
            },
            {
              "description": "The predictions. '",
              "name": "y_pred",
              "shape": "n,",
              "type": "np"
            }
          ]
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.LossFunction.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.LossFunction",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:175",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification",
        "decision tree"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Multinomial Deviance",
      "description": "'Multinomial deviance loss function for multi-class classification.\n\nFor multi-class classification we need to fit ``n_classes`` trees at\neach stage.\n'",
      "id": "sklearn.ensemble.gradient_boosting.MultinomialDeviance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.MultinomialDeviance.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'Compute negative gradient for the ``k``-th class. '",
          "id": "sklearn.ensemble.gradient_boosting.MultinomialDeviance.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.MultinomialDeviance.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.MultinomialDeviance",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:530",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The base estimator from which the ensemble is grown. ",
          "name": "base_estimator_",
          "type": "estimator"
        },
        {
          "description": "The collection of fitted base estimators.",
          "name": "estimators_",
          "type": "list"
        }
      ],
      "category": "ensemble.base",
      "common_name": "Base Ensemble",
      "description": "'Base class for all ensemble classes.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n",
      "id": "sklearn.ensemble.base.BaseEnsemble",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.base.BaseEnsemble.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.base.BaseEnsemble.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.base.BaseEnsemble",
      "parameters": [
        {
          "default": "None",
          "description": "The base estimator from which the ensemble is built. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "description": "The number of estimators in the ensemble. ",
          "name": "n_estimators",
          "type": "integer"
        },
        {
          "description": "The list of attributes to use as parameters when instantiating a new base estimator. If none are given, default parameters are used. ",
          "name": "estimator_params",
          "type": "list"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/base.pyc:55",
      "tags": [
        "ensemble",
        "base"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.funcsigs",
      "common_name": "Signature",
      "description": "\"A Signature object represents the overall signature of a function.\nIt stores a Parameter object for each parameter accepted by the\nfunction, as well as information specific to the function itself.\n\nA Signature object has the following public attributes and methods:\n\n* parameters : OrderedDict\nAn ordered mapping of parameters' names to the corresponding\nParameter objects (keyword-only arguments are in the same order\nas listed in `code.co_varnames`).\n* return_annotation : object\nThe annotation for the return type of the function if specified.\nIf the function has no annotation for its return type, this\nattribute is not set.\n* bind(*args, **kwargs) -> BoundArguments\nCreates a mapping from positional and keyword arguments to\nparameters.\n* bind_partial(*args, **kwargs) -> BoundArguments\nCreates a partial mapping from positional and keyword arguments\nto parameters (simulating 'functools.partial' behavior.)\n\"",
      "id": "sklearn.externals.funcsigs.Signature",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Get a BoundArguments object, that maps the passed `args`\nand `kwargs` to the function's signature.  Raises `TypeError`\nif the passed arguments can not be bound.\n\"",
          "id": "sklearn.externals.funcsigs.Signature.bind",
          "name": "bind",
          "parameters": []
        },
        {
          "description": "\"Get a BoundArguments object, that partially maps the\npassed `args` and `kwargs` to the function's signature.\nRaises `TypeError` if the passed arguments can not be bound.\n\"",
          "id": "sklearn.externals.funcsigs.Signature.bind_partial",
          "name": "bind_partial",
          "parameters": []
        },
        {
          "description": "'Constructs Signature for the given python function'",
          "id": "sklearn.externals.funcsigs.Signature.from_function",
          "name": "from_function",
          "parameters": []
        },
        {
          "description": "\"Creates a customized copy of the Signature.\nPass 'parameters' and/or 'return_annotation' arguments\nto override them in the new copy.\n\"",
          "id": "sklearn.externals.funcsigs.Signature.replace",
          "name": "replace",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.funcsigs.Signature",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/funcsigs.pyc:444",
      "tags": [
        "externals",
        "funcsigs"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Product",
      "description": "'Product-kernel k1 * k2 of two kernels k1 and k2.\n\nThe resulting kernel is defined as\nk_prod(X, Y) = k1(X, Y) * k2(X, Y)\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.Product",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.Product.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.Product.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.Product.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.Product.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.Product.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.Product",
      "parameters": [
        {
          "description": "The first base-kernel of the product-kernel  k2 : Kernel object The second base-kernel of the product-kernel  '",
          "name": "k1",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:708",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "White Kernel",
      "description": "'White kernel.\n\nThe main use-case of this kernel is as part of a sum-kernel where it\nexplains the noise-component of the signal. Tuning its parameter\ncorresponds to estimating the noise-level.\n\nk(x_1, x_2) = noise_level if x_1 == x_2 else 0\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.WhiteKernel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.WhiteKernel.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.WhiteKernel.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.WhiteKernel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.WhiteKernel.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.WhiteKernel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.WhiteKernel",
      "parameters": [
        {
          "description": "Parameter controlling the noise level ",
          "name": "noise_level",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on noise_level  '",
          "name": "noise_level_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1035",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "grid_search",
      "common_name": "Base Search CV",
      "description": "'Base class for hyper parameter search with cross-validation.'",
      "id": "sklearn.grid_search.BaseSearchCV",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.grid_search.BaseSearchCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n",
          "id": "sklearn.grid_search.BaseSearchCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Input data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- * The long-standing behavior of this method changed in version 0.16. * It no longer uses the metric provided by ``estimator.score`` if the ``scoring`` parameter was set when fitting.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.grid_search.BaseSearchCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.grid_search.BaseSearchCV",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc:377",
      "tags": [
        "grid_search"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cluster.bicluster",
      "common_name": "Base Spectral",
      "description": "'Base class for spectral biclustering.'",
      "id": "sklearn.cluster.bicluster.BaseSpectral",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Creates a biclustering for X.\n",
          "id": "sklearn.cluster.bicluster.BaseSpectral.fit",
          "name": "fit",
          "parameters": [
            {
              "description": " '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "\"Row and column indices of the i'th bicluster.\n\nOnly works if ``rows_`` and ``columns_`` attributes exist.\n",
          "id": "sklearn.cluster.bicluster.BaseSpectral.get_indices",
          "name": "get_indices",
          "parameters": [],
          "returns": {
            "description": "Indices of rows in the dataset that belong to the bicluster. col_ind : np.array, dtype=np.intp Indices of columns in the dataset that belong to the bicluster.  \"",
            "name": "row_ind",
            "type": "np"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.bicluster.BaseSpectral.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Shape of the i'th bicluster.\n",
          "id": "sklearn.cluster.bicluster.BaseSpectral.get_shape",
          "name": "get_shape",
          "parameters": [],
          "returns": {
            "description": "Number of rows and columns (resp.) in the bicluster. \"",
            "name": "shape",
            "type": ""
          }
        },
        {
          "description": "'Returns the submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if ``rows_`` and\n``columns_`` attributes exist.\n\n'",
          "id": "sklearn.cluster.bicluster.BaseSpectral.get_submatrix",
          "name": "get_submatrix",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.bicluster.BaseSpectral.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.bicluster.BaseSpectral",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/bicluster.pyc:89",
      "tags": [
        "cluster",
        "bicluster"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.format_stack.uniq_stable",
      "description": "'uniq_stable(elems) -> list\n\nReturn from an iterable, a list of all the unique elements in the input,\nbut maintaining the order in which they first appear.\n\nA naive solution to this problem which just makes a dictionary with the\nelements as keys fails to respect the stability condition, since\ndictionaries are unsorted by nature.\n\nNote: All elements in the input must be hashable.\n'",
      "id": "sklearn.externals.joblib.format_stack.uniq_stable",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.format_stack.uniq_stable",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "format_stack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.regression_models.quadratic",
      "description": "'\nSecond order polynomial (quadratic, p = n*(n-1)/2+n+1) regression model.\n\nx --> f(x) = [ 1, { x_i, i = 1,...,n }, { x_i * x_j,  (i,j) = 1,...,n } ].T\ni > j\n",
      "id": "sklearn.gaussian_process.regression_models.quadratic",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.regression_models.quadratic",
      "parameters": [
        {
          "description": "An array with shape (n_eval, n_features) giving the locations x at which the regression model should be evaluated. ",
          "name": "x",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, p) with the values of the regression model. '",
        "name": "f",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "regression_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.center_data",
      "description": "'DEPRECATED: center_data was deprecated in version 0.18 and will be removed in 0.20. Use utilities in preprocessing.data instead\n\n\nCenters data to have mean zero along axis 0. This is here because\nnearly all linear models will want their data to be centered.\nIf sample_weight is not None, then the weighted mean of X and y\nis zero, and not the mean itself\n'",
      "id": "sklearn.utils.deprecation.center_data",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.center_data",
      "parameters": [],
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "functools.update_wrapper",
      "description": "'Update a wrapper function to look like the wrapped function\n\nwrapper is the function to be updated\nwrapped is the original function\nassigned is a tuple naming the attributes assigned directly\nfrom the wrapped function to the wrapper function (defaults to\nfunctools.WRAPPER_ASSIGNMENTS)\nupdated is a tuple naming the attributes of the wrapper that\nare updated with the corresponding attribute from the wrapped\nfunction (defaults to functools.WRAPPER_UPDATES)\n'",
      "id": "functools.update_wrapper",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "functools.update_wrapper",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.multiclass.is_multilabel",
      "description": "' Check if ``y`` is in a multilabel format.\n",
      "id": "sklearn.utils.multiclass.is_multilabel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.multiclass.is_multilabel",
      "parameters": [
        {
          "description": "Target values. ",
          "name": "y",
          "shape": "n_samples",
          "type": "numpy"
        }
      ],
      "returns": {
        "description": "Return ``True``, if ``y`` is in a multilabel format, else ```False``.  Examples -------- >>> import numpy as np >>> from sklearn.utils.multiclass import is_multilabel >>> is_multilabel([0, 1, 0, 1]) False >>> is_multilabel([[1], [0, 2], []]) False >>> is_multilabel(np.array([[1, 0], [0, 0]])) True >>> is_multilabel(np.array([[1], [0], [0]])) False >>> is_multilabel(np.array([[1, 0, 0]])) True '",
        "name": "out",
        "type": "bool"
      },
      "tags": [
        "utils",
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.kernel_metrics",
      "description": "\" Valid metrics for pairwise_kernels\n\nThis function simply returns the valid pairwise distance metrics.\nIt exists, however, to allow for a verbose description of the mapping for\neach of the valid strings.\n\nThe valid distance metrics, and the function they map to, are:\n===============   ========================================\nmetric            Function\n===============   ========================================\n'additive_chi2'   sklearn.pairwise.additive_chi2_kernel\n'chi2'            sklearn.pairwise.chi2_kernel\n'linear'          sklearn.pairwise.linear_kernel\n'poly'            sklearn.pairwise.polynomial_kernel\n'polynomial'      sklearn.pairwise.polynomial_kernel\n'rbf'             sklearn.pairwise.rbf_kernel\n'laplacian'       sklearn.pairwise.laplacian_kernel\n'sigmoid'         sklearn.pairwise.sigmoid_kernel\n'cosine'          sklearn.pairwise.cosine_similarity\n===============   ========================================\n\nRead more in the :ref:`User Guide <metrics>`.\n\"",
      "id": "sklearn.metrics.pairwise.kernel_metrics",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.kernel_metrics",
      "parameters": [],
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.empirical_covariance_.log_likelihood",
      "description": "'Computes the sample mean of the log_likelihood under a covariance model\n\ncomputes the empirical expected log-likelihood (accounting for the\nnormalization terms and scaling), allowing for universal comparison (beyond\nthis software package)\n",
      "id": "sklearn.covariance.empirical_covariance_.log_likelihood",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.empirical_covariance_.log_likelihood",
      "parameters": [
        {
          "description": "Maximum Likelihood Estimator of covariance ",
          "name": "emp_cov",
          "type": ""
        },
        {
          "description": "The precision matrix of the covariance model to be tested ",
          "name": "precision",
          "type": ""
        }
      ],
      "returns": {
        "description": "'",
        "name": "sample mean of the log-likelihood"
      },
      "tags": [
        "covariance",
        "empirical_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.graph_lasso_.alpha_max",
      "description": "'Find the maximum alpha for which there are some non-zeros off-diagonal.\n",
      "id": "sklearn.covariance.graph_lasso_.alpha_max",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.graph_lasso_.alpha_max",
      "parameters": [
        {
          "description": "The sample covariance matrix  Notes -----  This results from the bound for the all the Lasso that are solved in GraphLasso: each time, the row of cov corresponds to Xy. As the bound for alpha is given by `max(abs(Xy))`, the result follows.  '",
          "name": "emp_cov",
          "type": ""
        }
      ],
      "tags": [
        "covariance",
        "graph_lasso_"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Hasher",
      "description": "' A subclass of pickler, to do cryptographic hashing, rather than\npickling.\n'",
      "id": "sklearn.externals.joblib.hashing.Hasher",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Clears the pickler\\'s \"memo\".\n\nThe memo is the data structure that remembers which objects the\npickler has already seen, so that shared or recursive objects are\npickled by reference and not by value.  This method is useful when\nre-using picklers.\n\n'",
          "id": "sklearn.externals.joblib.hashing.Hasher.clear_memo",
          "name": "clear_memo",
          "parameters": []
        },
        {
          "description": "'Write a pickled representation of obj to the open file.'",
          "id": "sklearn.externals.joblib.hashing.Hasher.dump",
          "name": "dump",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.get",
          "name": "get",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.hash",
          "name": "hash",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.memoize",
          "name": "memoize",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.persistent_id",
          "name": "persistent_id",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.put",
          "name": "put",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save",
          "name": "save",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_bool",
          "name": "save_bool",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_dict",
          "name": "save_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_empty_tuple",
          "name": "save_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_float",
          "name": "save_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_global",
          "name": "save_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_inst",
          "name": "save_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_int",
          "name": "save_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_list",
          "name": "save_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_long",
          "name": "save_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_none",
          "name": "save_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_pers",
          "name": "save_pers",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_reduce",
          "name": "save_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_set",
          "name": "save_set",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_string",
          "name": "save_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_tuple",
          "name": "save_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.Hasher.save_unicode",
          "name": "save_unicode",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.hashing.Hasher",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.pyc:51",
      "tags": [
        "externals",
        "joblib",
        "hashing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Numpy Hasher",
      "description": "' Special case the hasher for when numpy is loaded.\n'",
      "id": "sklearn.externals.joblib.hashing.NumpyHasher",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Clears the pickler\\'s \"memo\".\n\nThe memo is the data structure that remembers which objects the\npickler has already seen, so that shared or recursive objects are\npickled by reference and not by value.  This method is useful when\nre-using picklers.\n\n'",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.clear_memo",
          "name": "clear_memo",
          "parameters": []
        },
        {
          "description": "'Write a pickled representation of obj to the open file.'",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.dump",
          "name": "dump",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.get",
          "name": "get",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.hash",
          "name": "hash",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.memoize",
          "name": "memoize",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.persistent_id",
          "name": "persistent_id",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.put",
          "name": "put",
          "parameters": []
        },
        {
          "description": "' Subclass the save method, to hash ndarray subclass, rather\nthan pickling them. Off course, this is a total abuse of\nthe Pickler class.\n'",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save",
          "name": "save",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_bool",
          "name": "save_bool",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_dict",
          "name": "save_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_empty_tuple",
          "name": "save_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_float",
          "name": "save_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_global",
          "name": "save_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_inst",
          "name": "save_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_int",
          "name": "save_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_list",
          "name": "save_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_long",
          "name": "save_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_none",
          "name": "save_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_pers",
          "name": "save_pers",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_reduce",
          "name": "save_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_set",
          "name": "save_set",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_string",
          "name": "save_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_tuple",
          "name": "save_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.hashing.NumpyHasher.save_unicode",
          "name": "save_unicode",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.hashing.NumpyHasher",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/hashing.pyc:160",
      "tags": [
        "externals",
        "joblib",
        "hashing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Exponentiation",
      "description": "'Exponentiate kernel by given exponent.\n\nThe resulting kernel is defined as\nk_exp(X, Y) = k(X, Y) ** exponent\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.Exponentiation",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.Exponentiation.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.Exponentiation.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.Exponentiation.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.Exponentiation.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.Exponentiation.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.Exponentiation",
      "parameters": [
        {
          "description": "The base kernel ",
          "name": "kernel",
          "type": ""
        },
        {
          "description": "The exponent for the base kernel  '",
          "name": "exponent",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:783",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Kernel",
      "description": "'Base class for all kernels.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.gaussian_process.kernels.Kernel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.Kernel.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.Kernel.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.Kernel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.Kernel.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.Kernel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.Kernel",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:119",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Binomial Deviance",
      "description": "'Binomial deviance loss function for binary classification.\n\nBinary classification is a special case; here, we only need to\nfit one tree instead of ``n_classes`` trees.\n'",
      "id": "sklearn.ensemble.gradient_boosting.BinomialDeviance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.BinomialDeviance.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'Compute the residual (= negative gradient). '",
          "id": "sklearn.ensemble.gradient_boosting.BinomialDeviance.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.BinomialDeviance.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.BinomialDeviance",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:466",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Classification Loss Function",
      "description": "'Base class for classification loss functions. '",
      "id": "sklearn.ensemble.gradient_boosting.ClassificationLossFunction",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Default ``init`` estimator for loss function. '",
          "id": "sklearn.ensemble.gradient_boosting.ClassificationLossFunction.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "'Compute the negative gradient.\n",
          "id": "sklearn.ensemble.gradient_boosting.ClassificationLossFunction.negative_gradient",
          "name": "negative_gradient",
          "parameters": [
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "np"
            },
            {
              "description": "The predictions. '",
              "name": "y_pred",
              "shape": "n,",
              "type": "np"
            }
          ]
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.ClassificationLossFunction.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.ClassificationLossFunction",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:448",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.bagging",
      "common_name": "Base Bagging",
      "description": "'Base class for Bagging meta-estimator.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n'",
      "id": "sklearn.ensemble.bagging.BaseBagging",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Build a Bagging ensemble of estimators from the training\nset (X, y).\n",
          "id": "sklearn.ensemble.bagging.BaseBagging.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.bagging.BaseBagging.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.bagging.BaseBagging.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.bagging.BaseBagging",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/bagging.pyc:190",
      "tags": [
        "ensemble",
        "bagging"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "multioutput",
      "common_name": "Multi Output Estimator",
      "description": "None",
      "id": "sklearn.multioutput.MultiOutputEstimator",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Fit the model to data.\nFit a separate model for each output variable.\n",
          "id": "sklearn.multioutput.MultiOutputEstimator.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-output targets. An indicator matrix turns on multilabel estimation. ",
              "name": "y",
              "shape": "n_samples, n_outputs",
              "type": ""
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multioutput.MultiOutputEstimator.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict multi-output variable using a model\ntrained for each target variable.\n",
          "id": "sklearn.multioutput.MultiOutputEstimator.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor. '",
            "name": "y",
            "shape": "n_samples, n_outputs",
            "type": ""
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multioutput.MultiOutputEstimator.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multioutput.MultiOutputEstimator",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multioutput.pyc:40",
      "tags": [
        "multioutput"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.base",
      "common_name": "Linear Model",
      "description": "'Base class for Linear Models'",
      "id": "sklearn.linear_model.base.LinearModel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.base.LinearModel.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit model.'",
          "id": "sklearn.linear_model.base.LinearModel.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.base.LinearModel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.base.LinearModel.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.base.LinearModel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.base.LinearModel",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc:225",
      "tags": [
        "linear_model",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "random_projection",
      "common_name": "Base Random Projection",
      "description": "'Base class for random projections.\n\nWarning: This class should not be used directly.\nUse derived classes instead.\n'",
      "id": "sklearn.random_projection.BaseRandomProjection",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Generate a sparse random projection matrix\n",
          "id": "sklearn.random_projection.BaseRandomProjection.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": " '",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.random_projection.BaseRandomProjection.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.random_projection.BaseRandomProjection.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.random_projection.BaseRandomProjection.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Project the data by using matrix product with the random matrix\n",
          "id": "sklearn.random_projection.BaseRandomProjection.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input data to project into a smaller dimensional space. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": "Projected array.  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.random_projection.BaseRandomProjection",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/random_projection.pyc:290",
      "tags": [
        "random_projection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_sample_image",
      "description": "\"Load the numpy array of a single sample image\n",
      "id": "sklearn.datasets.base.load_sample_image",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_sample_image",
      "parameters": [
        {
          "description": "The name of the sample image loaded ",
          "name": "image_name",
          "type": "`china.jpg`, `flower.jpg`"
        }
      ],
      "returns": {
        "description": "The image as a numpy array: height x width x color  Examples ---------  >>> from sklearn.datasets import load_sample_image >>> china = load_sample_image('china.jpg')   # doctest: +SKIP >>> china.dtype                              # doctest: +SKIP dtype('uint8') >>> china.shape                              # doctest: +SKIP (427, 640, 3) >>> flower = load_sample_image('flower.jpg') # doctest: +SKIP >>> flower.dtype                             # doctest: +SKIP dtype('uint8') >>> flower.shape                             # doctest: +SKIP (427, 640, 3) \"",
        "name": "img: 3D array"
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.disk.rm_subdirs",
      "description": "'Remove all subdirectories in this path.\n\nThe directory indicated by `path` is left in place, and its subdirectories\nare erased.\n\nIf onerror is set, it is called to handle the error with arguments (func,\npath, exc_info) where func is os.listdir, os.remove, or os.rmdir;\npath is the argument to that function that caused it to fail; and\nexc_info is a tuple returned by sys.exc_info().  If onerror is None,\nan exception is raised.\n'",
      "id": "sklearn.externals.joblib.disk.rm_subdirs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.disk.rm_subdirs",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "disk"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.softmax",
      "description": "'\nCalculate the softmax function.\n\nThe softmax function is calculated by\nnp.exp(X) / np.sum(np.exp(X), axis=1)\n\nThis will cause overflow when large values are exponentiated.\nHence the largest value in each row is subtracted from each data\npoint to prevent this.\n",
      "id": "sklearn.utils.extmath.softmax",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.softmax",
      "parameters": [
        {
          "description": "Argument to the logistic function  copy: bool, optional Copy X or not. ",
          "name": "X",
          "shape": "M, N",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Softmax function evaluated at every point in x '",
        "name": "out: array, shape (M, N)"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.fixes.makedirs",
      "description": "'makedirs(name [, mode=0o777][, exist_ok=False])\n\nSuper-mkdir; create a leaf directory and all intermediate ones.  Works\nlike mkdir, except that any intermediate path segment (not just the\nrightmost) will be created if it does not exist. If the target\ndirectory already exists, raise an OSError if exist_ok is False.\nOtherwise no exception is raised.  This is recursive.\n\n'",
      "id": "sklearn.utils.fixes.makedirs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.fixes.makedirs",
      "parameters": [],
      "tags": [
        "utils",
        "fixes"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "abc.abstractmethod",
      "description": "\"A decorator indicating abstract methods.\n\nRequires that the metaclass is ABCMeta or derived from it.  A\nclass that has a metaclass derived from ABCMeta cannot be\ninstantiated unless all of its abstract methods are overridden.\nThe abstract methods can be called using any of the normal\n'super' call mechanisms.\n\nUsage:\n\nclass C:\n__metaclass__ = ABCMeta\n@abstractmethod\ndef my_abstract_method(self, ...):\n...\n\"",
      "id": "abc.abstractmethod",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "abc.abstractmethod",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.add_dummy_feature",
      "description": "'Augment dataset with an additional dummy feature.\n\nThis is useful for fitting an intercept term with implementations which\ncannot otherwise fit it directly.\n",
      "id": "sklearn.preprocessing.data.add_dummy_feature",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.add_dummy_feature",
      "parameters": [
        {
          "description": "Data. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Value to use for the dummy feature. ",
          "name": "value",
          "type": "float"
        }
      ],
      "returns": {
        "description": "X : {array, sparse matrix}, shape [n_samples, n_features + 1] Same data with dummy feature added as first column.  Examples --------  >>> from sklearn.preprocessing import add_dummy_feature >>> add_dummy_feature([[0, 1], [1, 0]]) array([[ 1.,  0.,  1.], [ 1.,  1.,  0.]]) '",
        "name": ""
      },
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.distance_metrics",
      "description": "\"Valid metrics for pairwise_distances.\n\nThis function simply returns the valid pairwise distance metrics.\nIt exists to allow for a description of the mapping for\neach of the valid strings.\n\nThe valid distance metrics, and the function they map to, are:\n\n============     ====================================\nmetric           Function\n============     ====================================\n'cityblock'      metrics.pairwise.manhattan_distances\n'cosine'         metrics.pairwise.cosine_distances\n'euclidean'      metrics.pairwise.euclidean_distances\n'l1'             metrics.pairwise.manhattan_distances\n'l2'             metrics.pairwise.euclidean_distances\n'manhattan'      metrics.pairwise.manhattan_distances\n============     ====================================\n\nRead more in the :ref:`User Guide <metrics>`.\n\n\"",
      "id": "sklearn.metrics.pairwise.distance_metrics",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.distance_metrics",
      "parameters": [],
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.laplacian_kernel",
      "description": "'Compute the laplacian kernel between X and Y.\n\nThe laplacian kernel is defined as::\n\nK(x, y) = exp(-gamma ||x-y||_1)\n\nfor each pair of rows x in X and y in Y.\nRead more in the :ref:`User Guide <laplacian_kernel>`.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.metrics.pairwise.laplacian_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.laplacian_kernel",
      "parameters": [
        {
          "description": " Y : array of shape (n_samples_Y, n_features) ",
          "name": "X",
          "shape": "n_samples_X, n_features",
          "type": "array"
        },
        {
          "description": "If None, defaults to 1.0 / n_samples_X ",
          "name": "gamma",
          "type": "float"
        }
      ],
      "returns": {
        "description": "'",
        "name": "kernel_matrix",
        "shape": "n_samples_X, n_samples_Y",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Haversine Distance",
      "description": "'Haversine (Spherical) Distance\n\nThe Haversine distance is the angular distance between two points on\nthe surface of a sphere.  The first distance of each point is assumed\nto be the latitude, the second is the longitude, given in radians.\nThe dimension of the points must be 2:\n\n.. math::\nD(x, y) = 2\\x07rcsin[\\\\sqrt{\\\\sin^2((x1 - y1) / 2)\n+ cos(x1)cos(y1)sin^2((x2 - y2) / 2)}]\n'",
      "id": "sklearn.neighbors.dist_metrics.HaversineDistance",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.HaversineDistance",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.kd_tree",
      "common_name": "Neighbors Heap",
      "description": "'A max-heap structure to keep track of distances/indices of neighbors\n\nThis implements an efficient pre-allocated set of fixed-size heaps\nfor chasing neighbors, holding both an index and a distance.\nWhen any row of the heap is full, adding an additional point will push\nthe furthest point off the heap.\n",
      "id": "sklearn.neighbors.kd_tree.NeighborsHeap",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.kd_tree.NeighborsHeap",
      "parameters": [
        {
          "description": "the number of heaps to use",
          "name": "n_pts",
          "type": "int"
        },
        {
          "description": "the size of each heap. '",
          "name": "n_nbrs",
          "type": "int"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "kd_tree"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.kd_tree",
      "common_name": "Node Heap",
      "description": "'NodeHeap\n\nThis is a min-heap implementation for keeping track of nodes\nduring a breadth-first search.  Unlike the NeighborsHeap above,\nthe NodeHeap does not have a fixed size and must be able to grow\nas elements are added.\n\nInternally, the data is stored in a simple binary heap which meets\nthe min heap condition:\n\nheap[i].val < min(heap[2 * i + 1].val, heap[2 * i + 2].val)\n'",
      "id": "sklearn.neighbors.kd_tree.NodeHeap",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.kd_tree.NodeHeap",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "kd_tree"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Holds the label for each class. ",
          "name": "classes_",
          "shape": "n_class,",
          "type": "array"
        }
      ],
      "category": "preprocessing.label",
      "common_name": "Label Encoder",
      "description": "'Encode labels with value between 0 and n_classes-1.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.\n\nAttributes\n----------\nclasses_ : array of shape (n_class,)\nHolds the label for each class.\n\nExamples\n--------\n`LabelEncoder` can be used to normalize labels.\n\n>>> from sklearn import preprocessing\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([1, 2, 2, 6])\nLabelEncoder()\n>>> le.classes_\narray([1, 2, 6])\n>>> le.transform([1, 1, 2, 6]) #doctest: +ELLIPSIS\narray([0, 0, 1, 2]...)\n>>> le.inverse_transform([0, 0, 1, 2])\narray([1, 1, 2, 6])\n\nIt can also be used to transform non-numerical labels (as long as they are\nhashable and comparable) to numerical labels.\n\n>>> le = preprocessing.LabelEncoder()\n>>> le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\nLabelEncoder()\n>>> list(le.classes_)\n[\\'amsterdam\\', \\'paris\\', \\'tokyo\\']\n>>> le.transform([\"tokyo\", \"tokyo\", \"paris\"]) #doctest: +ELLIPSIS\narray([2, 2, 1]...)\n>>> list(le.inverse_transform([2, 2, 1]))\n[\\'tokyo\\', \\'tokyo\\', \\'paris\\']\n\nSee also\n--------\nsklearn.preprocessing.OneHotEncoder : encode categorical integer features\nusing a one-hot aka one-of-K scheme.\n'",
      "id": "sklearn.preprocessing.label.LabelEncoder",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit label encoder\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit label encoder and return encoded labels\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y",
            "shape": "n_samples",
            "type": "array-like"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform labels back to original encoding.\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y",
            "shape": "n_samples",
            "type": "numpy"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform labels to normalized encoding.\n",
          "id": "sklearn.preprocessing.label.LabelEncoder.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y",
            "shape": "n_samples",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.preprocessing.label.LabelEncoder",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.pyc:56",
      "tags": [
        "preprocessing",
        "label"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.regression.median_absolute_error",
      "description": "'Median absolute error regression loss\n\nRead more in the :ref:`User Guide <median_absolute_error>`.\n",
      "id": "sklearn.metrics.regression.median_absolute_error",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.regression.median_absolute_error",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Estimated target values. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "A positive floating point value (the best value is 0.0).  Examples -------- >>> from sklearn.metrics import median_absolute_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> median_absolute_error(y_true, y_pred) 0.5  '",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "regression"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_spd_matrix",
      "description": "'Generate a random symmetric, positive-definite matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_spd_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_spd_matrix",
      "parameters": [
        {
          "description": "The matrix dimension. ",
          "name": "n_dim",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The random symmetric, positive-definite matrix.  See also -------- make_sparse_spd_matrix '",
        "name": "X",
        "shape": "n_dim, n_dim",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.multiclass.class_distribution",
      "description": "'Compute class priors from multioutput-multiclass target data\n",
      "id": "sklearn.utils.multiclass.class_distribution",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.multiclass.class_distribution",
      "parameters": [
        {
          "description": "The labels for each example. ",
          "name": "y",
          "size": "n_samples, n_outputs",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "List of classes for each column.  n_classes : list of integers of size n_outputs Number of classes in each column  class_prior : list of size n_outputs of arrays of size (n_classes,) Class distribution of each column.  '",
        "name": "classes",
        "size": "n_classes,",
        "type": "list"
      },
      "tags": [
        "utils",
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.base.clone",
      "description": "'Constructs a new estimator with the same parameters.\n\nClone does a deep copy of the model in an estimator\nwithout actually copying attached data. It yields a new estimator\nwith the same parameters that has not been fit on any data.\n",
      "id": "sklearn.base.clone",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.base.clone",
      "parameters": [
        {
          "description": "The estimator or group of estimators to be cloned  safe: boolean, optional If safe is false, clone will fall back to a deepcopy on objects that are not estimators.  '",
          "name": "estimator",
          "type": "estimator"
        }
      ],
      "tags": [
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Array Memmap Reducer",
      "description": "\"Reducer callable to dump large arrays to memmap files.\n",
      "id": "sklearn.externals.joblib.pool.ArrayMemmapReducer",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.externals.joblib.pool.ArrayMemmapReducer",
      "parameters": [
        {
          "description": "Threshold to trigger memmaping of large arrays to files created a folder. temp_folder: str Path of a folder where files for backing memmaped arrays are created. mmap_mode: 'r', 'r+' or 'c' Mode for the created memmap datastructure. See the documentation of numpy.memmap for more details. Note: 'w+' is coerced to 'r+' automatically to avoid zeroing the data on unpickling. verbose: int, optional, 0 by default If verbose > 0, memmap creations are logged. If verbose > 1, both memmap creations, reuse and array pickling are logged. prewarm: bool, optional, False by default. Force a read on newly memmaped array to make sure that OS pre-cache it memory. This can be useful to avoid concurrent disk access when the same data array is passed to different worker processes. \"",
          "name": "max_nbytes",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc:170",
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Customizable Pickler",
      "description": "'Pickler that accepts custom reducers.\n\nHIGHEST_PROTOCOL is selected by default as this pickler is used\nto pickle ephemeral datastructures for interprocess communication\nhence no backward compatibility is required.\n\n`reducers` is expected to be a dictionary with key/values\nbeing `(type, callable)` pairs where `callable` is a function that\ngive an instance of `type` will return a tuple `(constructor,\ntuple_of_objects)` to rebuild an instance out of the pickled\n`tuple_of_objects` as would return a `__reduce__` method. See the\nstandard library documentation on pickling for more details.\n\n'",
      "id": "sklearn.externals.joblib.pool.CustomizablePickler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Clears the pickler\\'s \"memo\".\n\nThe memo is the data structure that remembers which objects the\npickler has already seen, so that shared or recursive objects are\npickled by reference and not by value.  This method is useful when\nre-using picklers.\n\n'",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.clear_memo",
          "name": "clear_memo",
          "parameters": []
        },
        {
          "description": "'Write a pickled representation of obj to the open file.'",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.dump",
          "name": "dump",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.get",
          "name": "get",
          "parameters": []
        },
        {
          "description": "'Store an object in the memo.'",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.memoize",
          "name": "memoize",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.persistent_id",
          "name": "persistent_id",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.put",
          "name": "put",
          "parameters": []
        },
        {
          "description": "'Attach a reducer function to a given type in the dispatch table.'",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.register",
          "name": "register",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save",
          "name": "save",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_bool",
          "name": "save_bool",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_dict",
          "name": "save_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_empty_tuple",
          "name": "save_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_float",
          "name": "save_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_global",
          "name": "save_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_inst",
          "name": "save_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_int",
          "name": "save_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_list",
          "name": "save_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_long",
          "name": "save_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_none",
          "name": "save_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_pers",
          "name": "save_pers",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_reduce",
          "name": "save_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_string",
          "name": "save_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_tuple",
          "name": "save_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.CustomizablePickler.save_unicode",
          "name": "save_unicode",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.pool.CustomizablePickler",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc:265",
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Compound Kernel",
      "description": "'Kernel which is composed of a set of other kernels.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.gaussian_process.kernels.CompoundKernel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.CompoundKernel.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.CompoundKernel.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X, n_kernels",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.CompoundKernel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.CompoundKernel.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.CompoundKernel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.CompoundKernel",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:394",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Constant Kernel",
      "description": "'Constant kernel.\n\nCan be used as part of a product-kernel where it scales the magnitude of\nthe other factor (kernel) or as part of a sum-kernel, where it modifies\nthe mean of the Gaussian process.\n\nk(x_1, x_2) = constant_value for all x_1, x_2\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.ConstantKernel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.ConstantKernel.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.ConstantKernel.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.ConstantKernel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.ConstantKernel.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.ConstantKernel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.ConstantKernel",
      "parameters": [
        {
          "description": "The constant value which defines the covariance: k(x_1, x_2) = constant_value ",
          "name": "constant_value",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on constant_value  '",
          "name": "constant_value_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:940",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Sum",
      "description": "'Sum-kernel k1 + k2 of two kernels k1 and k2.\n\nThe resulting kernel is defined as\nk_sum(X, Y) = k1(X, Y) + k2(X, Y)\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.Sum",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.Sum.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.Sum.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.Sum.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.Sum.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.Sum.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.Sum",
      "parameters": [
        {
          "description": "The first base-kernel of the sum-kernel  k2 : Kernel object The second base-kernel of the sum-kernel  '",
          "name": "k1",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:634",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "path to root of joblib cache  func: function or string function whose output is cached. The string case is intended only for instanciation based on the output of repr() on another instance. (namely eval(repr(memorized_instance)) works).  argument_hash: string hash of the function arguments  mmap_mode: {None, 'r+', 'r', 'w+', 'c'} The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the different values.  verbose: int verbosity level (0 means no message)  timestamp, metadata: string for internal use only",
          "name": "cachedir",
          "type": "string"
        }
      ],
      "category": "externals.joblib.memory",
      "common_name": "Memorized Result",
      "description": "\"Object representing a cached value.\n\nAttributes\n----------\ncachedir: string\npath to root of joblib cache\n\nfunc: function or string\nfunction whose output is cached. The string case is intended only for\ninstanciation based on the output of repr() on another instance.\n(namely eval(repr(memorized_instance)) works).\n\nargument_hash: string\nhash of the function arguments\n\nmmap_mode: {None, 'r+', 'r', 'w+', 'c'}\nThe memmapping mode used when loading from cache numpy arrays. See\nnumpy.load for the meaning of the different values.\n\nverbose: int\nverbosity level (0 means no message)\n\ntimestamp, metadata: string\nfor internal use only\n\"",
      "id": "sklearn.externals.joblib.memory.MemorizedResult",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Clear value from cache'",
          "id": "sklearn.externals.joblib.memory.MemorizedResult.clear",
          "name": "clear",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedResult.debug",
          "name": "debug",
          "parameters": []
        },
        {
          "description": "' Return the formated representation of the object.\n'",
          "id": "sklearn.externals.joblib.memory.MemorizedResult.format",
          "name": "format",
          "parameters": []
        },
        {
          "description": "'Read value from cache and return it.'",
          "id": "sklearn.externals.joblib.memory.MemorizedResult.get",
          "name": "get",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedResult.warn",
          "name": "warn",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.memory.MemorizedResult",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:144",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Exponential Loss",
      "description": "'Exponential loss function for binary classification.\n\nSame loss as AdaBoost.\n\nReferences\n----------\nGreg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007\n'",
      "id": "sklearn.ensemble.gradient_boosting.ExponentialLoss",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ExponentialLoss.init_estimator",
          "name": "init_estimator",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.ensemble.gradient_boosting.ExponentialLoss.negative_gradient",
          "name": "negative_gradient",
          "parameters": []
        },
        {
          "description": "'Update the terminal regions (=leaves) of the given tree and\nupdates the current predictions of the model. Traverses tree\nand invokes template method `_update_terminal_region`.\n",
          "id": "sklearn.ensemble.gradient_boosting.ExponentialLoss.update_terminal_regions",
          "name": "update_terminal_regions",
          "parameters": [
            {
              "description": "The tree object. X : ndarray, shape=(n, m) The data array.",
              "name": "tree",
              "type": "tree"
            },
            {
              "description": "The target labels.",
              "name": "y",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The residuals (usually the negative gradient).",
              "name": "residual",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The predictions.",
              "name": "y_pred",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The weight of each sample.",
              "name": "sample_weight",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "The sample mask to be used.",
              "name": "sample_mask",
              "shape": "n,",
              "type": "ndarray"
            },
            {
              "description": "learning rate shrinks the contribution of each tree by ``learning_rate``.",
              "name": "learning_rate",
              "type": "float"
            },
            {
              "description": "The index of the estimator being updated.  '",
              "name": "k",
              "type": "int"
            }
          ]
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.ExponentialLoss",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:594",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.base",
      "common_name": "Linear Classifier Mixin",
      "description": "'Mixin for linear classifiers.\n\nHandles prediction for sparse and dense X.\n'",
      "id": "sklearn.linear_model.base.LinearClassifierMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.base.LinearClassifierMixin.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.base.LinearClassifierMixin.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.base.LinearClassifierMixin.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.linear_model.base.LinearClassifierMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc:284",
      "tags": [
        "linear_model",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization"
      ],
      "attributes": [],
      "category": "linear_model.coordinate_descent",
      "common_name": "Linear Model CV",
      "description": "'Base class for iterative model fitting along a regularization path'",
      "id": "sklearn.linear_model.coordinate_descent.LinearModelCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.coordinate_descent.LinearModelCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model with coordinate descent\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n",
          "id": "sklearn.linear_model.coordinate_descent.LinearModelCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Pass directly as float64, Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.LinearModelCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.LinearModelCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.LinearModelCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.LinearModelCV",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1026",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "The current learning rate ",
          "name": "learning_rate",
          "type": "float"
        },
        {
          "description": "Timestep ",
          "name": "t",
          "type": "int"
        },
        {
          "description": "First moment vectors ",
          "name": "ms",
          "type": "list"
        },
        {
          "description": "Second moment vectors ",
          "name": "vs",
          "type": "list"
        }
      ],
      "category": "neural_network._stochastic_optimizers",
      "common_name": "Adam Optimizer",
      "description": "'Stochastic gradient descent optimizer with Adam\n\nNote: All default values are from the original Adam paper\n",
      "id": "sklearn.neural_network._stochastic_optimizers.AdamOptimizer",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Perform update to learning rate and potentially other states at the\nend of an iteration\n'",
          "id": "sklearn.neural_network._stochastic_optimizers.AdamOptimizer.iteration_ends",
          "name": "iteration_ends",
          "parameters": []
        },
        {
          "description": "'Decides whether it is time to stop training\n",
          "id": "sklearn.neural_network._stochastic_optimizers.AdamOptimizer.trigger_stopping",
          "name": "trigger_stopping",
          "parameters": [
            {
              "description": "Message passed in for verbose output ",
              "name": "msg",
              "type": "str"
            },
            {
              "description": "Print message to stdin if True ",
              "name": "verbose",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "True if training needs to stop '",
            "name": "is_stopping",
            "type": "bool"
          }
        },
        {
          "description": "'Update parameters with given gradients\n",
          "id": "sklearn.neural_network._stochastic_optimizers.AdamOptimizer.update_params",
          "name": "update_params",
          "parameters": [
            {
              "description": "Containing gradients with respect to coefs_ and intercepts_ in MLP model. So length should be aligned with params '",
              "name": "grads",
              "type": "list"
            }
          ]
        }
      ],
      "name": "sklearn.neural_network._stochastic_optimizers.AdamOptimizer",
      "parameters": [
        {
          "description": "The concatenated list containing coefs_ and intercepts_ in MLP model. Used for initializing velocities and updating params ",
          "name": "params",
          "type": "list"
        },
        {
          "description": "The initial learning rate used. It controls the step-size in updating the weights  beta_1 : float, optional, default 0.9 Exponential decay rate for estimates of first moment vector, should be in [0, 1)  beta_2 : float, optional, default 0.999 Exponential decay rate for estimates of second moment vector, should be in [0, 1) ",
          "name": "learning_rate_init",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Value for numerical stability ",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/_stochastic_optimizers.pyc:184",
      "tags": [
        "neural_network",
        "_stochastic_optimizers"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Shuffle Split",
      "description": "'Random permutation cross-validation iterator.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.ShuffleSplit` instead.\n\nYields indices to split data into training and test sets.\n\nNote: contrary to other cross-validation strategies, random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.ShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.ShuffleSplit",
      "parameters": [
        {
          "description": "Total number of elements in the dataset. ",
          "name": "n",
          "type": "int"
        },
        {
          "description": "Number of re-shuffling & splitting iterations. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:790",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "preprocessing.data",
      "common_name": "Kernel Centerer",
      "description": "'Center a kernel matrix\n\nLet K(x, z) be a kernel defined by phi(x)^T phi(z), where phi is a\nfunction mapping x to a Hilbert space. KernelCenterer centers (i.e.,\nnormalize to have zero mean) the data without explicitly computing phi(x).\nIt is equivalent to centering phi(x) with\nsklearn.preprocessing.StandardScaler(with_std=False).\n\nRead more in the :ref:`User Guide <kernel_centering>`.\n'",
      "id": "sklearn.preprocessing.data.KernelCenterer",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit KernelCenterer\n",
          "id": "sklearn.preprocessing.data.KernelCenterer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Kernel matrix. ",
              "name": "K",
              "shape": "n_samples, n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.KernelCenterer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.KernelCenterer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.KernelCenterer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Center kernel matrix.\n",
          "id": "sklearn.preprocessing.data.KernelCenterer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Kernel matrix. ",
              "name": "K",
              "shape": "n_samples1, n_samples2",
              "type": "numpy"
            },
            {
              "description": "Set to False to perform inplace computation. ",
              "name": "copy",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "K_new",
            "shape": "n_samples1, n_samples2",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.preprocessing.data.KernelCenterer",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:1554",
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [],
      "category": "neural_network.multilayer_perceptron",
      "common_name": "Base Multilayer Perceptron",
      "description": "'Base class for MLP classification and regression.\n\nWarning: This class should not be used directly.\nUse derived classes instead.\n\n.. versionadded:: 0.18\n'",
      "id": "sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model to data matrix X and target y.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neural_network.multilayer_perceptron.BaseMultilayerPerceptron",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc:39",
      "tags": [
        "neural_network",
        "multilayer_perceptron"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Group K Fold",
      "description": "'K-fold iterator variant with non-overlapping groups.\n\nThe same group will not appear in two different folds (the number of\ndistinct groups has to be at least equal to the number of folds).\n\nThe folds are approximately balanced in the sense that the number of\ndistinct groups is approximately the same in each fold.\n",
      "id": "sklearn.model_selection._split.GroupKFold",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.GroupKFold.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.GroupKFold.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.GroupKFold",
      "parameters": [
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_splits",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:423",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "feature_selection.base",
      "common_name": "Selector Mixin",
      "description": "'\nTranformer mixin that performs feature selection given a support mask\n\nThis mixin provides a feature selector implementation with `transform` and\n`inverse_transform` functionality given an implementation of\n`_get_support_mask`.\n'",
      "id": "sklearn.feature_selection.base.SelectorMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.base.SelectorMixin.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.base.SelectorMixin.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.base.SelectorMixin.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.base.SelectorMixin.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.base.SelectorMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/base.pyc:18",
      "tags": [
        "feature_selection",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Pickling Pool",
      "description": "'Pool implementation with customizable pickling reducers.\n\nThis is useful to control how data is shipped between processes\nand makes it possible to use shared memory without useless\ncopies induces by the default pickling methods of the original\nobjects passed as arguments to dispatch.\n\n`forward_reducers` and `backward_reducers` are expected to be\ndictionaries with key/values being `(type, callable)` pairs where\n`callable` is a function that, given an instance of `type`, will return a\ntuple `(constructor, tuple_of_objects)` to rebuild an instance out of the\npickled `tuple_of_objects` as would return a `__reduce__` method.\nSee the standard library documentation about pickling for more details.\n\n'",
      "id": "sklearn.externals.joblib.pool.PicklingPool",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nEquivalent of `apply()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.apply",
          "name": "apply",
          "parameters": []
        },
        {
          "description": "'\nAsynchronous equivalent of `apply()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.PicklingPool.close",
          "name": "close",
          "parameters": []
        },
        {
          "description": "'\nEquivalent of `itertools.imap()` -- can be MUCH slower than `Pool.map()`\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.imap",
          "name": "imap",
          "parameters": []
        },
        {
          "description": "'\nLike `imap()` method but ordering of results is arbitrary\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.imap_unordered",
          "name": "imap_unordered",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.PicklingPool.join",
          "name": "join",
          "parameters": []
        },
        {
          "description": "'\nEquivalent of `map()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.map",
          "name": "map",
          "parameters": []
        },
        {
          "description": "'\nAsynchronous equivalent of `map()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.PicklingPool.map_async",
          "name": "map_async",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.PicklingPool.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.pool.PicklingPool",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc:393",
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "kernel_approximation",
      "common_name": "Additive Chi2 Sampler",
      "description": "'Approximate feature map for additive chi2 kernel.\n\nUses sampling the fourier transform of the kernel characteristic\nat regular intervals.\n\nSince the kernel that is to be approximated is additive, the components of\nthe input vectors can be treated separately.  Each entry in the original\nspace is transformed into 2*sample_steps+1 features, where sample_steps is\na parameter of the method. Typical values of sample_steps include 1, 2 and\n3.\n\nOptimal choices for the sampling interval for certain data ranges can be\ncomputed (see the reference). The default values should be reasonable.\n\nRead more in the :ref:`User Guide <additive_chi_kernel_approx>`.\n",
      "id": "sklearn.kernel_approximation.AdditiveChi2Sampler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Set parameters.'",
          "id": "sklearn.kernel_approximation.AdditiveChi2Sampler.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.kernel_approximation.AdditiveChi2Sampler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.kernel_approximation.AdditiveChi2Sampler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.kernel_approximation.AdditiveChi2Sampler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply approximate feature map to X.\n",
          "id": "sklearn.kernel_approximation.AdditiveChi2Sampler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Whether the return value is an array of sparse matrix depends on the type of the input X. '",
            "name": "X_new",
            "shape": "n_samples, n_features * (2*sample_steps + 1",
            "type": "array, sparse matrix"
          }
        }
      ],
      "name": "sklearn.kernel_approximation.AdditiveChi2Sampler",
      "parameters": [
        {
          "description": "Gives the number of (complex) sampling points.",
          "name": "sample_steps",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Sampling interval. Must be specified when sample_steps not in {1,2,3}. ",
          "name": "sample_interval",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/kernel_approximation.pyc:206",
      "tags": [
        "kernel_approximation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.randomized_l1",
      "common_name": "Base Randomized Linear Model",
      "description": "'Base class to implement randomized linear models for feature selection\n\nThis implements the strategy by Meinshausen and Buhlman:\nstability selection with randomized sampling, and random re-weighting of\nthe penalty.\n'",
      "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Return a mask, or list, of the features/indices selected.'",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.get_support",
          "name": "get_support",
          "parameters": []
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.inverse_transform",
          "name": "inverse_transform",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.linear_model.randomized_l1.BaseRandomizedLinearModel",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/randomized_l1.pyc:61",
      "tags": [
        "linear_model",
        "randomized_l1"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Shuffle Split",
      "description": "'Random permutation cross-validator\n\nYields indices to split data into training and test sets.\n\nNote: contrary to other cross-validation strategies, random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.ShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.ShuffleSplit.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.ShuffleSplit.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.ShuffleSplit",
      "parameters": [
        {
          "description": "Number of re-shuffling & splitting iterations. ",
          "name": "n_splits",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:985",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "naive_bayes",
      "common_name": "Base NB",
      "description": "'Abstract base class for naive Bayes estimators'",
      "id": "sklearn.naive_bayes.BaseNB",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.naive_bayes.BaseNB.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nPerform classification on an array of test vectors X.\n",
          "id": "sklearn.naive_bayes.BaseNB.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log-probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BaseNB.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BaseNB.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.naive_bayes.BaseNB.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.naive_bayes.BaseNB.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.naive_bayes.BaseNB",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc:38",
      "tags": [
        "naive_bayes"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Sequential Backend",
      "description": "'A ParallelBackend which will execute all batches sequentially.\n\nDoes not use/create any threading objects, and hence has minimal\noverhead. Used when n_jobs == 1.\n'",
      "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Abort any running tasks\n\nThis is called when an exception has been raised when executing a tasks\nand all the remaining tasks will be ignored and can therefore be\naborted to spare computation resources.\n\nIf ensure_ready is True, the backend should be left in an operating\nstate as future tasks might be re-submitted via that same backend\ninstance.\n\nIf ensure_ready is False, the implementer of this method can decide\nto leave the backend in a closed / terminated state as no new task\nare expected to be submitted to this backend.\n\nSetting ensure_ready to False is an optimization that can be leveraged\nwhen aborting tasks via killing processes from a local process pool\nmanaged by the backend it-self: if we expect no new tasks, there is no\npoint in re-creating a new working pool.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.abort_everything",
          "name": "abort_everything",
          "parameters": []
        },
        {
          "description": "'Schedule a func to be run'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "'Callback indicate how long it took to run a batch'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.batch_completed",
          "name": "batch_completed",
          "parameters": []
        },
        {
          "description": "'Determine the optimal batch size'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.compute_batch_size",
          "name": "compute_batch_size",
          "parameters": []
        },
        {
          "description": "'Reconfigure the backend and return the number of workers.\n\nThis makes it possible to reuse an existing backend instance for\nsuccessive independent calls to Parallel with different parameters.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.configure",
          "name": "configure",
          "parameters": []
        },
        {
          "description": "'Determine the number of jobs which are going to run in parallel'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.effective_n_jobs",
          "name": "effective_n_jobs",
          "parameters": []
        },
        {
          "description": "'List of exception types to be captured.'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.get_exceptions",
          "name": "get_exceptions",
          "parameters": []
        },
        {
          "description": "'Shutdown the process or thread pool'",
          "id": "sklearn.externals.joblib._parallel_backends.SequentialBackend.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib._parallel_backends.SequentialBackend",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:94",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.weight_boosting",
      "common_name": "Base Weight Boosting",
      "description": "'Base class for AdaBoost estimators.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n'",
      "id": "sklearn.ensemble.weight_boosting.BaseWeightBoosting",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Build a boosted classifier/regressor from the training set (X, y).\n",
          "id": "sklearn.ensemble.weight_boosting.BaseWeightBoosting.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is forced to DTYPE from tree._tree if the base classifier of this ensemble weighted boosting classifier is a tree or forest. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, the sample weights are initialized to 1 / n_samples. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.weight_boosting.BaseWeightBoosting.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.weight_boosting.BaseWeightBoosting.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Return staged scores for X, y.\n\nThis generator method yields the ensemble score after each iteration of\nboosting and therefore allows monitoring, such as to determine the\nscore on a test set after each boost.\n",
          "id": "sklearn.ensemble.weight_boosting.BaseWeightBoosting.staged_score",
          "name": "staged_score",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "z",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.ensemble.weight_boosting.BaseWeightBoosting",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc:50",
      "tags": [
        "ensemble",
        "weight_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification",
        "decision tree"
      ],
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "K Fold",
      "description": "'K-Folds cross-validator\n\nProvides train/test indices to split data in train/test sets. Split\ndataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used once as a validation while the k - 1 remaining\nfolds form the training set.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.KFold",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.KFold.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.KFold.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.KFold",
      "parameters": [
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_splits",
          "type": "int"
        },
        {
          "description": "Whether to shuffle the data before splitting into batches. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "When shuffle=True, pseudo-random number generator state used for shuffling. If None, use default numpy RNG for shuffling. ",
          "name": "random_state",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:347",
      "tags": [
        "model_selection",
        "_split"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Predefined Split",
      "description": "'Predefined split cross-validator\n\nSplits the data into training/test set folds according to a predefined\nscheme. Each sample can be assigned to at most one test set fold, as\nspecified by the user through the ``test_fold`` parameter.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nExamples\n--------\n>>> from sklearn.model_selection import PredefinedSplit\n>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n>>> y = np.array([0, 0, 1, 1])\n>>> test_fold = [0, 1, -1, 1]\n>>> ps = PredefinedSplit(test_fold)\n>>> ps.get_n_splits()\n2\n>>> print(ps)       # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\nPredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n>>> for train_index, test_index in ps.split():\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\nTRAIN: [1 2 3] TEST: [0]\nTRAIN: [0 2] TEST: [1 3]\n'",
      "id": "sklearn.model_selection._split.PredefinedSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.PredefinedSplit.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.PredefinedSplit.split",
          "name": "split",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.PredefinedSplit",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:1402",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "externals.joblib.memory",
      "common_name": "Memory",
      "description": "\" A context object for caching a function's return value each time it\nis called with the same input arguments.\n\nAll values are cached on the filesystem, in a deep directory\nstructure.\n\nsee :ref:`memory_reference`\n\"",
      "id": "sklearn.externals.joblib.memory.Memory",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\" Decorates the given function func to only compute its return\nvalue for input arguments not cached on disk.\n",
          "id": "sklearn.externals.joblib.memory.Memory.cache",
          "name": "cache",
          "parameters": [
            {
              "description": "The function to be decorated ignore: list of strings A list of arguments name to ignore in the hashing verbose: integer, optional The verbosity mode of the function. By default that of the memory object is used. mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the arguments. By default that of the memory object is used. ",
              "name": "func",
              "optional": "true",
              "type": "callable"
            }
          ],
          "returns": {
            "description": "The returned object is a MemorizedFunc object, that is callable (behaves like a function), but offers extra methods for cache lookup and management. See the documentation for :class:`joblib.memory.MemorizedFunc`. \"",
            "name": "decorated_func: MemorizedFunc object"
          }
        },
        {
          "description": "' Erase the complete cache directory.\n'",
          "id": "sklearn.externals.joblib.memory.Memory.clear",
          "name": "clear",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.Memory.debug",
          "name": "debug",
          "parameters": []
        },
        {
          "description": "' Eval function func with arguments `*args` and `**kwargs`,\nin the context of the memory.\n\nThis method works similarly to the builtin `apply`, except\nthat the function is called only if the cache is not\nup to date.\n\n'",
          "id": "sklearn.externals.joblib.memory.Memory.eval",
          "name": "eval",
          "parameters": []
        },
        {
          "description": "' Return the formated representation of the object.\n'",
          "id": "sklearn.externals.joblib.memory.Memory.format",
          "name": "format",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.Memory.warn",
          "name": "warn",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.memory.Memory",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:783",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.svd_flip",
      "description": "'Sign correction to ensure deterministic output from SVD.\n\nAdjusts the columns of u and the rows of v such that the loadings in the\ncolumns in u that are largest in absolute value are always positive.\n",
      "id": "sklearn.utils.extmath.svd_flip",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.svd_flip",
      "parameters": [
        {
          "description": "u and v are the output of `linalg.svd` or `sklearn.utils.extmath.randomized_svd`, with matching inner dimensions so one can compute `np.dot(u * s, v)`. ",
          "name": "u, v",
          "type": "ndarray"
        },
        {
          "description": "If True, use the columns of u as the basis for sign flipping. Otherwise, use the rows of v. The choice of which variable to base the decision on is generally algorithm dependent.  ",
          "name": "u_based_decision",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": " '",
        "name": "u_adjusted, v_adjusted",
        "type": "arrays"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.univariate_selection.f_classif",
      "description": "'Compute the ANOVA F-value for the provided sample.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.f_classif",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.univariate_selection.f_classif",
      "parameters": [
        {
          "description": "The set of regressors that will be tested sequentially. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "The data matrix. ",
          "name": "y",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "The set of F values.  pval : array, shape = [n_features,] The set of p-values.  See also -------- chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. '",
        "name": "F",
        "shape": "n_features,",
        "type": "array"
      },
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_s_curve",
      "description": "'Generate an S curve dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_s_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_s_curve",
      "parameters": [
        {
          "default": "100",
          "description": "The number of sample points on the S curve. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The points.  t : array of shape [n_samples] The univariate position of the sample according to the main dimension of the points in the manifold. '",
        "name": "X",
        "shape": "n_samples, 3",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_sample_images",
      "description": "\"Load sample images for image manipulation.\nLoads both, ``china`` and ``flower``.\n",
      "id": "sklearn.datasets.base.load_sample_images",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_sample_images",
      "parameters": [],
      "returns": {
        "description": "Dictionary-like object with the following attributes : 'images', the two sample images, 'filenames', the file names for the images, and 'DESCR' the full description of the dataset.  Examples -------- To load the data and visualize the images:  >>> from sklearn.datasets import load_sample_images >>> dataset = load_sample_images()     #doctest: +SKIP >>> len(dataset.images)                #doctest: +SKIP 2 >>> first_img_data = dataset.images[0] #doctest: +SKIP >>> first_img_data.shape               #doctest: +SKIP (427, 640, 3) >>> first_img_data.dtype               #doctest: +SKIP dtype('uint8') \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.graph.single_source_shortest_path_length",
      "description": "'Return the shortest path length from source to all reachable nodes.\n\nReturns a dictionary of shortest path lengths keyed by target.\n",
      "id": "sklearn.utils.graph.single_source_shortest_path_length",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.graph.single_source_shortest_path_length",
      "parameters": [
        {
          "description": "Adjacency matrix of the graph",
          "name": "graph",
          "type": "sparse"
        },
        {
          "description": "Starting node for path",
          "name": "source",
          "type": "node"
        },
        {
          "description": "Depth to stop the search - only paths of length <= cutoff are returned.  Examples -------- >>> from sklearn.utils.graph import single_source_shortest_path_length >>> import numpy as np >>> graph = np.array([[ 0, 1, 0, 0], ...                   [ 1, 0, 1, 0], ...                   [ 0, 1, 0, 1], ...                   [ 0, 0, 1, 0]]) >>> single_source_shortest_path_length(graph, 0) {0: 0, 1: 1, 2: 2, 3: 3} >>> single_source_shortest_path_length(np.ones((6, 6)), 2) {0: 1, 1: 1, 2: 0, 3: 1, 4: 1, 5: 1} '",
          "name": "cutoff",
          "optional": "true",
          "type": "integer"
        }
      ],
      "tags": [
        "utils",
        "graph"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.random.random_choice_csc",
      "description": "'Generate a sparse random matrix given column class distributions\n",
      "id": "sklearn.utils.random.random_choice_csc",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.random.random_choice_csc",
      "parameters": [
        {
          "description": "Number of samples to draw in each column. ",
          "name": "n_samples",
          "type": "int"
        },
        {
          "description": "List of classes for each column. ",
          "name": "classes",
          "size": "n_classes,",
          "type": "list"
        },
        {
          "description": "Optional (default=None). Class distribution of each column. If None the uniform distribution is assumed. ",
          "name": "class_probability",
          "size": "n_classes,",
          "type": "list"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": " '",
        "name": "random_matrix",
        "size": "n_samples, n_outputs",
        "type": "sparse"
      },
      "tags": [
        "utils",
        "random"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.image.reconstruct_from_patches_2d",
      "description": "'Reconstruct the image from all of its patches.\n\nPatches are assumed to overlap and the image is constructed by filling in\nthe patches from left to right, top to bottom, averaging the overlapping\nregions.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.image.reconstruct_from_patches_2d",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.image.reconstruct_from_patches_2d",
      "parameters": [
        {
          "description": "(n_patches, patch_height, patch_width, n_channels) The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have `n_channels=3`. ",
          "name": "patches",
          "shape": "n_patches, patch_height, patch_width",
          "type": "array"
        },
        {
          "description": "(image_height, image_width, n_channels) the size of the image that will be reconstructed ",
          "name": "image_size",
          "type": "tuple"
        }
      ],
      "returns": {
        "description": "the reconstructed image  '",
        "name": "image",
        "type": "array"
      },
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "AudioFeaturization",
      "description": "Converts time series audio data into a vector of features",
      "id": "spider.featurization.audio_featurization.AudioFeaturization",
      "is_class": true,
      "language": "python",
      "library": "spider",
      "name": "spider.featurization.audio_featurization.AudioFeaturization",
      "tags": [
        "feature_extraction",
        "audio"
      ],
      "parameters":
      [
        {
          "name": "sampling_rate",
          "description": "uniform sampling rate of the incoming audio data",
          "type": "int",
          "optional": true,
          "default": "44100",
          "is_hyperparameter": false
        },
        {
          "name": "frame_length",
          "description": "duration in seconds that defines the length of the audio processing window",
          "type": "float",
          "optional": true,
          "default": "0.050",
          "is_hyperparameter": true
        },
        {
          "name": "overlap",
          "description": "duration in seconds that defines the step size taken along the time series during subsequent processing steps",
          "type": "float",
          "optional": true,
          "default": "0.025",
          "is_hyperparameter": true
        }
      ],
      "returns": {
        "description": "the type of sound'",
        "name": "audio",
        "type": "array"
      },
      "tags": [
        "feature_extraction",
        "audio"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.func_inspect.get_func_code",
      "description": "\" Attempts to retrieve a reliable function code hash.\n\nThe reason we don't use inspect.getsource is that it caches the\nsource, whereas we want this to be modified on the fly when the\nfunction is modified.\n",
      "id": "sklearn.externals.joblib.func_inspect.get_func_code",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.func_inspect.get_func_code",
      "parameters": [],
      "returns": {
        "description": "The function code source_file: string The path to the file in which the function is defined. first_line: int The first line of the code in the source file.  Notes ------ This function does a bit more magic than inspect, and is thus more robust. \"",
        "name": "func_code: string"
      },
      "tags": [
        "externals",
        "joblib",
        "func_inspect"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_circles",
      "description": "'Make a large circle containing a smaller circle in 2d.\n\nA simple toy dataset to visualize clustering and classification\nalgorithms.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_circles",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_circles",
      "parameters": [
        {
          "default": "100",
          "description": "The total number of points generated.  shuffle: bool, optional (default=True) Whether to shuffle the samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Standard deviation of Gaussian noise added to the data. ",
          "name": "noise",
          "type": "double"
        },
        {
          "description": "Scale factor between inner and outer circle. ",
          "name": "factor",
          "type": "double"
        }
      ],
      "returns": {
        "description": "The generated samples.  y : array of shape [n_samples] The integer labels (0 or 1) for class membership of each sample. '",
        "name": "X",
        "shape": "n_samples, 2",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_moons",
      "description": "'Make two interleaving half circles\n\nA simple toy dataset to visualize clustering and classification\nalgorithms. Read more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_moons",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_moons",
      "parameters": [
        {
          "default": "100",
          "description": "The total number of points generated. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Whether to shuffle the samples. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Standard deviation of Gaussian noise added to the data. ",
          "name": "noise",
          "type": "double"
        }
      ],
      "returns": {
        "description": "The generated samples.  y : array of shape [n_samples] The integer labels (0 or 1) for class membership of each sample. '",
        "name": "X",
        "shape": "n_samples, 2",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.get_data_home",
      "description": "\"Return the path of the scikit-learn data dir.\n\nThis folder is used by some large dataset loaders to avoid\ndownloading the data several times.\n\nBy default the data dir is set to a folder named 'scikit_learn_data'\nin the user home folder.\n\nAlternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\nvariable or programmatically by giving an explicit folder path. The\n'~' symbol is expanded to the user home folder.\n\nIf the folder does not already exist, it is automatically created.\n\"",
      "id": "sklearn.datasets.base.get_data_home",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.get_data_home",
      "parameters": [],
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.linear",
      "description": "\"\nLinear correlation model::\n\ntheta, d --> r(theta, d) =\nn\nprod max(0, 1 - theta_j*d_ij) ,  i = 1,...,m\nj = 1\n",
      "id": "sklearn.gaussian_process.correlation_models.linear",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.linear",
      "parameters": [
        {
          "description": "An array with shape 1 (isotropic) or n (anisotropic) giving the autocorrelation parameter(s). ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) with the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.l1_cross_distances",
      "description": "'DEPRECATED: l1_cross_distances was deprecated in version 0.18 and will be removed in 0.20.\n\n\nComputes the nonzero componentwise L1 cross-distances between the vectors\nin X.\n",
      "id": "sklearn.utils.deprecation.l1_cross_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.l1_cross_distances",
      "parameters": [],
      "returns": {
        "description": "D: array with shape (n_samples * (n_samples - 1) / 2, n_features) The array of componentwise L1 cross-distances.  ij: arrays with shape (n_samples * (n_samples - 1) / 2, 2) The indices i and j of the vectors in X associated to the cross- distances in D: D[k] = np.abs(X[ij[k, 0]] - Y[ij[k, 1]]). '",
        "name": ""
      },
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "collections.namedtuple",
      "description": "\"Returns a new subclass of tuple with named fields.\n\n>>> Point = namedtuple('Point', ['x', 'y'])\n>>> Point.__doc__                   # docstring for the new class\n'Point(x, y)'\n>>> p = Point(11, y=22)             # instantiate with positional args or keywords\n>>> p[0] + p[1]                     # indexable like a plain tuple\n33\n>>> x, y = p                        # unpack like a regular tuple\n>>> x, y\n(11, 22)\n>>> p.x + p.y                       # fields also accessible by name\n33\n>>> d = p._asdict()                 # convert to a dictionary\n>>> d['x']\n11\n>>> Point(**d)                      # convert from a dictionary\nPoint(x=11, y=22)\n>>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\nPoint(x=100, y=22)\n\n\"",
      "id": "collections.namedtuple",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "collections.namedtuple",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_diabetes",
      "description": "\"Load and return the diabetes dataset (regression).\n\n==============      ==================\nSamples total       442\nDimensionality      10\nFeatures            real, -.2 < x < .2\nTargets             integer 25 - 346\n==============      ==================\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.base.load_diabetes",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_diabetes",
      "parameters": [
        {
          "description": "If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "return_X_y",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn and 'target', the regression target for each sample.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18 \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_linnerud",
      "description": "\"Load and return the linnerud dataset (multivariate regression).\n\nSamples total: 20\nDimensionality: 3 for both data and targets\nFeatures: integer\nTargets: integer\n",
      "id": "sklearn.datasets.base.load_linnerud",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_linnerud",
      "parameters": [
        {
          "description": "If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "return_X_y",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data' and 'targets', the two multivariate datasets, with 'data' corresponding to the exercise and 'targets' corresponding to the physiological measurements, as well as 'feature_names' and 'target_names'.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18 \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.pipeline.make_union",
      "description": "\"Construct a FeatureUnion from the given transformers.\n\nThis is a shorthand for the FeatureUnion constructor; it does not require,\nand does not permit, naming the transformers. Instead, they will be given\nnames automatically based on their types. It also does not allow weighting.\n\nExamples\n--------\n>>> from sklearn.decomposition import PCA, TruncatedSVD\n>>> make_union(PCA(), TruncatedSVD())    # doctest: +NORMALIZE_WHITESPACE\nFeatureUnion(n_jobs=1,\ntransformer_list=[('pca',\nPCA(copy=True, iterated_power='auto',\nn_components=None, random_state=None,\nsvd_solver='auto', tol=0.0, whiten=False)),\n('truncatedsvd',\nTruncatedSVD(algorithm='randomized',\nn_components=2, n_iter=5,\nrandom_state=None, tol=0.0))],\ntransformer_weights=None)\n\n",
      "id": "sklearn.pipeline.make_union",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.pipeline.make_union",
      "parameters": [],
      "returns": {
        "description": "\"",
        "name": "f",
        "type": ""
      },
      "tags": [
        "pipeline"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "File object handle used for serializing the input object. protocol: int Pickle protocol used. Default is pickle.DEFAULT_PROTOCOL under python 3, pickle.HIGHEST_PROTOCOL otherwise.",
          "name": "fp",
          "type": "file"
        }
      ],
      "common_name": "Numpy Pickler",
      "description": "'A pickler to persist big data efficiently.\n\nThe main features of this object are:\n* persistence of numpy arrays in a single file.\n* optional compression with a special care on avoiding memory copies.\n\nAttributes\n----------\nfp: file\nFile object handle used for serializing the input object.\nprotocol: int\nPickle protocol used. Default is pickle.DEFAULT_PROTOCOL under\npython 3, pickle.HIGHEST_PROTOCOL otherwise.\n'",
      "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Clears the pickler\\'s \"memo\".\n\nThe memo is the data structure that remembers which objects the\npickler has already seen, so that shared or recursive objects are\npickled by reference and not by value.  This method is useful when\nre-using picklers.\n\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.clear_memo",
          "name": "clear_memo",
          "parameters": []
        },
        {
          "description": "'Write a pickled representation of obj to the open file.'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.dump",
          "name": "dump",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.get",
          "name": "get",
          "parameters": []
        },
        {
          "description": "'Store an object in the memo.'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.memoize",
          "name": "memoize",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.persistent_id",
          "name": "persistent_id",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.put",
          "name": "put",
          "parameters": []
        },
        {
          "description": "'Subclass the Pickler `save` method.\n\nThis is a total abuse of the Pickler class in order to use the numpy\npersistence function `save` instead of the default pickle\nimplementation. The numpy array is replaced by a custom wrapper in the\npickle persistence stack and the serialized array is written right\nafter in the file. Warning: the file produced does not follow the\npickle format. As such it can not be read with `pickle.load`.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save",
          "name": "save",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_bool",
          "name": "save_bool",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_dict",
          "name": "save_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_empty_tuple",
          "name": "save_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_float",
          "name": "save_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_global",
          "name": "save_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_inst",
          "name": "save_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_int",
          "name": "save_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_list",
          "name": "save_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_long",
          "name": "save_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_none",
          "name": "save_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_pers",
          "name": "save_pers",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_reduce",
          "name": "save_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_string",
          "name": "save_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_tuple",
          "name": "save_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyPickler.save_unicode",
          "name": "save_unicode",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle.NumpyPickler",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc:200",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Zip Numpy Unpickler",
      "description": "'A subclass of the Unpickler to unpickle our numpy pickles.'",
      "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.find_class",
          "name": "find_class",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.get_extension",
          "name": "get_extension",
          "parameters": []
        },
        {
          "description": "'Read a pickled object representation from the open file.\n\nReturn the reconstituted object hierarchy specified in the file.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load",
          "name": "load",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_append",
          "name": "load_append",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_appends",
          "name": "load_appends",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binfloat",
          "name": "load_binfloat",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binget",
          "name": "load_binget",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binint",
          "name": "load_binint",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binint1",
          "name": "load_binint1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binint2",
          "name": "load_binint2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binpersid",
          "name": "load_binpersid",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binput",
          "name": "load_binput",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binstring",
          "name": "load_binstring",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_binunicode",
          "name": "load_binunicode",
          "parameters": []
        },
        {
          "description": "'Set the state of a newly created object.\n\nWe capture it to replace our place-holder objects,\nNDArrayWrapper, by the array we are interested in. We\nreplace them directly in the stack of pickler.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_build",
          "name": "load_build",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_dict",
          "name": "load_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_dup",
          "name": "load_dup",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_empty_dictionary",
          "name": "load_empty_dictionary",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_empty_list",
          "name": "load_empty_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_empty_tuple",
          "name": "load_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_eof",
          "name": "load_eof",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_ext1",
          "name": "load_ext1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_ext2",
          "name": "load_ext2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_ext4",
          "name": "load_ext4",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_false",
          "name": "load_false",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_float",
          "name": "load_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_get",
          "name": "load_get",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_global",
          "name": "load_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_inst",
          "name": "load_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_int",
          "name": "load_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_list",
          "name": "load_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_long",
          "name": "load_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_long1",
          "name": "load_long1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_long4",
          "name": "load_long4",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_long_binget",
          "name": "load_long_binget",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_long_binput",
          "name": "load_long_binput",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_mark",
          "name": "load_mark",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_newobj",
          "name": "load_newobj",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_none",
          "name": "load_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_obj",
          "name": "load_obj",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_persid",
          "name": "load_persid",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_pop",
          "name": "load_pop",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_pop_mark",
          "name": "load_pop_mark",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_proto",
          "name": "load_proto",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_put",
          "name": "load_put",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_reduce",
          "name": "load_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_setitem",
          "name": "load_setitem",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_setitems",
          "name": "load_setitems",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_short_binstring",
          "name": "load_short_binstring",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_stop",
          "name": "load_stop",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_string",
          "name": "load_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_true",
          "name": "load_true",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_tuple",
          "name": "load_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_tuple1",
          "name": "load_tuple1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_tuple2",
          "name": "load_tuple2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_tuple3",
          "name": "load_tuple3",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.load_unicode",
          "name": "load_unicode",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler.marker",
          "name": "marker",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle_compat.ZipNumpyUnpickler",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_compat.pyc:149",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_compat"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "X block weights vectors. ",
          "name": "x_weights_",
          "type": "array"
        },
        {
          "description": "Y block weights vectors. ",
          "name": "y_weights_",
          "type": "array"
        },
        {
          "description": "X scores. ",
          "name": "x_scores_",
          "type": "array"
        },
        {
          "description": "Y scores.  See also -------- PLSCanonical CCA",
          "name": "y_scores_",
          "type": "array"
        }
      ],
      "category": "cross_decomposition.pls_",
      "common_name": "PLSSVD",
      "description": "\"Partial Least Square SVD\n\nSimply perform a svd on the crosscovariance matrix: X'Y\nThere are no iterative deflation here.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n",
      "id": "sklearn.cross_decomposition.pls_.PLSSVD",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.cross_decomposition.pls_.PLSSVD.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Learn and apply the dimension reduction on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSSVD.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSSVD.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSSVD.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.'",
          "id": "sklearn.cross_decomposition.pls_.PLSSVD.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.cross_decomposition.pls_.PLSSVD",
      "parameters": [
        {
          "description": "Number of components to keep. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Whether to scale X and Y. ",
          "name": "scale",
          "type": "boolean"
        },
        {
          "description": "Whether to copy X and Y, or perform in-place computations. ",
          "name": "copy",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.pyc:746",
      "tags": [
        "cross_decomposition",
        "pls_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "kernel_approximation",
      "common_name": "RBF Sampler",
      "description": "'Approximates feature map of an RBF kernel by Monte Carlo approximation\nof its Fourier transform.\n\nIt implements a variant of Random Kitchen Sinks.[1]\n\nRead more in the :ref:`User Guide <rbf_kernel_approx>`.\n",
      "id": "sklearn.kernel_approximation.RBFSampler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model with X.\n\nSamples random projection according to n_features.\n",
          "id": "sklearn.kernel_approximation.RBFSampler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns the transformer. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.kernel_approximation.RBFSampler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.kernel_approximation.RBFSampler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.kernel_approximation.RBFSampler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the approximate feature map to X.\n",
          "id": "sklearn.kernel_approximation.RBFSampler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.kernel_approximation.RBFSampler",
      "parameters": [
        {
          "description": "Parameter of RBF kernel: exp(-gamma * x^2) ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "If int, random_state is the seed used by the random number generator; if RandomState instance, random_state is the random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "int, RandomState"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/kernel_approximation.pyc:24",
      "tags": [
        "kernel_approximation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "kernel_approximation",
      "common_name": "Skewed Chi2 Sampler",
      "description": "'Approximates feature map of the \"skewed chi-squared\" kernel by Monte\nCarlo approximation of its Fourier transform.\n\nRead more in the :ref:`User Guide <skewed_chi_kernel_approx>`.\n",
      "id": "sklearn.kernel_approximation.SkewedChi2Sampler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model with X.\n\nSamples random projection according to n_features.\n",
          "id": "sklearn.kernel_approximation.SkewedChi2Sampler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the transformer. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.kernel_approximation.SkewedChi2Sampler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.kernel_approximation.SkewedChi2Sampler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.kernel_approximation.SkewedChi2Sampler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the approximate feature map to X.\n",
          "id": "sklearn.kernel_approximation.SkewedChi2Sampler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.kernel_approximation.SkewedChi2Sampler",
      "parameters": [
        {
          "description": "\"skewedness\" parameter of the kernel. Needs to be cross-validated. ",
          "name": "skewedness",
          "type": "float"
        },
        {
          "description": "number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "If int, random_state is the seed used by the random number generator; if RandomState instance, random_state is the random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "int, RandomState"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/kernel_approximation.pyc:112",
      "tags": [
        "kernel_approximation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Dot Product",
      "description": "'Dot-Product kernel.\n\nThe DotProduct kernel is non-stationary and can be obtained from linear\nregression by putting N(0, 1) priors on the coefficients of x_d (d = 1, . .\n. , D) and a prior of N(0, \\\\sigma_0^2) on the bias. The DotProduct kernel\nis invariant to a rotation of the coordinates about the origin, but not\ntranslations. It is parameterized by a parameter sigma_0^2. For\nsigma_0^2 =0, the kernel is called the homogeneous linear kernel, otherwise\nit is inhomogeneous. The kernel is given by\n\nk(x_i, x_j) = sigma_0 ^ 2 + x_i \\\\cdot x_j\n\nThe DotProduct kernel is commonly combined with exponentiation.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.DotProduct",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.DotProduct.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.DotProduct.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.DotProduct.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.DotProduct.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.DotProduct.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.DotProduct",
      "parameters": [
        {
          "description": "Parameter controlling the inhomogenity of the kernel. If sigma_0=0, the kernel is homogenous.  sigma_0_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on l  '",
          "name": "sigma_0",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1619",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Rational Quadratic",
      "description": "'Rational Quadratic kernel.\n\nThe RationalQuadratic kernel can be seen as a scale mixture (an infinite\nsum) of RBF kernels with different characteristic length-scales. It is\nparameterized by a length-scale parameter length_scale>0 and a scale\nmixture parameter alpha>0. Only the isotropic variant where length_scale is\na scalar is supported at the moment. The kernel given by:\n\nk(x_i, x_j) = (1 + d(x_i, x_j)^2 / (2*alpha * length_scale^2))^-alpha\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.RationalQuadratic",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.RationalQuadratic.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.RationalQuadratic.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.RationalQuadratic.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.RationalQuadratic.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.RationalQuadratic.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.RationalQuadratic",
      "parameters": [
        {
          "description": "The length scale of the kernel. ",
          "name": "length_scale",
          "type": "float"
        },
        {
          "description": "Scale mixture parameter ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on length_scale ",
          "name": "length_scale_bounds",
          "type": "pair"
        },
        {
          "description": "The lower and upper bound on alpha  '",
          "name": "alpha_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1395",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Leave P Groups Out",
      "description": "'Leave P Group(s) Out cross-validator\n\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and LeaveOneGroupOut is that\nthe former builds the test sets with all the samples assigned to\n``p`` different values of the groups while the latter uses samples\nall assigned the same groups.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.LeavePGroupsOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.LeavePGroupsOut.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ``np.zeros(n_samples)`` may be used as a placeholder. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.LeavePGroupsOut.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.LeavePGroupsOut",
      "parameters": [
        {
          "description": "Number of groups (``p``) to leave out in the test split. ",
          "name": "n_groups",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:814",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Time Series Split",
      "description": "'Time Series cross-validator\n\nProvides train/test indices to split time series data samples\nthat are observed at fixed time intervals, in train/test sets.\nIn each split, test indices must be higher than before, and thus shuffling\nin cross validator is inappropriate.\n\nThis cross-validation object is a variation of :class:`KFold`.\nIn the kth split, it returns first k folds as train set and the\n(k+1)th fold as test set.\n\nNote that unlike standard cross-validation methods, successive\ntraining sets are supersets of those that come before them.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.TimeSeriesSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.TimeSeriesSplit.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.TimeSeriesSplit.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.TimeSeriesSplit",
      "parameters": [
        {
          "description": "Number of splits. Must be at least 1. ",
          "name": "n_splits",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:646",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "the current learning rate ",
          "name": "learning_rate",
          "type": "float"
        },
        {
          "description": "velocities that are used to update params",
          "name": "velocities",
          "type": "list"
        }
      ],
      "category": "neural_network._stochastic_optimizers",
      "common_name": "SGD Optimizer",
      "description": "\"Stochastic gradient descent optimizer with momentum\n",
      "id": "sklearn.neural_network._stochastic_optimizers.SGDOptimizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Perform updates to learning rate and potential other states at the\nend of an iteration\n",
          "id": "sklearn.neural_network._stochastic_optimizers.SGDOptimizer.iteration_ends",
          "name": "iteration_ends",
          "parameters": [
            {
              "description": "number of training samples trained on so far, used to update learning rate for 'invscaling' \"",
              "name": "time_step",
              "type": "int"
            }
          ]
        },
        {
          "description": "None",
          "id": "sklearn.neural_network._stochastic_optimizers.SGDOptimizer.trigger_stopping",
          "name": "trigger_stopping",
          "parameters": []
        },
        {
          "description": "'Update parameters with given gradients\n",
          "id": "sklearn.neural_network._stochastic_optimizers.SGDOptimizer.update_params",
          "name": "update_params",
          "parameters": [
            {
              "description": "Containing gradients with respect to coefs_ and intercepts_ in MLP model. So length should be aligned with params '",
              "name": "grads",
              "type": "list"
            }
          ]
        }
      ],
      "name": "sklearn.neural_network._stochastic_optimizers.SGDOptimizer",
      "parameters": [
        {
          "description": "The concatenated list containing coefs_ and intercepts_ in MLP model. Used for initializing velocities and updating params ",
          "name": "params",
          "type": "list"
        },
        {
          "description": "The initial learning rate used. It controls the step-size in updating the weights ",
          "name": "learning_rate_init",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Learning rate schedule for weight updates.  -'constant', is a constant learning rate given by 'learning_rate_init'.  -'invscaling' gradually decreases the learning rate 'learning_rate_' at each time step 't' using an inverse scaling exponent of 'power_t'. learning_rate_ = learning_rate_init / pow(t, power_t)  -'adaptive', keeps the learning rate constant to 'learning_rate_init' as long as the training keeps decreasing. Each time 2 consecutive epochs fail to decrease the training loss by tol, or fail to increase validation score by tol if 'early_stopping' is on, the current learning rate is divided by 5. ",
          "name": "lr_schedule",
          "type": "'constant', 'adaptive', 'invscaling'"
        },
        {
          "description": "Value of momentum used, must be larger than or equal to 0 ",
          "name": "momentum",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Whether to use nesterov's momentum or not. Use nesterov's if True ",
          "name": "nesterov",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/_stochastic_optimizers.pyc:74",
      "tags": [
        "neural_network",
        "_stochastic_optimizers"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.cubic",
      "description": "\"\nCubic correlation model::\n\ntheta, d --> r(theta, d) =\nn\nprod max(0, 1 - 3(theta_j*d_ij)^2 + 2(theta_j*d_ij)^3) ,  i = 1,...,m\nj = 1\n",
      "id": "sklearn.gaussian_process.correlation_models.cubic",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.cubic",
      "parameters": [
        {
          "description": "An array with shape 1 (isotropic) or n (anisotropic) giving the autocorrelation parameter(s). ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) with the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.squared_exponential",
      "description": "\"\nSquared exponential correlation model (Radial Basis Function).\n(Infinitely differentiable stochastic process, very smooth)::\n\nn\ntheta, d --> r(theta, d) = exp(  sum  - theta_i * (d_i)^2 )\ni = 1\n",
      "id": "sklearn.gaussian_process.correlation_models.squared_exponential",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.squared_exponential",
      "parameters": [
        {
          "description": "An array with shape 1 (isotropic) or n (anisotropic) giving the autocorrelation parameter(s). ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) containing the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.axis0_safe_slice",
      "description": "'\nThis mask is safer than safe_mask since it returns an\nempty array, when a sparse matrix is sliced with a boolean mask\nwith all False, instead of raising an unhelpful error in older\nversions of SciPy.\n\nSee: https://github.com/scipy/scipy/issues/5361\n\nAlso note that we can avoid doing the dot product by checking if\nthe len_mask is not zero in _huber_loss_and_gradient but this\nis not going to be the bottleneck, since the number of outliers\nand non_outliers are typically non-zero and it makes the code\ntougher to follow.\n'",
      "id": "sklearn.utils.axis0_safe_slice",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.axis0_safe_slice",
      "parameters": [],
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.cartesian",
      "description": "'Generate a cartesian product of input arrays.\n",
      "id": "sklearn.utils.extmath.cartesian",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.cartesian",
      "parameters": [
        {
          "description": "1-D arrays to form the cartesian product of.",
          "name": "arrays",
          "type": "list"
        },
        {
          "description": "Array to place the cartesian product in. ",
          "name": "out",
          "type": "ndarray"
        }
      ],
      "returns": {
        "description": "2-D array of shape (M, len(arrays)) containing cartesian products formed of input arrays.  Examples -------- >>> cartesian(([1, 2, 3], [4, 5], [6, 7])) array([[1, 4, 6], [1, 4, 7], [1, 5, 6], [1, 5, 7], [2, 4, 6], [2, 4, 7], [2, 5, 6], [2, 5, 7], [3, 4, 6], [3, 4, 7], [3, 5, 6], [3, 5, 7]])  '",
        "name": "out",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.core.numeric.array_equal",
      "description": "'\nTrue if two arrays have the same shape and elements, False otherwise.\n",
      "id": "numpy.core.numeric.array_equal",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.core.numeric.array_equal",
      "parameters": [
        {
          "description": "Input arrays. ",
          "name": "a1, a2",
          "type": "array"
        }
      ],
      "returns": {
        "description": "Returns True if the arrays are equal.  See Also -------- allclose: Returns True if two arrays are element-wise equal within a tolerance. array_equiv: Returns True if input arrays are shape consistent and all elements equal.  Examples -------- >>> np.array_equal([1, 2], [1, 2]) True >>> np.array_equal(np.array([1, 2]), np.array([1, 2])) True >>> np.array_equal([1, 2], [1, 2, 3]) False >>> np.array_equal([1, 2], [1, 4]) False  '",
        "name": "b",
        "type": "bool"
      },
      "tags": [
        "core",
        "numeric"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_boston",
      "description": "\"Load and return the boston house-prices dataset (regression).\n\n==============     ==============\nSamples total                 506\nDimensionality                 13\nFeatures           real, positive\nTargets             real 5. - 50.\n==============     ==============\n",
      "id": "sklearn.datasets.base.load_boston",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_boston",
      "parameters": [
        {
          "description": "If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "return_X_y",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn, 'target', the regression targets, and 'DESCR', the full description of the dataset.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18  Examples -------- >>> from sklearn.datasets import load_boston >>> boston = load_boston() >>> print(boston.data.shape) (506, 13) \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cross_validation",
      "common_name": "Label Shuffle Split",
      "description": "'Shuffle-Labels-Out cross-validation iterator\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.GroupShuffleSplit` instead.\n\nProvides randomized train/test indices to split data according to a\nthird-party provided label. This label information can be used to encode\narbitrary domain specific stratifications of the samples as integers.\n\nFor instance the labels could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePLabelOut and LabelShuffleSplit is that\nthe former generates splits using all subsets of size ``p`` unique labels,\nwhereas LabelShuffleSplit generates a user-determined number of random\ntest splits, each with a user-determined fraction of unique labels.\n\nFor example, a less computationally intensive alternative to\n``LeavePLabelOut(labels, p=10)`` would be\n``LabelShuffleSplit(labels, test_size=10, n_iter=100)``.\n\nNote: The parameters ``test_size`` and ``train_size`` refer to labels, and\nnot to samples, as in ShuffleSplit.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.cross_validation.LabelShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.cross_validation.LabelShuffleSplit",
      "parameters": [
        {
          "description": "Labels of samples ",
          "name": "labels",
          "type": "array"
        },
        {
          "description": "Number of re-shuffling and splitting iterations. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the labels to include in the test split. If int, represents the absolute number of test labels. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the labels to include in the train split. If int, represents the absolute number of train labels. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling.  '",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc:1180",
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Exp Sine Squared",
      "description": "'Exp-Sine-Squared kernel.\n\nThe ExpSineSquared kernel allows modeling periodic functions. It is\nparameterized by a length-scale parameter length_scale>0 and a periodicity\nparameter periodicity>0. Only the isotropic variant where l is a scalar is\nsupported at the moment. The kernel given by:\n\nk(x_i, x_j) = exp(-2 sin(\\\\pi / periodicity * d(x_i, x_j)) / length_scale)^2\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.ExpSineSquared",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.ExpSineSquared.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.ExpSineSquared.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.ExpSineSquared.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.ExpSineSquared.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.ExpSineSquared.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.ExpSineSquared",
      "parameters": [
        {
          "description": "The length scale of the kernel. ",
          "name": "length_scale",
          "type": "float"
        },
        {
          "description": "The periodicity of the kernel. ",
          "name": "periodicity",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on length_scale ",
          "name": "length_scale_bounds",
          "type": "pair"
        },
        {
          "description": "The lower and upper bound on periodicity  '",
          "name": "periodicity_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1507",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "grid_search",
      "common_name": "Parameter Sampler",
      "description": "\"Generator on parameters sampled from given distributions.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.ParameterSampler` instead.\n\nNon-deterministic iterable over random candidate combinations for hyper-\nparameter search. If all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nNote that as of SciPy 0.12, the ``scipy.stats.distributions`` do not accept\na custom RNG instance and always use the singleton RNG from\n``numpy.random``. Hence setting ``random_state`` will not guarantee a\ndeterministic iteration whenever ``scipy.stats`` distributions are used to\ndefine the parameter search space.\n\nRead more in the :ref:`User Guide <grid_search>`.\n",
      "id": "sklearn.grid_search.ParameterSampler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.grid_search.ParameterSampler",
      "parameters": [
        {
          "description": "Dictionary where the keys are parameters and values are distributions from which a parameter is to be sampled. Distributions either have to provide a ``rvs`` function to sample from them, or can be given as a list of values, where a uniform distribution is assumed. ",
          "name": "param_distributions",
          "type": "dict"
        },
        {
          "description": "Number of parameter settings that are produced. ",
          "name": "n_iter",
          "type": "integer"
        },
        {
          "description": "Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions.  Returns -------",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "**Yields** dictionaries mapping each estimator parameter to as sampled value. ",
          "name": "params",
          "type": "dict"
        }
      ],
      "returns": {
        "description": "**Yields** dictionaries mapping each estimator parameter to as sampled value.  Examples -------- >>> from sklearn.grid_search import ParameterSampler >>> from scipy.stats.distributions import expon >>> import numpy as np >>> np.random.seed(0) >>> param_grid = {'a':[1, 2], 'b': expon()} >>> param_list = list(ParameterSampler(param_grid, n_iter=4)) >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items()) ...                 for d in param_list] >>> rounded_list == [{'b': 0.89856, 'a': 1}, ...                  {'b': 0.923223, 'a': 1}, ...                  {'b': 1.878964, 'a': 2}, ...                  {'b': 1.038159, 'a': 2}] True \"",
        "name": "params",
        "type": "dict"
      },
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc:169",
      "tags": [
        "grid_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "model_selection._search",
      "common_name": "Parameter Sampler",
      "description": "\"Generator on parameters sampled from given distributions.\n\nNon-deterministic iterable over random candidate combinations for hyper-\nparameter search. If all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nNote that before SciPy 0.16, the ``scipy.stats.distributions`` do not\naccept a custom RNG instance and always use the singleton RNG from\n``numpy.random``. Hence setting ``random_state`` will not guarantee a\ndeterministic iteration whenever ``scipy.stats`` distributions are used to\ndefine the parameter search space. Deterministic behavior is however\nguaranteed from SciPy 0.16 onwards.\n\nRead more in the :ref:`User Guide <search>`.\n",
      "id": "sklearn.model_selection._search.ParameterSampler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.model_selection._search.ParameterSampler",
      "parameters": [
        {
          "description": "Dictionary where the keys are parameters and values are distributions from which a parameter is to be sampled. Distributions either have to provide a ``rvs`` function to sample from them, or can be given as a list of values, where a uniform distribution is assumed. ",
          "name": "param_distributions",
          "type": "dict"
        },
        {
          "description": "Number of parameter settings that are produced. ",
          "name": "n_iter",
          "type": "integer"
        },
        {
          "description": "Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions.  Returns -------",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "**Yields** dictionaries mapping each estimator parameter to as sampled value. ",
          "name": "params",
          "type": "dict"
        }
      ],
      "returns": {
        "description": "**Yields** dictionaries mapping each estimator parameter to as sampled value.  Examples -------- >>> from sklearn.model_selection import ParameterSampler >>> from scipy.stats.distributions import expon >>> import numpy as np >>> np.random.seed(0) >>> param_grid = {'a':[1, 2], 'b': expon()} >>> param_list = list(ParameterSampler(param_grid, n_iter=4)) >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items()) ...                 for d in param_list] >>> rounded_list == [{'b': 0.89856, 'a': 1}, ...                  {'b': 0.923223, 'a': 1}, ...                  {'b': 1.878964, 'a': 2}, ...                  {'b': 1.038159, 'a': 2}] True \"",
        "name": "params",
        "type": "dict"
      },
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc:164",
      "tags": [
        "model_selection",
        "_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "feature_extraction.image",
      "common_name": "Patch Extractor",
      "description": "'Extracts patches from a collection of images\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.image.PatchExtractor",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Do nothing and return the estimator unchanged\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n'",
          "id": "sklearn.feature_extraction.image.PatchExtractor.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.image.PatchExtractor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.image.PatchExtractor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transforms the image samples in X into a matrix of patch data.\n",
          "id": "sklearn.feature_extraction.image.PatchExtractor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "(n_samples, image_height, image_width, n_channels) Array of images from which to extract patches. For color images, the last dimension specifies the channel: a RGB image would have `n_channels=3`. ",
              "name": "X",
              "shape": "n_samples, image_height, image_width",
              "type": "array"
            }
          ],
          "returns": {
            "description": "(n_patches, patch_height, patch_width, n_channels) The collection of patches extracted from the images, where `n_patches` is either `n_samples * max_patches` or the total number of patches that can be extracted.  '",
            "name": "patches: array, shape = (n_patches, patch_height, patch_width) or"
          }
        }
      ],
      "name": "sklearn.feature_extraction.image.PatchExtractor",
      "parameters": [
        {
          "description": "the dimensions of one patch ",
          "name": "patch_size",
          "type": "tuple"
        },
        {
          "description": "The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches. ",
          "name": "max_patches",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Pseudo number generator state used for random sampling.  '",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/image.pyc:438",
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Leave P Out",
      "description": "'Leave-P-Out cross-validator\n\nProvides train/test indices to split data in train/test sets. This results\nin testing on all distinct samples of size p, while the remaining n - p\nsamples form the training set in each iteration.\n\nNote: ``LeavePOut(p)`` is NOT equivalent to\n``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n\nDue to the high number of iterations which grows combinatorically with the\nnumber of samples this cross-validation method can be very costly. For\nlarge datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\nor :class:`ShuffleSplit`.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.LeavePOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.LeavePOut.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. '",
              "name": "groups",
              "type": "object"
            }
          ]
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.LeavePOut.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.LeavePOut",
      "parameters": [
        {
          "description": "Size of the test sets. ",
          "name": "p",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:194",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Stratified K Fold",
      "description": "'Stratified K-Folds cross-validator\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a variation of KFold that returns\nstratified folds. The folds are made by preserving the percentage of\nsamples for each class.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.StratifiedKFold",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.StratifiedKFold.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.StratifiedKFold.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. Stratification is done based on the y labels. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.StratifiedKFold",
      "parameters": [
        {
          "description": "Number of folds. Must be at least 2. ",
          "name": "n_splits",
          "type": "int"
        },
        {
          "description": "Whether to shuffle each stratification of the data before splitting into batches. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "When shuffle=True, pseudo-random number generator state used for shuffling. If None, use default numpy RNG for shuffling. ",
          "name": "random_state",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:511",
      "tags": [
        "model_selection",
        "_split"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.testing.utils.assert_equal",
      "description": "\"\nRaises an AssertionError if two objects are not equal.\n\nGiven two objects (scalars, lists, tuples, dictionaries or numpy arrays),\ncheck that all elements of these objects are equal. An exception is raised\nat the first conflicting values.\n",
      "id": "numpy.testing.utils.assert_equal",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.testing.utils.assert_equal",
      "parameters": [
        {
          "description": "The object to check.",
          "name": "actual",
          "type": "array"
        },
        {
          "description": "The expected object.",
          "name": "desired",
          "type": "array"
        },
        {
          "description": "The error message to be printed in case of failure.",
          "name": "err_msg",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "If True, the conflicting values are appended to the error message.  Raises ------ AssertionError If actual and desired are not equal.  Examples -------- >>> np.testing.assert_equal([4,5], [4,6]) ... <type 'exceptions.AssertionError'>: Items are not equal: item=1 ACTUAL: 5 DESIRED: 6  \"",
          "name": "verbose",
          "optional": "true",
          "type": "bool"
        }
      ],
      "tags": [
        "testing",
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.construct.hstack",
      "description": "'\nStack sparse matrices horizontally (column wise)\n",
      "id": "scipy.sparse.construct.hstack",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.construct.hstack",
      "parameters": [
        {
          "description": "sparse format of the result (e.g. \"csr\") by default an appropriate sparse matrix format is returned. This choice is subject to change.",
          "name": "format",
          "type": "str"
        },
        {
          "description": "The data-type of the output matrix.  If not given, the dtype is determined from that of `blocks`.  See Also --------",
          "name": "dtype",
          "optional": "true",
          "type": "dtype"
        },
        {
          "description": " Examples -------- >>> from scipy.sparse import coo_matrix, hstack >>> A = coo_matrix([[1, 2], [3, 4]]) >>> B = coo_matrix([[5], [6]]) >>> hstack([A,B]).toarray() array([[1, 2, 5], [3, 4, 6]])  '",
          "name": "vstack",
          "type": "stack"
        }
      ],
      "tags": [
        "sparse",
        "construct"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.unsupervised.calinski_harabaz_score",
      "description": "'Compute the Calinski and Harabaz score.\n\nThe score is defined as ratio between the within-cluster dispersion and\nthe between-cluster dispersion.\n\nRead more in the :ref:`User Guide <calinski_harabaz_index>`.\n",
      "id": "sklearn.metrics.cluster.unsupervised.calinski_harabaz_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.unsupervised.calinski_harabaz_score",
      "parameters": [
        {
          "description": "List of ``n_features``-dimensional data points. Each row corresponds to a single data point. ",
          "name": "X",
          "shape": "``n_samples``, ``n_features``",
          "type": "array-like"
        },
        {
          "description": "Predicted labels for each sample. ",
          "name": "labels",
          "shape": "``n_samples``,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "The resulting Calinski-Harabaz score.  References ---------- .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster analysis\". Communications in Statistics <http://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_ '",
        "name": "score: float"
      },
      "tags": [
        "metrics",
        "cluster",
        "unsupervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.scorer.check_scoring",
      "description": "\"Determine scorer from user options.\n\nA TypeError will be thrown if the estimator cannot be scored.\n",
      "id": "sklearn.metrics.scorer.check_scoring",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.scorer.check_scoring",
      "parameters": [
        {
          "description": "The object to use to fit the data. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "If no scoring is specified and the estimator has no score function, we can either return None or raise an exception. ",
          "name": "allow_none",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "A scorer callable object / function with signature ``scorer(estimator, X, y)``. \"",
        "name": "scoring",
        "type": "callable"
      },
      "tags": [
        "metrics",
        "scorer"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.isotonic.check_increasing",
      "description": "'Determine whether y is monotonically correlated with x.\n\ny is found increasing or decreasing with respect to x based on a Spearman\ncorrelation test.\n",
      "id": "sklearn.isotonic.check_increasing",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.isotonic.check_increasing",
      "parameters": [
        {
          "description": "Training data. ",
          "name": "x",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Training target. ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Whether the relationship is increasing or decreasing.  Notes ----- The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result.  In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised.  References ---------- Fisher transformation. Wikipedia. https://en.wikipedia.org/wiki/Fisher_transformation '",
        "name": "`increasing_bool`",
        "type": "boolean"
      },
      "tags": [
        "isotonic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.sample_gaussian",
      "description": "\"DEPRECATED: The function sample_gaussian is deprecated in 0.18 and will be removed in 0.20. Use numpy.random.multivariate_normal instead.\n\nGenerate random samples from a Gaussian distribution.\n",
      "id": "sklearn.utils.deprecation.sample_gaussian",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.sample_gaussian",
      "parameters": [
        {
          "description": "Mean of the distribution. ",
          "name": "mean",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Covariance of the distribution. The shape depends on `covariance_type`: scalar if 'spherical', (n_features) if 'diag', (n_features, n_features)  if 'tied', or 'full' ",
          "name": "covar",
          "optional": "true",
          "type": "array"
        },
        {
          "description": "Type of the covariance parameters. Must be one of 'spherical', 'tied', 'diag', 'full'. Defaults to 'diag'. ",
          "name": "covariance_type",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of samples to generate. Defaults to 1. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Randomly generated sample \"",
        "name": "X",
        "shape": "n_features, n_samples",
        "type": "array"
      },
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.construct.identity",
      "description": "'Identity matrix in sparse format\n\nReturns an identity matrix with shape (n,n) using a given\nsparse format and dtype.\n",
      "id": "scipy.sparse.construct.identity",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.construct.identity",
      "parameters": [
        {
          "description": "Shape of the identity matrix.",
          "name": "n",
          "type": "int"
        },
        {
          "description": "Data type of the matrix",
          "name": "dtype",
          "optional": "true",
          "type": "dtype"
        },
        {
          "description": "Sparse format of the result, e.g. format=\"csr\", etc.  Examples -------- >>> from scipy.sparse import identity >>> identity(3).toarray() array([[ 1.,  0.,  0.], [ 0.,  1.,  0.], [ 0.,  0.,  1.]]) >>> identity(3, dtype=\\'int8\\', format=\\'dia\\') <3x3 sparse matrix of type \\'<type \\'numpy.int8\\'>\\' with 3 stored elements (1 diagonals) in DIAgonal format>  '",
          "name": "format",
          "optional": "true",
          "type": "str"
        }
      ],
      "tags": [
        "sparse",
        "construct"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.locally_linear.barycenter_weights",
      "description": "'Compute barycenter weights of X from Y along the first axis\n\nWe estimate the weights to assign to each point in Y[i] to recover\nthe point X[i]. The barycenter weights sum to 1.\n",
      "id": "sklearn.manifold.locally_linear.barycenter_weights",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.locally_linear.barycenter_weights",
      "parameters": [
        {
          "description": " Z : array-like, shape (n_samples, n_neighbors, n_dim)  reg: float, optional amount of regularization to add for the problem to be well-posed in the case of n_neighbors > n_dim ",
          "name": "X",
          "shape": "n_samples, n_dim",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " Notes ----- See developers note for more information. '",
        "name": "B",
        "shape": "n_samples, n_neighbors",
        "type": "array-like"
      },
      "tags": [
        "manifold",
        "locally_linear"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.log_logistic",
      "description": "'Compute the log of the logistic function, ``log(1 / (1 + e ** -x))``.\n\nThis implementation is numerically stable because it splits positive and\nnegative values::\n\n-log(1 + exp(-x_i))     if x_i > 0\nx_i - log(1 + exp(x_i)) if x_i <= 0\n\nFor the ordinary logistic function, use ``sklearn.utils.fixes.expit``.\n",
      "id": "sklearn.utils.extmath.log_logistic",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.log_logistic",
      "parameters": [
        {
          "description": "Argument to the logistic function  out: array-like, shape: (M, N) or (M, ), optional: Preallocated output array. ",
          "name": "X",
          "shape": "M, N",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Log of the logistic function evaluated at every point in x  Notes ----- See the blog post describing this implementation: http://fa.bianp.net/blog/2013/numerical-optimizers-for-logistic-regression/ '",
        "name": "out: array, shape (M, N) or (M, )"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.gen_even_slices",
      "description": "'Generator to create n_packs slices going up to n.\n\nPass n_samples when the slices are to be used for sparse matrix indexing;\nslicing off-the-end raises an exception, while it works for NumPy arrays.\n\nExamples\n--------\n>>> from sklearn.utils import gen_even_slices\n>>> list(gen_even_slices(10, 1))\n[slice(0, 10, None)]\n>>> list(gen_even_slices(10, 10))                     #doctest: +ELLIPSIS\n[slice(0, 1, None), slice(1, 2, None), ..., slice(9, 10, None)]\n>>> list(gen_even_slices(10, 5))                      #doctest: +ELLIPSIS\n[slice(0, 2, None), slice(2, 4, None), ..., slice(8, 10, None)]\n>>> list(gen_even_slices(10, 3))\n[slice(0, 4, None), slice(4, 7, None), slice(7, 10, None)]\n'",
      "id": "sklearn.utils.gen_even_slices",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.gen_even_slices",
      "parameters": [],
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.optimize.linesearch.line_search_wolfe1",
      "description": "'\nAs `scalar_search_wolfe1` but do a line search to direction `pk`\n",
      "id": "scipy.optimize.linesearch.line_search_wolfe1",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.optimize.linesearch.line_search_wolfe1",
      "parameters": [
        {
          "description": "Function `f(x)`",
          "name": "f",
          "type": "callable"
        },
        {
          "description": "Gradient of `f`",
          "name": "fprime",
          "type": "callable"
        },
        {
          "description": "Current point",
          "name": "xk",
          "type": "array"
        },
        {
          "description": "Search direction ",
          "name": "pk",
          "type": "array"
        },
        {
          "description": "Gradient of `f` at point `xk`",
          "name": "gfk",
          "optional": "true",
          "type": "array"
        },
        {
          "description": "Value of `f` at point `xk`",
          "name": "old_fval",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Value of `f` at point preceding `xk`  The rest of the parameters are the same as for `scalar_search_wolfe1`. ",
          "name": "old_old_fval",
          "optional": "true",
          "type": "float"
        }
      ],
      "returns": {
        "description": "As in `line_search_wolfe1` gval : array Gradient of `f` at the final point  '",
        "name": "stp, f_count, g_count, fval, old_fval"
      },
      "tags": [
        "optimize",
        "linesearch"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.as_float_array",
      "description": "\"Converts an array-like to an array of floats\n\nThe new dtype will be np.float32 or np.float64, depending on the original\ntype. The function can create a copy or modify the argument depending\non the argument copy.\n",
      "id": "sklearn.utils.validation.as_float_array",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.as_float_array",
      "parameters": [
        {
          "description": "",
          "name": "X",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "If True, a copy of X will be created. If False, a copy may still be returned if X's dtype is not a floating point type. ",
          "name": "copy",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to raise an error on np.inf and np.nan in X. ",
          "name": "force_all_finite",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "An array of type np.float \"",
        "name": "XT",
        "type": "array, sparse matrix"
      },
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_cholesky.cho_solve",
      "description": "'Solve the linear equations A x = b, given the Cholesky factorization of A.\n",
      "id": "scipy.linalg.decomp_cholesky.cho_solve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_cholesky.cho_solve",
      "parameters": [
        {
          "description": "Cholesky factorization of a, as given by cho_factor",
          "name": "(c, lower)",
          "type": "tuple"
        },
        {
          "description": "Right-hand side",
          "name": "b",
          "type": "array"
        },
        {
          "description": "Whether to overwrite data in b (may improve performance)",
          "name": "overwrite_b",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrices contain only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The solution to the system A x = b  See also -------- cho_factor : Cholesky factorization of a matrix  '",
        "name": "x",
        "type": "array"
      },
      "tags": [
        "linalg",
        "decomp_cholesky"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.absolute_exponential",
      "description": "\"\nAbsolute exponential autocorrelation model.\n(Ornstein-Uhlenbeck stochastic process)::\n\nn\ntheta, d --> r(theta, d) = exp(  sum  - theta_i * |d_i| )\ni = 1\n",
      "id": "sklearn.gaussian_process.correlation_models.absolute_exponential",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.absolute_exponential",
      "parameters": [
        {
          "description": "An array with shape 1 (isotropic) or n (anisotropic) giving the autocorrelation parameter(s). ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) containing the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.gaussian_process.correlation_models.generalized_exponential",
      "description": "\"\nGeneralized exponential correlation model.\n(Useful when one does not know the smoothness of the function to be\npredicted.)::\n\nn\ntheta, d --> r(theta, d) = exp(  sum  - theta_i * |d_i|^p )\ni = 1\n",
      "id": "sklearn.gaussian_process.correlation_models.generalized_exponential",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.gaussian_process.correlation_models.generalized_exponential",
      "parameters": [
        {
          "description": "An array with shape 1+1 (isotropic) or n+1 (anisotropic) giving the autocorrelation parameter(s) (theta, p). ",
          "name": "theta",
          "type": "array"
        },
        {
          "description": "An array with shape (n_eval, n_features) giving the componentwise distances between locations x and x' at which the correlation model should be evaluated. ",
          "name": "d",
          "type": "array"
        }
      ],
      "returns": {
        "description": "An array with shape (n_eval, ) with the values of the autocorrelation model. \"",
        "name": "r",
        "type": "array"
      },
      "tags": [
        "gaussian_process",
        "correlation_models"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Binary Zlib File",
      "description": "\"A file object providing transparent zlib (de)compression.\n\nA BinaryZlibFile can act as a wrapper for an existing file object, or refer\ndirectly to a named file on disk.\n\nNote that BinaryZlibFile provides only a *binary* file interface: data read\nis returned as bytes, and data to be written should be given as bytes.\n\nThis object is an adaptation of the BZ2File object and is compatible with\nversions of python >= 2.6.\n\nIf filename is a str or bytes object, it gives the name\nof the file to be opened. Otherwise, it should be a file object,\nwhich will be used to read or write the compressed data.\n\nmode can be 'rb' for reading (default) or 'wb' for (over)writing\n\nIf mode is 'wb', compresslevel can be a number between 1\nand 9 specifying the level of compression: 1 produces the least\ncompression, and 9 (default) produces the most compression.\n\"",
      "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Flush and close the file.\n\nMay be called more than once without error. Once the file is\nclosed, any other operation on it will raise a ValueError.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.close",
          "name": "close",
          "parameters": []
        },
        {
          "description": "'Return the file descriptor for the underlying file.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.fileno",
          "name": "fileno",
          "parameters": []
        },
        {
          "description": "\"Read up to size uncompressed bytes from the file.\n\nIf size is negative or omitted, read until EOF is reached.\nReturns b'' if the file is already at EOF.\n\"",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.read",
          "name": "read",
          "parameters": []
        },
        {
          "description": "'Return whether the file was opened for reading.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.readable",
          "name": "readable",
          "parameters": []
        },
        {
          "description": "'Read up to len(b) bytes into b.\n\nReturns the number of bytes read (0 for EOF).\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.readinto",
          "name": "readinto",
          "parameters": []
        },
        {
          "description": "'Change the file position.\n\nThe new position is specified by offset, relative to the\nposition indicated by whence. Values for whence are:\n\n0: start of stream (default); offset must not be negative\n1: current stream position\n2: end of stream; offset must not be positive\n\nReturns the new file position.\n\nNote that seeking is emulated, so depending on the parameters,\nthis operation may be extremely slow.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.seek",
          "name": "seek",
          "parameters": []
        },
        {
          "description": "'Return whether the file supports seeking.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.seekable",
          "name": "seekable",
          "parameters": []
        },
        {
          "description": "'Return the current file position.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.tell",
          "name": "tell",
          "parameters": []
        },
        {
          "description": "'Return whether the file was opened for writing.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.writable",
          "name": "writable",
          "parameters": []
        },
        {
          "description": "'Write a byte string to the file.\n\nReturns the number of uncompressed bytes written, which is\nalways len(data). Note that due to buffering, the file on disk\nmay not reflect the data written until close() is called.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile.write",
          "name": "write",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle_utils.BinaryZlibFile",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_utils.pyc:247",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_utils"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores, None if `score_func` returned only scores. ",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Select Percentile",
      "description": "'Select features according to a percentile of the highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.SelectPercentile",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectPercentile.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.SelectPercentile",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below \"See also\"). The default function only works with classification tasks. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "Percent of features to keep. ",
          "name": "percentile",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:349",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters. ",
          "name": "named_steps",
          "type": "dict"
        }
      ],
      "category": "pipeline",
      "common_name": "Pipeline",
      "description": "\"Pipeline of transforms with a final estimator.\n\nSequentially apply a list of transforms and a final estimator.\nIntermediate steps of the pipeline must be 'transforms', that is, they\nmust implement fit and transform methods.\nThe final estimator only needs to implement fit.\n\nThe purpose of the pipeline is to assemble several steps that can be\ncross-validated together while setting different parameters.\nFor this, it enables setting parameters of the various steps using their\nnames and the parameter name separated by a '__', as in the example below.\nA step's estimator may be replaced entirely by setting the parameter\nwith its name to another estimator, or a transformer removed by setting\nto None.\n\nRead more in the :ref:`User Guide <pipeline>`.\n",
      "id": "sklearn.pipeline.Pipeline",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model\n\nFit all the transforms one after the other and transform the\ndata, then fit the transformed data using the final estimator.\n",
          "id": "sklearn.pipeline.Pipeline.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Must fulfill input requirements of first step of the pipeline. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "Training targets. Must fulfill label requirements for all steps of the pipeline.  **fit_params : dict of string -> object Parameters passed to the ``fit`` method of each step, where each parameter name is prefixed such that parameter ``p`` for step ``s`` has key ``s__p``. ",
              "name": "y",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "This estimator '",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Fit the model and transform with the final estimator\n\nFits all the transforms one after the other and transforms the\ndata, then uses fit_transform on transformed data with the final\nestimator.\n",
          "id": "sklearn.pipeline.Pipeline.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training data. Must fulfill input requirements of first step of the pipeline. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "Training targets. Must fulfill label requirements for all steps of the pipeline.  **fit_params : dict of string -> object Parameters passed to the ``fit`` method of each step, where each parameter name is prefixed such that parameter ``p`` for step ``s`` has key ``s__p``. ",
              "name": "y",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "Transformed samples '",
            "name": "Xt",
            "shape": "n_samples, n_transformed_features",
            "type": "array-like"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.pipeline.Pipeline.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``.\n",
          "id": "sklearn.pipeline.Pipeline.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.pipeline.Pipeline",
      "parameters": [
        {
          "description": "List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator. ",
          "name": "steps",
          "type": "list"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc:86",
      "tags": [
        "pipeline"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "preprocessing.data",
      "common_name": "Polynomial Features",
      "description": "'Generate polynomial and interaction features.\n\nGenerate a new feature matrix consisting of all polynomial combinations\nof the features with degree less than or equal to the specified degree.\nFor example, if an input sample is two dimensional and of the form\n[a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
      "id": "sklearn.preprocessing.data.PolynomialFeatures",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nCompute number of output features.\n'",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'\nReturn feature names for output features\n",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.get_feature_names",
          "name": "get_feature_names",
          "parameters": [
            {
              "description": "String names for input features if available. By default, \"x0\", \"x1\", ... \"xn_features\" is used. ",
              "name": "input_features",
              "optional": "true",
              "type": "list"
            }
          ],
          "returns": {
            "description": " '",
            "name": "output_feature_names",
            "type": "list"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform data to polynomial features\n",
          "id": "sklearn.preprocessing.data.PolynomialFeatures.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data to transform, row by row. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The matrix of features, where NP is the number of polynomial features generated from the combination of inputs. '",
            "name": "XP",
            "shape": "n_samples, NP",
            "type": "np"
          }
        }
      ],
      "name": "sklearn.preprocessing.data.PolynomialFeatures",
      "parameters": [
        {
          "description": "The degree of the polynomial features. Default = 2. ",
          "name": "degree",
          "type": "integer"
        },
        {
          "description": "If true, only interaction features are produced: features that are products of at most ``degree`` *distinct* input features (so not ``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.). ",
          "name": "interaction_only",
          "type": "boolean"
        },
        {
          "description": "If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).  Examples -------- >>> X = np.arange(6).reshape(3, 2) >>> X array([[0, 1], [2, 3], [4, 5]]) >>> poly = PolynomialFeatures(2) >>> poly.fit_transform(X) array([[  1.,   0.,   1.,   0.,   0.,   1.], [  1.,   2.,   3.,   4.,   6.,   9.], [  1.,   4.,   5.,  16.,  20.,  25.]]) >>> poly = PolynomialFeatures(interaction_only=True) >>> poly.fit_transform(X) array([[  1.,   0.,   1.,   0.], [  1.,   2.,   3.,   6.], [  1.,   4.,   5.,  20.]]) ",
          "name": "include_bias",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:1139",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Leave One Out",
      "description": "'Leave-One-Out cross-validator\n\nProvides train/test indices to split data in train/test sets. Each\nsample is used once as a test set (singleton) while the remaining\nsamples form the training set.\n\nNote: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n``LeavePOut(p=1)`` where ``n`` is the number of samples.\n\nDue to the high number of test sets (which is the same as the\nnumber of samples) this cross-validation method can be very costly.\nFor large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\nor :class:`StratifiedKFold`.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nExamples\n--------\n>>> from sklearn.model_selection import LeaveOneOut\n>>> X = np.array([[1, 2], [3, 4]])\n>>> y = np.array([1, 2])\n>>> loo = LeaveOneOut()\n>>> loo.get_n_splits(X)\n2\n>>> print(loo)\nLeaveOneOut()\n>>> for train_index, test_index in loo.split(X):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\n...    print(X_train, X_test, y_train, y_test)\nTRAIN: [1] TEST: [0]\n[[3 4]] [[1 2]] [2] [1]\nTRAIN: [0] TEST: [1]\n[[1 2]] [[3 4]] [1] [2]\n\nSee also\n--------\nLeaveOneGroupOut\nFor splitting the data according to explicit, domain-specific\nstratification of the dataset.\n\nGroupKFold: K-fold iterator variant with non-overlapping groups.\n'",
      "id": "sklearn.model_selection._split.LeaveOneOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.LeaveOneOut.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.LeaveOneOut.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.LeaveOneOut",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:120",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Binary Gzip File",
      "description": "\"A file object providing transparent gzip (de)compression.\n\nIf filename is a str or bytes object, it gives the name\nof the file to be opened. Otherwise, it should be a file object,\nwhich will be used to read or write the compressed data.\n\nmode can be 'rb' for reading (default) or 'wb' for (over)writing\n\nIf mode is 'wb', compresslevel can be a number between 1\nand 9 specifying the level of compression: 1 produces the least\ncompression, and 9 (default) produces the most compression.\n\"",
      "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Flush and close the file.\n\nMay be called more than once without error. Once the file is\nclosed, any other operation on it will raise a ValueError.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.close",
          "name": "close",
          "parameters": []
        },
        {
          "description": "'Return the file descriptor for the underlying file.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.fileno",
          "name": "fileno",
          "parameters": []
        },
        {
          "description": "\"Read up to size uncompressed bytes from the file.\n\nIf size is negative or omitted, read until EOF is reached.\nReturns b'' if the file is already at EOF.\n\"",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.read",
          "name": "read",
          "parameters": []
        },
        {
          "description": "'Return whether the file was opened for reading.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.readable",
          "name": "readable",
          "parameters": []
        },
        {
          "description": "'Read up to len(b) bytes into b.\n\nReturns the number of bytes read (0 for EOF).\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.readinto",
          "name": "readinto",
          "parameters": []
        },
        {
          "description": "'Change the file position.\n\nThe new position is specified by offset, relative to the\nposition indicated by whence. Values for whence are:\n\n0: start of stream (default); offset must not be negative\n1: current stream position\n2: end of stream; offset must not be positive\n\nReturns the new file position.\n\nNote that seeking is emulated, so depending on the parameters,\nthis operation may be extremely slow.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.seek",
          "name": "seek",
          "parameters": []
        },
        {
          "description": "'Return whether the file supports seeking.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.seekable",
          "name": "seekable",
          "parameters": []
        },
        {
          "description": "'Return the current file position.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.tell",
          "name": "tell",
          "parameters": []
        },
        {
          "description": "'Return whether the file was opened for writing.'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.writable",
          "name": "writable",
          "parameters": []
        },
        {
          "description": "'Write a byte string to the file.\n\nReturns the number of uncompressed bytes written, which is\nalways len(data). Note that due to buffering, the file on disk\nmay not reflect the data written until close() is called.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile.write",
          "name": "write",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle_utils.BinaryGzipFile",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle_utils.pyc:561",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle_utils"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores. ",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Select Fdr",
      "description": "'Filter: Select the p-values for an estimated false discovery rate\n\nThis uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\non the expected false discovery rate.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.SelectFdr",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFdr.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.SelectFdr",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below \"See also\"). The default function only works with classification tasks. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "The highest uncorrected p-value for features to keep.  ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:544",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores, None if `score_func` returned only scores. ",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Select K Best",
      "description": "'Select features according to the k highest scores.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.SelectKBest",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectKBest.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.SelectKBest",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below \"See also\"). The default function only works with classification tasks. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "Number of top features to select. The \"all\" option bypasses selection, for use in a parameter search. ",
          "name": "k",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:422",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Pairwise Kernel",
      "description": "'Wrapper for kernels in sklearn.metrics.pairwise.\n\nA thin wrapper around the functionality of the kernels in\nsklearn.metrics.pairwise.\n\nNote: Evaluation of eval_gradient is not analytic but numeric and all\nkernels support only isotropic distances. The parameter gamma is\nconsidered to be a hyperparameter and may be optimized. The other\nkernel parameters are set directly at initialization and are kept\nfixed.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.PairwiseKernel",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.PairwiseKernel.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.PairwiseKernel.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.PairwiseKernel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.PairwiseKernel.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.PairwiseKernel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.PairwiseKernel",
      "parameters": [
        {
          "description": "Parameter gamma of the pairwise kernel specified by metric ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on gamma ",
          "name": "gamma_bounds",
          "type": "pair"
        },
        {
          "description": "The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel function.  '",
          "name": "pairwise_kernels_kwargs",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1741",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Group Shuffle Split",
      "description": "'Shuffle-Group(s)-Out cross-validation iterator\n\nProvides randomized train/test indices to split data according to a\nthird-party provided group. This group information can be used to encode\narbitrary domain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nThe difference between LeavePGroupsOut and GroupShuffleSplit is that\nthe former generates splits using all subsets of size ``p`` unique groups,\nwhereas GroupShuffleSplit generates a user-determined number of random\ntest splits, each with a user-determined fraction of unique groups.\n\nFor example, a less computationally intensive alternative to\n``LeavePGroupsOut(p=10)`` would be\n``GroupShuffleSplit(test_size=10, n_splits=100)``.\n\nNote: The parameters ``test_size`` and ``train_size`` refer to groups, and\nnot to samples, as in ShuffleSplit.\n\n",
      "id": "sklearn.model_selection._split.GroupShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.GroupShuffleSplit.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.GroupShuffleSplit.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.GroupShuffleSplit",
      "parameters": [
        {
          "description": "Number of re-shuffling & splitting iterations. ",
          "name": "n_splits",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the test split. If int, represents the absolute number of test groups. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. '",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:1055",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [
        {
          "description": "Estimators used for predictions.",
          "name": "estimators_",
          "type": "list"
        }
      ],
      "category": "multioutput",
      "common_name": "Multi Output Classifier",
      "description": "'Multi target classification\n\nThis strategy consists of fitting one classifier per target. This is a\nsimple strategy for extending classifiers that do not natively support\nmulti-target classification\n",
      "id": "sklearn.multioutput.MultiOutputClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Fit the model to data.\nFit a separate model for each output variable.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-output targets. An indicator matrix turns on multilabel estimation. ",
              "name": "y",
              "shape": "n_samples, n_outputs",
              "type": ""
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict multi-output variable using a model\ntrained for each target variable.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor. '",
            "name": "y",
            "shape": "n_samples, n_outputs",
            "type": ""
          }
        },
        {
          "description": "'Probability estimates.\nReturns prediction probabilites for each class of each output.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The class probabilities of the samples for each of the outputs '",
            "name": "T",
            "shape": "n_samples, n_classes, n_outputs",
            "type": ""
          }
        },
        {
          "description": "'\"Returns the mean accuracy on the given test data and labels.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X ",
              "name": "y",
              "shape": "n_samples, n_outputs",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "accuracy_score of self.predict(X) versus y '",
            "name": "scores",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multioutput.MultiOutputClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multioutput.MultiOutputClassifier",
      "parameters": [
        {
          "description": "An estimator object implementing `fit`, `score` and `predict_proba`. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. The number of jobs to use for the computation. It does each target variable in y in parallel. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multioutput.pyc:177",
      "tags": [
        "multioutput"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "multioutput",
      "common_name": "Multi Output Regressor",
      "description": "'Multi target regression\n\nThis strategy consists of fitting one regressor per target. This is a\nsimple strategy for extending regressors that do not natively support\nmulti-target regression.\n",
      "id": "sklearn.multioutput.MultiOutputRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Fit the model to data.\nFit a separate model for each output variable.\n",
          "id": "sklearn.multioutput.MultiOutputRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-output targets. An indicator matrix turns on multilabel estimation. ",
              "name": "y",
              "shape": "n_samples, n_outputs",
              "type": ""
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multioutput.MultiOutputRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict multi-output variable using a model\ntrained for each target variable.\n",
          "id": "sklearn.multioutput.MultiOutputRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor. '",
            "name": "y",
            "shape": "n_samples, n_outputs",
            "type": ""
          }
        },
        {
          "description": "\"Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n\nNotes\n-----\nR^2 is calculated by weighting all the targets equally using\n`multioutput='uniform_average'`.\n",
          "id": "sklearn.multioutput.MultiOutputRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. \"",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multioutput.MultiOutputRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multioutput.MultiOutputRegressor",
      "parameters": [
        {
          "description": "An estimator object implementing `fit` and `predict`. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The number of jobs to run in parallel for `fit`. If -1, then the number of jobs is set to the number of cores. When individual estimators are fast to train or predict using `n_jobs>1` can result in slower performance due to the overhead of spawning processes. '",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multioutput.pyc:117",
      "tags": [
        "multioutput"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "bayesian"
      ],
      "attributes": [],
      "category": "preprocessing.data",
      "common_name": "Binarizer",
      "description": "'Binarize data (set feature values to 0 or 1) according to a threshold\n\nValues greater than the threshold map to 1, while values less than\nor equal to the threshold map to 0. With the default threshold of 0,\nonly positive values map to 1.\n\nBinarization is a common operation on text count data where the\nanalyst can decide to only consider the presence or absence of a\nfeature rather than a quantified number of occurrences for instance.\n\nIt can also be used as a pre-processing step for estimators that\nconsider boolean random variables (e.g. modelled using the Bernoulli\ndistribution in a Bayesian setting).\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "id": "sklearn.preprocessing.data.Binarizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Do nothing and return the estimator unchanged\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n'",
          "id": "sklearn.preprocessing.data.Binarizer.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.Binarizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.Binarizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.Binarizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Binarize each element of X\n",
          "id": "sklearn.preprocessing.data.Binarizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.Binarizer",
      "parameters": [
        {
          "description": "Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices. ",
          "name": "threshold",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:1487",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [],
      "category": "svm.base",
      "common_name": "Base Lib SVM",
      "description": "'Base class for estimators that use libsvm as backing library\n\nThis implements support vector machine classification and regression.\n\nParameter documentation is in the derived `SVC` class.\n'",
      "id": "sklearn.svm.base.BaseLibSVM",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDistance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.base.BaseLibSVM.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train]. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. '",
            "name": "X",
            "shape": "n_samples, n_class * (n_class-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.base.BaseLibSVM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.base.BaseLibSVM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform regression on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.base.BaseLibSVM.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is (n_samples_test, n_samples_train). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.base.BaseLibSVM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.base.BaseLibSVM",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc:61",
      "tags": [
        "svm",
        "base"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "The original, undecorated, function.  cachedir: string Path to the base cache directory of the memory context.  ignore: list or None List of variable names to ignore when choosing whether to recompute.  mmap_mode: {None, 'r+', 'r', 'w+', 'c'} The memmapping mode used when loading from cache numpy arrays. See numpy.load for the meaning of the different values.  compress: boolean, or integer Whether to zip the stored data on disk. If an integer is given, it should be between 1 and 9, and sets the amount of compression. Note that compressed arrays cannot be read by memmapping.  verbose: int, optional The verbosity flag, controls messages that are issued as the function is evaluated.",
          "name": "func",
          "type": "callable"
        }
      ],
      "common_name": "Memorized Func",
      "description": "\" Callable object decorating a function for caching its return value\neach time it is called.\n\nAll values are cached on the filesystem, in a deep directory\nstructure. Methods are provided to inspect the cache or clean it.\n\nAttributes\n----------\nfunc: callable\nThe original, undecorated, function.\n\ncachedir: string\nPath to the base cache directory of the memory context.\n\nignore: list or None\nList of variable names to ignore when choosing whether to\nrecompute.\n\nmmap_mode: {None, 'r+', 'r', 'w+', 'c'}\nThe memmapping mode used when loading from cache\nnumpy arrays. See numpy.load for the meaning of the different\nvalues.\n\ncompress: boolean, or integer\nWhether to zip the stored data on disk. If an integer is\ngiven, it should be between 1 and 9, and sets the amount\nof compression. Note that compressed arrays cannot be\nread by memmapping.\n\nverbose: int, optional\nThe verbosity flag, controls messages that are issued as\nthe function is evaluated.\n\"",
      "id": "sklearn.externals.joblib.memory.MemorizedFunc",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Force the execution of the function with the given arguments and\npersist the output values.\n'",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.call",
          "name": "call",
          "parameters": []
        },
        {
          "description": "'Call wrapped function, cache result and return a reference.\n\nThis method returns a reference to the cached result instead of the\nresult itself. The reference object is small and pickeable, allowing\nto send or store it easily. Call .get() on reference object to get\nresult.\n",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.call_and_shelve",
          "name": "call_and_shelve",
          "parameters": [],
          "returns": {
            "description": "reference to the value returned by the wrapped function. The class \"NotMemorizedResult\" is used when there is no cache activated (e.g. cachedir=None in Memory). '",
            "name": "cached_result: MemorizedResult or NotMemorizedResult"
          }
        },
        {
          "description": "\" Empty the function's cache.\n\"",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.clear",
          "name": "clear",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.debug",
          "name": "debug",
          "parameters": []
        },
        {
          "description": "' Return the formated representation of the object.\n'",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.format",
          "name": "format",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.format_call",
          "name": "format_call",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.format_signature",
          "name": "format_signature",
          "parameters": []
        },
        {
          "description": "' Read the results of a previous calculation from the directory\nit was cached in.\n'",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.load_output",
          "name": "load_output",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.memory.MemorizedFunc.warn",
          "name": "warn",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.memory.MemorizedFunc",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/memory.pyc:305",
      "tags": [
        "externals",
        "joblib",
        "memory"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Determine the subclass of the wrapped array. shape: numpy.ndarray shape Determine the shape of the wrapped array. order: {'C', 'F'} Determine the order of wrapped array data. 'C' is for C order, 'F' is for fortran order. dtype: numpy.ndarray dtype Determine the data type of the wrapped array. allow_mmap: bool Determine if memory mapping is allowed on the wrapped array. Default: False.",
          "name": "subclass",
          "type": "numpy"
        }
      ],
      "common_name": "Numpy Array Wrapper",
      "description": "\"An object to be persisted instead of numpy arrays.\n\nThis object is used to hack into the pickle machinery and read numpy\narray data from our custom persistence format.\nMore precisely, this object is used for:\n* carrying the information of the persisted array: subclass, shape, order,\ndtype. Those ndarray metadata are used to correctly reconstruct the array\nwith low level numpy functions.\n* determining if memmap is allowed on the array.\n* reading the array bytes from a file.\n* reading the array using memorymap from a file.\n* writing the array bytes to a file.\n\nAttributes\n----------\nsubclass: numpy.ndarray subclass\nDetermine the subclass of the wrapped array.\nshape: numpy.ndarray shape\nDetermine the shape of the wrapped array.\norder: {'C', 'F'}\nDetermine the order of wrapped array data. 'C' is for C order, 'F' is\nfor fortran order.\ndtype: numpy.ndarray dtype\nDetermine the data type of the wrapped array.\nallow_mmap: bool\nDetermine if memory mapping is allowed on the wrapped array.\nDefault: False.\n\"",
      "id": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Read the array corresponding to this wrapper.\n\nUse the unpickler to get all information to correctly read the array.\n",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read",
          "name": "read",
          "parameters": [
            {
              "description": "",
              "name": "unpickler",
              "type": ""
            }
          ],
          "returns": {
            "description": " '",
            "name": "array: numpy.ndarray"
          }
        },
        {
          "description": "'Read array from unpickler file handle.\n\nThis function is an adaptation of the numpy read_array function\navailable in version 1.10.1 in numpy/lib/format.py.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read_array",
          "name": "read_array",
          "parameters": []
        },
        {
          "description": "'Read an array using numpy memmap.'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.read_mmap",
          "name": "read_mmap",
          "parameters": []
        },
        {
          "description": "'Write array bytes to pickler file handle.\n\nThis function is an adaptation of the numpy write_array function\navailable in version 1.10.1 in numpy/lib/format.py.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper.write_array",
          "name": "write_array",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle.NumpyArrayWrapper",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc:34",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "The memorymap mode to use for reading numpy arrays. file_handle: file_like File object to unpickle from. filename: str Name of the file to unpickle from. It should correspond to file_handle. This parameter is required when using mmap_mode. np: module Reference to numpy module if numpy is installed else None. ",
          "name": "mmap_mode",
          "type": "str"
        }
      ],
      "common_name": "Numpy Unpickler",
      "description": "'A subclass of the Unpickler to unpickle our numpy pickles.\n\nAttributes\n----------\nmmap_mode: str\nThe memorymap mode to use for reading numpy arrays.\nfile_handle: file_like\nFile object to unpickle from.\nfilename: str\nName of the file to unpickle from. It should correspond to file_handle.\nThis parameter is required when using mmap_mode.\nnp: module\nReference to numpy module if numpy is installed else None.\n\n'",
      "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.find_class",
          "name": "find_class",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.get_extension",
          "name": "get_extension",
          "parameters": []
        },
        {
          "description": "'Read a pickled object representation from the open file.\n\nReturn the reconstituted object hierarchy specified in the file.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load",
          "name": "load",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_append",
          "name": "load_append",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_appends",
          "name": "load_appends",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binfloat",
          "name": "load_binfloat",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binget",
          "name": "load_binget",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binint",
          "name": "load_binint",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binint1",
          "name": "load_binint1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binint2",
          "name": "load_binint2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binpersid",
          "name": "load_binpersid",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binput",
          "name": "load_binput",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binstring",
          "name": "load_binstring",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_binunicode",
          "name": "load_binunicode",
          "parameters": []
        },
        {
          "description": "'Called to set the state of a newly created object.\n\nWe capture it to replace our place-holder objects, NDArrayWrapper or\nNumpyArrayWrapper, by the array we are interested in. We\nreplace them directly in the stack of pickler.\nNDArrayWrapper is used for backward compatibility with joblib <= 0.9.\n'",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_build",
          "name": "load_build",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_dict",
          "name": "load_dict",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_dup",
          "name": "load_dup",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_empty_dictionary",
          "name": "load_empty_dictionary",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_empty_list",
          "name": "load_empty_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_empty_tuple",
          "name": "load_empty_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_eof",
          "name": "load_eof",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_ext1",
          "name": "load_ext1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_ext2",
          "name": "load_ext2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_ext4",
          "name": "load_ext4",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_false",
          "name": "load_false",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_float",
          "name": "load_float",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_get",
          "name": "load_get",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_global",
          "name": "load_global",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_inst",
          "name": "load_inst",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_int",
          "name": "load_int",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_list",
          "name": "load_list",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_long",
          "name": "load_long",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_long1",
          "name": "load_long1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_long4",
          "name": "load_long4",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_long_binget",
          "name": "load_long_binget",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_long_binput",
          "name": "load_long_binput",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_mark",
          "name": "load_mark",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_newobj",
          "name": "load_newobj",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_none",
          "name": "load_none",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_obj",
          "name": "load_obj",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_persid",
          "name": "load_persid",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_pop",
          "name": "load_pop",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_pop_mark",
          "name": "load_pop_mark",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_proto",
          "name": "load_proto",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_put",
          "name": "load_put",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_reduce",
          "name": "load_reduce",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_setitem",
          "name": "load_setitem",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_setitems",
          "name": "load_setitems",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_short_binstring",
          "name": "load_short_binstring",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_stop",
          "name": "load_stop",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_string",
          "name": "load_string",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_true",
          "name": "load_true",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_tuple",
          "name": "load_tuple",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_tuple1",
          "name": "load_tuple1",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_tuple2",
          "name": "load_tuple2",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_tuple3",
          "name": "load_tuple3",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.load_unicode",
          "name": "load_unicode",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler.marker",
          "name": "marker",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.numpy_pickle.NumpyUnpickler",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc:283",
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "RBF",
      "description": "'Radial-basis function kernel (aka squared-exponential kernel).\n\nThe RBF kernel is a stationary kernel. It is also known as the\n\"squared exponential\" kernel. It is parameterized by a length-scale\nparameter length_scale>0, which can either be a scalar (isotropic variant\nof the kernel) or a vector with the same number of dimensions as the inputs\nX (anisotropic variant of the kernel). The kernel is given by:\n\nk(x_i, x_j) = exp(-1 / 2 d(x_i / length_scale, x_j / length_scale)^2)\n\nThis kernel is infinitely differentiable, which implies that GPs with this\nkernel as covariance function have mean square derivatives of all orders,\nand are thus very smooth.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.RBF",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.RBF.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.RBF.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.RBF.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.RBF.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.RBF.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.RBF",
      "parameters": [
        {
          "description": "The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension. ",
          "name": "length_scale",
          "shape": "n_features,",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on length_scale  '",
          "name": "length_scale_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1131",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Variances of individual features. ",
          "name": "variances_",
          "shape": "n_features,",
          "type": "array"
        }
      ],
      "category": "feature_selection.variance_threshold",
      "common_name": "Variance Threshold",
      "description": "'Feature selector that removes all low-variance features.\n\nThis feature selection algorithm looks only at the features (X), not the\ndesired outputs (y), and can thus be used for unsupervised learning.\n\nRead more in the :ref:`User Guide <variance_threshold>`.\n",
      "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Learn empirical variances from X.\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Sample vectors from which to compute variances. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline. ",
              "name": "y",
              "type": "any"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.variance_threshold.VarianceThreshold.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.variance_threshold.VarianceThreshold",
      "parameters": [
        {
          "description": "Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples. ",
          "name": "threshold",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/variance_threshold.pyc:12",
      "tags": [
        "feature_selection",
        "variance_threshold"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "mixture.base",
      "common_name": "Base Mixture",
      "description": "'Base class for mixture models.\n\nThis abstract class specifies an interface for all mixture classes and\nprovides basic common methods for mixture models.\n'",
      "id": "sklearn.mixture.base.BaseMixture",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Estimate model parameters with the EM algorithm.\n\nThe method fit the model `n_init` times and set the parameters with\nwhich the model has the largest likelihood or lower bound. Within each\ntrial, the method iterates between E-step and M-step for `max_iter`\ntimes until the change of likelihood or lower bound is less than\n`tol`, otherwise, a `ConvergenceWarning` is raised.\n",
          "id": "sklearn.mixture.base.BaseMixture.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.base.BaseMixture.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the labels for the data samples in X using trained model.\n",
          "id": "sklearn.mixture.base.BaseMixture.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Component labels. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data per each component.\n",
          "id": "sklearn.mixture.base.BaseMixture.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "resp",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        },
        {
          "description": "'Generate random samples from the fitted Gaussian distribution.\n",
          "id": "sklearn.mixture.base.BaseMixture.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "Randomly generated sample  y : array, shape (nsamples,) Component labels  '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the per-sample average log-likelihood of the given data X.\n",
          "id": "sklearn.mixture.base.BaseMixture.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log likelihood of the Gaussian mixture given X. '",
            "name": "log_likelihood",
            "type": "float"
          }
        },
        {
          "description": "'Compute the weighted log probabilities for each sample.\n",
          "id": "sklearn.mixture.base.BaseMixture.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X. '",
            "name": "log_prob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.base.BaseMixture.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.base.BaseMixture",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/base.pyc:66",
      "tags": [
        "mixture",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "K Neighbors Mixin",
      "description": "'Mixin for k-neighbors searches'",
      "id": "sklearn.neighbors.base.KNeighborsMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Finds the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.\n",
          "id": "sklearn.neighbors.base.KNeighborsMixin.kneighbors",
          "name": "kneighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors to get (default is the value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the lengths to points, only present if return_distance=True  ind : array Indices of the nearest points in the population matrix.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1,1,1]  >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=1) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS (array([[ 0.5]]), array([[2]]...))  As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:  >>> X = [[0., 1., 0.], [1., 0., 1.]] >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS array([[1], [2]]...)  \"",
            "name": "dist",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n",
          "id": "sklearn.neighbors.base.KNeighborsMixin.kneighbors_graph",
          "name": "kneighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors for each sample. (default is value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=2) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.kneighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- NearestNeighbors.radius_neighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples_fit",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.neighbors.base.KNeighborsMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:266",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [
        {
          "description": "Centroid of each class ",
          "name": "centroids_",
          "shape": "n_classes, n_features",
          "type": "array-like"
        }
      ],
      "category": "neighbors.nearest_centroid",
      "common_name": "Nearest Centroid",
      "description": "'Nearest centroid classifier.\n\nEach class is represented by its centroid, with test samples classified to\nthe class with the nearest centroid.\n\nRead more in the :ref:`User Guide <nearest_centroid_classifier>`.\n",
      "id": "sklearn.neighbors.nearest_centroid.NearestCentroid",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nFit the NearestCentroid model according to the given training data.\n",
          "id": "sklearn.neighbors.nearest_centroid.NearestCentroid.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (integers) '",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.nearest_centroid.NearestCentroid.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on an array of test vectors X.\n\nThe predicted class C for each sample in X is returned.\n",
          "id": "sklearn.neighbors.nearest_centroid.NearestCentroid.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- If the metric constructor parameter is \"precomputed\", X is assumed to be the distance matrix between the data to be predicted and ``self.centroids_``. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.neighbors.nearest_centroid.NearestCentroid.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.nearest_centroid.NearestCentroid.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.nearest_centroid.NearestCentroid",
      "parameters": [
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the \"manhattan\" metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Threshold for shrinking centroids to remove features. ",
          "name": "shrink_threshold",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/nearest_centroid.pyc:22",
      "tags": [
        "neighbors",
        "nearest_centroid"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [],
      "category": "preprocessing.data",
      "common_name": "Normalizer",
      "description": "\"Normalize samples individually to unit norm.\n\nEach sample (i.e. each row of the data matrix) with at least one\nnon zero component is rescaled independently of other samples so\nthat its norm (l1 or l2) equals one.\n\nThis transformer is able to work both with dense numpy arrays and\nscipy.sparse matrix (use CSR format if you want to avoid the burden of\na copy / conversion).\n\nScaling inputs to unit norms is a common operation for text\nclassification or clustering for instance. For instance the dot\nproduct of two l2-normalized TF-IDF vectors is the cosine similarity\nof the vectors and is the base similarity metric for the Vector\nSpace Model commonly used by the Information Retrieval community.\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "id": "sklearn.preprocessing.data.Normalizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Do nothing and return the estimator unchanged\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n'",
          "id": "sklearn.preprocessing.data.Normalizer.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.Normalizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.Normalizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.Normalizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Scale each non zero row of X to unit norm\n",
          "id": "sklearn.preprocessing.data.Normalizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.Normalizer",
      "parameters": [
        {
          "description": "The norm to use to normalize each non zero sample. ",
          "name": "norm",
          "optional": "true",
          "type": ""
        },
        {
          "description": "set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:1377",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Concrete number of components computed when n_components=\"auto\". ",
          "name": "n_component_",
          "type": "int"
        },
        {
          "description": "Random matrix used for the projection.  See Also -------- SparseRandomProjection ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": "numpy"
        }
      ],
      "category": "random_projection",
      "common_name": "Gaussian Random Projection",
      "description": "'Reduce dimensionality through Gaussian random projection\n\nThe components of the random matrix are drawn from N(0, 1 / n_components).\n\nRead more in the :ref:`User Guide <gaussian_random_matrix>`.\n",
      "id": "sklearn.random_projection.GaussianRandomProjection",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Generate a sparse random projection matrix\n",
          "id": "sklearn.random_projection.GaussianRandomProjection.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": " '",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.random_projection.GaussianRandomProjection.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.random_projection.GaussianRandomProjection.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.random_projection.GaussianRandomProjection.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Project the data by using matrix product with the random matrix\n",
          "id": "sklearn.random_projection.GaussianRandomProjection.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input data to project into a smaller dimensional space. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": "Projected array.  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.random_projection.GaussianRandomProjection",
      "parameters": [
        {
          "description": "Dimensionality of the target projection space.  n_components can be automatically adjusted according to the number of samples in the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the quality of the embedding is controlled by the ``eps`` parameter.  It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.1",
          "description": "Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to \\'auto\\'.  Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space. ",
          "name": "eps",
          "optional": "true",
          "type": "strictly"
        },
        {
          "description": "Control the pseudo random number generator used to generate the matrix at fit time. ",
          "name": "random_state",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/random_projection.pyc:425",
      "tags": [
        "random_projection"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.arpack",
      "common_name": "Iter Inv",
      "description": "'\nIterInv:\nhelper class to repeatedly solve M*x=b\nusing an iterative method.\n'",
      "id": "sklearn.utils.arpack.IterInv",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Hermitian adjoint.\n\nReturns the Hermitian adjoint of self, aka the Hermitian\nconjugate or Hermitian transpose. For a complex matrix, the\nHermitian adjoint is equal to the conjugate transpose.\n\nCan be abbreviated self.H instead of self.adjoint().\n",
          "id": "sklearn.utils.arpack.IterInv.adjoint",
          "name": "adjoint",
          "parameters": [],
          "returns": {
            "description": "Hermitian adjoint of self. '",
            "name": "A_H",
            "type": ""
          }
        },
        {
          "description": "'Matrix-matrix or matrix-vector multiplication.\n",
          "id": "sklearn.utils.arpack.IterInv.dot",
          "name": "dot",
          "parameters": [
            {
              "description": "1-d or 2-d array, representing a vector or matrix. ",
              "name": "x",
              "type": "array"
            }
          ],
          "returns": {
            "description": "1-d or 2-d array (depending on the shape of x) that represents the result of applying this linear operator on x.  '",
            "name": "Ax",
            "type": "array"
          }
        },
        {
          "description": "'Matrix-matrix multiplication.\n\nPerforms the operation y=A*X where A is an MxN linear\noperator and X dense N*K matrix or ndarray.\n",
          "id": "sklearn.utils.arpack.IterInv.matmat",
          "name": "matmat",
          "parameters": [
            {
              "description": "An array with shape (N,K). ",
              "name": "X",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,K) depending on the type of the X argument.  Notes ----- This matmat wraps any user-specified matmat routine or overridden _matmat method to ensure that y has the correct type.  '",
            "name": "Y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Matrix-vector multiplication.\n\nPerforms the operation y=A*x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.IterInv.matvec",
          "name": "matvec",
          "parameters": [
            {
              "description": "An array with shape (N,) or (N,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,) or (M,1) depending on the type and shape of the x argument.  Notes ----- This matvec wraps the user-specified matvec routine or overridden _matvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Adjoint matrix-vector multiplication.\n\nPerforms the operation y = A^H * x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.IterInv.rmatvec",
          "name": "rmatvec",
          "parameters": [
            {
              "description": "An array with shape (M,) or (M,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (N,) or (N,1) depending on the type and shape of the x argument.  Notes ----- This rmatvec wraps the user-specified rmatvec routine or overridden _rmatvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Transpose this linear operator.\n\nReturns a LinearOperator that represents the transpose of this one.\nCan be abbreviated self.T instead of self.transpose().\n'",
          "id": "sklearn.utils.arpack.IterInv.transpose",
          "name": "transpose",
          "parameters": []
        }
      ],
      "name": "sklearn.utils.arpack.IterInv",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:993",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.arpack",
      "common_name": "Lu Inv",
      "description": "'\nLuInv:\nhelper class to repeatedly solve M*x=b\nusing an LU-decomposition of M\n'",
      "id": "sklearn.utils.arpack.LuInv",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Hermitian adjoint.\n\nReturns the Hermitian adjoint of self, aka the Hermitian\nconjugate or Hermitian transpose. For a complex matrix, the\nHermitian adjoint is equal to the conjugate transpose.\n\nCan be abbreviated self.H instead of self.adjoint().\n",
          "id": "sklearn.utils.arpack.LuInv.adjoint",
          "name": "adjoint",
          "parameters": [],
          "returns": {
            "description": "Hermitian adjoint of self. '",
            "name": "A_H",
            "type": ""
          }
        },
        {
          "description": "'Matrix-matrix or matrix-vector multiplication.\n",
          "id": "sklearn.utils.arpack.LuInv.dot",
          "name": "dot",
          "parameters": [
            {
              "description": "1-d or 2-d array, representing a vector or matrix. ",
              "name": "x",
              "type": "array"
            }
          ],
          "returns": {
            "description": "1-d or 2-d array (depending on the shape of x) that represents the result of applying this linear operator on x.  '",
            "name": "Ax",
            "type": "array"
          }
        },
        {
          "description": "'Matrix-matrix multiplication.\n\nPerforms the operation y=A*X where A is an MxN linear\noperator and X dense N*K matrix or ndarray.\n",
          "id": "sklearn.utils.arpack.LuInv.matmat",
          "name": "matmat",
          "parameters": [
            {
              "description": "An array with shape (N,K). ",
              "name": "X",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,K) depending on the type of the X argument.  Notes ----- This matmat wraps any user-specified matmat routine or overridden _matmat method to ensure that y has the correct type.  '",
            "name": "Y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Matrix-vector multiplication.\n\nPerforms the operation y=A*x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.LuInv.matvec",
          "name": "matvec",
          "parameters": [
            {
              "description": "An array with shape (N,) or (N,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,) or (M,1) depending on the type and shape of the x argument.  Notes ----- This matvec wraps the user-specified matvec routine or overridden _matvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Adjoint matrix-vector multiplication.\n\nPerforms the operation y = A^H * x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.LuInv.rmatvec",
          "name": "rmatvec",
          "parameters": [
            {
              "description": "An array with shape (M,) or (M,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (N,) or (N,1) depending on the type and shape of the x argument.  Notes ----- This rmatvec wraps the user-specified rmatvec routine or overridden _rmatvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Transpose this linear operator.\n\nReturns a LinearOperator that represents the transpose of this one.\nCan be abbreviated self.T instead of self.transpose().\n'",
          "id": "sklearn.utils.arpack.LuInv.transpose",
          "name": "transpose",
          "parameters": []
        }
      ],
      "name": "sklearn.utils.arpack.LuInv",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:978",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.label_ranking_loss",
      "description": "'Compute Ranking loss measure\n\nCompute the average number of label pairs that are incorrectly ordered\ngiven y_score weighted by the size of the label set and the number of\nlabels not in the label set.\n\nThis is similar to the error set size, but weighted by the number of\nrelevant and irrelevant labels. The best performance is achieved with\na ranking loss of zero.\n\nRead more in the :ref:`User Guide <label_ranking_loss>`.\n\n.. versionadded:: 0.17\nA function *label_ranking_loss*\n",
      "id": "sklearn.metrics.ranking.label_ranking_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.label_ranking_loss",
      "parameters": [
        {
          "description": "True binary labels in binary indicator format. ",
          "name": "y_true",
          "shape": "n_samples, n_labels",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples, n_labels",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.  '",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.mean_shift_.estimate_bandwidth",
      "description": "\"Estimate the bandwidth to use with the mean-shift algorithm.\n\nThat this function takes time at least quadratic in n_samples. For large\ndatasets, it's wise to set that parameter to a small value.\n",
      "id": "sklearn.cluster.mean_shift_.estimate_bandwidth",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.mean_shift_.estimate_bandwidth",
      "parameters": [
        {
          "description": "Input points. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "should be between [0, 1] 0.5 means that the median of all pairwise distances is used. ",
          "name": "quantile",
          "type": "float"
        },
        {
          "description": "The number of samples to use. If not given, all samples are used. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The bandwidth parameter. \"",
        "name": "bandwidth",
        "type": "float"
      },
      "tags": [
        "cluster",
        "mean_shift_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.multiclass.unique_labels",
      "description": "'Extract an ordered array of unique labels\n\nWe don\\'t allow:\n- mix of multilabel and multiclass (single label) targets\n- mix of label indicator matrix and anything else,\nbecause there are no explicit labels)\n- mix of label indicator matrices of different sizes\n- mix of string and integer labels\n\nAt the moment, we also don\\'t allow \"multiclass-multioutput\" input type.\n",
      "id": "sklearn.utils.multiclass.unique_labels",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.multiclass.unique_labels",
      "parameters": [
        {
          "description": "",
          "name": "*ys",
          "type": "array-likes"
        }
      ],
      "returns": {
        "description": "An ordered array of unique labels.  Examples -------- >>> from sklearn.utils.multiclass import unique_labels >>> unique_labels([3, 5, 5, 5, 7, 7]) array([3, 5, 7]) >>> unique_labels([1, 2, 3, 4], [2, 2, 3, 4]) array([1, 2, 3, 4]) >>> unique_labels([1, 2, 10], [5, 11]) array([ 1,  2,  5, 10, 11]) '",
        "name": "out",
        "shape": "n_unique_labels",
        "type": "numpy"
      },
      "tags": [
        "utils",
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.calibration.calibration_curve",
      "description": "'Compute true and predicted probabilities for a calibration curve.\n\nRead more in the :ref:`User Guide <calibration>`.\n",
      "id": "sklearn.calibration.calibration_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.calibration.calibration_curve",
      "parameters": [
        {
          "description": "True targets. ",
          "name": "y_true",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Probabilities of the positive class. ",
          "name": "y_prob",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper probability. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto 1. ",
          "name": "normalize",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Number of bins. A bigger number requires more data. ",
          "name": "n_bins",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The true probability in each bin (fraction of positives).  prob_pred : array, shape (n_bins,) The mean predicted probability in each bin.  References ---------- Alexandru Niculescu-Mizil and Rich Caruana (2005) Predicting Good Probabilities With Supervised Learning, in Proceedings of the 22nd International Conference on Machine Learning (ICML). See section 4 (Qualitative Analysis of Predictions). '",
        "name": "prob_true",
        "shape": "n_bins,",
        "type": "array"
      },
      "tags": [
        "calibration"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.shrunk_covariance_.ledoit_wolf_shrinkage",
      "description": "'Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n",
      "id": "sklearn.covariance.shrunk_covariance_.ledoit_wolf_shrinkage",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.shrunk_covariance_.ledoit_wolf_shrinkage",
      "parameters": [
        {
          "description": "Data from which to compute the Ledoit-Wolf shrunk covariance shrinkage. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": ""
        },
        {
          "description": "Size of the blocks into which the covariance matrix will be split. ",
          "name": "block_size",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Coefficient in the convex combination used for the computation of the shrunk estimate.  Notes ----- The regularized (shrunk) covariance is:  (1 - shrinkage)*cov + shrinkage * mu * np.identity(n_features)  where mu = trace(cov) / n_features  '",
        "name": "shrinkage: float"
      },
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.shrunk_covariance_.oas",
      "description": "\"Estimate covariance with the Oracle Approximating Shrinkage algorithm.\n",
      "id": "sklearn.covariance.shrunk_covariance_.oas",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.shrunk_covariance_.oas",
      "parameters": [
        {
          "description": "Data from which to compute the covariance estimate. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Shrunk covariance.  shrinkage : float Coefficient in the convex combination used for the computation of the shrunk estimate.  Notes ----- The regularised (shrunk) covariance is:  (1 - shrinkage)*cov + shrinkage * mu * np.identity(n_features)  where mu = trace(cov) / n_features  The formula we used to implement the OAS does not correspond to the one given in the article. It has been taken from the MATLAB program available from the author's webpage (http://tbayes.eecs.umich.edu/yilun/covestimation).  \"",
        "name": "shrunk_cov",
        "shape": "n_features, n_features",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.effective_n_jobs",
      "description": "'Determine the number of jobs that can actually run in parallel\n\nn_jobs is the is the number of workers requested by the callers.\nPassing n_jobs=-1 means requesting all available workers for instance\nmatching the number of CPU cores on the worker host(s).\n\nThis method should return a guesstimate of the number of workers that can\nactually perform work concurrently with the currently enabled default\nbackend. The primary use case is to make it possible for the caller to know\nin how many chunks to slice the work.\n\nIn general working on larger data chunks is more efficient (less\nscheduling overhead and better use of CPU cache prefetching heuristics)\nas long as all the workers have enough work to do.\n\nWarning: this function is experimental and subject to change in a future\nversion of joblib.\n\n.. versionadded:: 0.10\n\n'",
      "id": "sklearn.externals.joblib.parallel.effective_n_jobs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.effective_n_jobs",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_hastie_10_2",
      "description": "'Generates data for binary classification used in\nHastie et al. 2009, Example 10.2.\n\nThe ten features are standard independent Gaussian and\nthe target ``y`` is defined by::\n\ny[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_hastie_10_2",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_hastie_10_2",
      "parameters": [
        {
          "default": "12000",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] The output values.  References ---------- .. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical Learning Ed. 2\", Springer, 2009.  See also -------- make_gaussian_quantiles: a generalization of this dataset approach '",
        "name": "X",
        "shape": "n_samples, 10",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.locally_linear.null_space",
      "description": "\"\nFind the null space of a matrix M.\n",
      "id": "sklearn.manifold.locally_linear.null_space",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.locally_linear.null_space",
      "parameters": [
        {
          "description": "Input covariance matrix: should be symmetric positive semi-definite ",
          "name": "M",
          "type": "array, matrix, sparse matrix, LinearOperator"
        },
        {
          "description": "Number of eigenvalues/vectors to return ",
          "name": "k",
          "type": "integer"
        },
        {
          "description": "Number of low eigenvalues to skip. ",
          "name": "k_skip",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "",
          "name": "eigen_solver",
          "type": "string"
        },
        {
          "description": "",
          "name": "auto",
          "type": "algorithm"
        },
        {
          "description": "For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results.",
          "name": "arpack",
          "type": "use"
        },
        {
          "description": "decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems. ",
          "name": "dense",
          "type": "use"
        },
        {
          "description": "Tolerance for 'arpack' method. Not used if eigen_solver=='dense'. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "not used if eigen_solver=='dense' ",
          "name": "max_iter",
          "type": "maximum"
        },
        {
          "description": "The generator or seed used to determine the starting vector for arpack iterations.  Defaults to numpy.random.  \"",
          "name": "random_state",
          "optional": "true",
          "type": "numpy"
        }
      ],
      "tags": [
        "manifold",
        "locally_linear"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.sag.get_auto_step_size",
      "description": "'Compute automatic step size for SAG solver\n\nThe step size is set to 1 / (alpha_scaled + L + fit_intercept) where L is\nthe max sum of squares for over all samples.\n",
      "id": "sklearn.linear_model.sag.get_auto_step_size",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.sag.get_auto_step_size",
      "parameters": [
        {
          "description": "Maximum squared sum of X over samples. ",
          "name": "max_squared_sum",
          "type": "float"
        },
        {
          "description": "Constant that multiplies the regularization term, scaled by 1. / n_samples, the number of samples. ",
          "name": "alpha_scaled",
          "type": "float"
        },
        {
          "description": "The loss function used in SAG solver. ",
          "name": "loss",
          "type": "string"
        },
        {
          "description": "Specifies if a constant (a.k.a. bias or intercept) will be added to the decision function. ",
          "name": "fit_intercept",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Step size used in SAG solver.  References ---------- Schmidt, M., Roux, N. L., & Bach, F. (2013). Minimizing finite sums with the stochastic average gradient https://hal.inria.fr/hal-00860051/document '",
        "name": "step_size",
        "type": "float"
      },
      "tags": [
        "linear_model",
        "sag"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_iris",
      "description": "\"Load and return the iris dataset (classification).\n\nThe iris dataset is a classic and very easy multi-class classification\ndataset.\n\n=================   ==============\nClasses                          3\nSamples per class               50\nSamples total                  150\nDimensionality                   4\nFeatures            real, positive\n=================   ==============\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.base.load_iris",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_iris",
      "parameters": [
        {
          "description": "If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "return_X_y",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn, 'target', the classification labels, 'target_names', the meaning of the labels, 'feature_names', the meaning of the features, and 'DESCR', the full description of the dataset.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18  Examples -------- Let's say you are interested in the samples 10, 25, and 50, and want to know their class name.  >>> from sklearn.datasets import load_iris >>> data = load_iris() >>> data.target[[10, 25, 50]] array([0, 0, 1]) >>> list(data.target_names) ['setosa', 'versicolor', 'virginica'] \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.pinvh",
      "description": "\"Compute the (Moore-Penrose) pseudo-inverse of a hermetian matrix.\n\nCalculate a generalized inverse of a symmetric matrix using its\neigenvalue decomposition and including all 'large' eigenvalues.\n",
      "id": "sklearn.utils.extmath.pinvh",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.pinvh",
      "parameters": [
        {
          "description": "Real symmetric or complex hermetian matrix to be pseudo-inverted ",
          "name": "a",
          "shape": "N, N",
          "type": "array"
        },
        {
          "description": "Cutoff for 'small' eigenvalues. Singular values smaller than rcond * largest_eigenvalue are considered zero.  If None or -1, suitable machine precision is used. ",
          "name": "cond",
          "type": "float"
        },
        {
          "description": "Cutoff for 'small' eigenvalues. Singular values smaller than rcond * largest_eigenvalue are considered zero.  If None or -1, suitable machine precision is used. ",
          "name": "rcond",
          "type": "float"
        },
        {
          "description": "Whether the pertinent array data is taken from the lower or upper triangle of a. (Default: lower) ",
          "name": "lower",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": " Raises ------ LinAlgError If eigenvalue does not converge  Examples -------- >>> import numpy as np >>> a = np.random.randn(9, 6) >>> a = np.dot(a, a.T) >>> B = pinvh(a) >>> np.allclose(a, np.dot(a, np.dot(B, a))) True >>> np.allclose(B, np.dot(B, np.dot(a, B))) True  \"",
        "name": "B",
        "shape": "N, N",
        "type": "array"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.optimize.newton_cg",
      "description": "\"\nMinimization of scalar function of one or more variables using the\nNewton-CG algorithm.\n",
      "id": "sklearn.utils.optimize.newton_cg",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.optimize.newton_cg",
      "parameters": [
        {
          "description": "Should return the gradient and a callable returning the matvec product of the Hessian. ",
          "name": "grad_hess",
          "type": "callable"
        },
        {
          "description": "Should return the value of the function. ",
          "name": "func",
          "type": "callable"
        },
        {
          "description": "Should return the function value and the gradient. This is used by the linesearch functions.  x0 : array of float Initial guess.  args: tuple, optional Arguments passed to func_grad_hess, func and grad. ",
          "name": "grad",
          "type": "callable"
        },
        {
          "description": "Stopping criterion. The iteration will stop when ``max{|g_i | i = 1, ..., n} <= tol`` where ``g_i`` is the i-th component of the gradient. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Number of Newton iterations. ",
          "name": "maxiter",
          "type": "int"
        },
        {
          "description": "Number of CG iterations.  line_search: boolean Whether to use a line search or not.  warn: boolean Whether to warn when didn't converge. ",
          "name": "maxinner",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Estimated minimum. \"",
        "name": "xk",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "optimize"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_is_fitted",
      "description": "'Perform is_fitted validation for estimator.\n\nChecks if the estimator is fitted by verifying the presence of\n\"all_or_any\" of the passed attributes and raises a NotFittedError with the\ngiven message.\n",
      "id": "sklearn.utils.validation.check_is_fitted",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_is_fitted",
      "parameters": [
        {
          "description": "estimator instance for which the check is performed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Eg. : [\"coef_\", \"estimator_\", ...], \"coef_\" ",
          "name": "attributes",
          "type": "attribute"
        },
        {
          "description": "The default error message is, \"This %(name)s instance is not fitted yet. Call \\'fit\\' with appropriate arguments before using this method.\"  For custom messages if \"%(name)s\" is present in the message string, it is substituted for the estimator name.  Eg. : \"Estimator, %(name)s, must be fitted before sparsifying\". ",
          "name": "msg",
          "type": "string"
        },
        {
          "description": "Specify whether all or any of the given attributes must exist. '",
          "name": "all_or_any",
          "type": "callable"
        }
      ],
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_symmetric",
      "description": "'Make sure that array is 2D, square and symmetric.\n\nIf the array is not symmetric, then a symmetrized version is returned.\nOptionally, a warning or exception is raised if the matrix is not\nsymmetric.\n",
      "id": "sklearn.utils.validation.check_symmetric",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_symmetric",
      "parameters": [
        {
          "description": "Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised.",
          "name": "array",
          "type": "nd-array"
        },
        {
          "description": "Absolute tolerance for equivalence of arrays. Default = 1E-10.",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "If True then raise a warning if conversion is required.",
          "name": "raise_warning",
          "type": "boolean"
        },
        {
          "description": "If True then raise an exception if array is not symmetric. ",
          "name": "raise_exception",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Symmetrized version of the input array, i.e. the average of array and array.transpose(). If sparse, then duplicate entries are first summed and zeros are eliminated. '",
        "name": "array_sym",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Stores the embedding vectors. ",
          "name": "embedding_",
          "shape": "n_samples, n_components",
          "type": "array-like"
        },
        {
          "description": "`KernelPCA` object used to implement the embedding. ",
          "name": "kernel_pca_",
          "type": "object"
        },
        {
          "description": "Stores the training data. ",
          "name": "training_data_",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Stores nearest neighbors instance, including BallTree or KDtree if applicable. ",
          "name": "nbrs_",
          "type": "sklearn"
        },
        {
          "description": "Stores the geodesic distance matrix of training data. ",
          "name": "dist_matrix_",
          "shape": "n_samples, n_samples",
          "type": "array-like"
        }
      ],
      "category": "manifold.isomap",
      "common_name": "Isomap",
      "description": "\"Isomap Embedding\n\nNon-linear dimensionality reduction through Isometric Mapping\n\nRead more in the :ref:`User Guide <isomap>`.\n",
      "id": "sklearn.manifold.isomap.Isomap",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the embedding vectors for data X\n",
          "id": "sklearn.manifold.isomap.Isomap.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Sample data, shape = (n_samples, n_features), in the form of a numpy array, precomputed tree, or NearestNeighbors object. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree, NearestNeighbors"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit the model from data in X and transform X.\n",
          "id": "sklearn.manifold.isomap.Isomap.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.manifold.isomap.Isomap.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Compute the reconstruction error for the embedding.\n",
          "id": "sklearn.manifold.isomap.Isomap.reconstruction_error",
          "name": "reconstruction_error",
          "parameters": [],
          "returns": {
            "description": " Notes ------- The cost function of an isomap embedding is  ``E = frobenius_norm[K(D) - K(D_fit)] / n_samples``  Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel:  ``K(D) = -0.5 * (I - 1/n_samples) * D^2 * (I - 1/n_samples)`` '",
            "name": "reconstruction_error",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.manifold.isomap.Isomap.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform X.\n\nThis is implemented by linking the points X into the graph of geodesic\ndistances of the training data. First the `n_neighbors` nearest\nneighbors of X are found in the training data, and from these the\nshortest geodesic distances from each point in X to each point in\nthe training data are computed in order to construct the kernel.\nThe embedding of X is the projection of this kernel onto the\nembedding vectors of the training set.\n",
          "id": "sklearn.manifold.isomap.Isomap.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.manifold.isomap.Isomap",
      "parameters": [
        {
          "description": "number of neighbors to consider for each point. ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "number of coordinates for the manifold ",
          "name": "n_components",
          "type": "integer"
        },
        {
          "description": "'auto' : Attempt to choose the most efficient solver for the given problem.  'arpack' : Use Arnoldi decomposition to find the eigenvalues and eigenvectors.  'dense' : Use a direct solver (i.e. LAPACK) for the eigenvalue decomposition. ",
          "name": "eigen_solver",
          "type": ""
        },
        {
          "description": "Convergence tolerance passed to arpack or lobpcg. not used if eigen_solver == 'dense'. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations for the arpack solver. not used if eigen_solver == 'dense'. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "Method to use in finding shortest path.  'auto' : attempt to choose the best algorithm automatically.  'FW' : Floyd-Warshall algorithm.  'D' : Dijkstra's algorithm. ",
          "name": "path_method",
          "type": "string"
        },
        {
          "description": "Algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance. ",
          "name": "neighbors_algorithm",
          "type": "string"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/manifold/isomap.pyc:15",
      "tags": [
        "manifold",
        "isomap"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "lda",
      "common_name": "LDA",
      "description": "'\nAlias for\n:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`.\n\n.. deprecated:: 0.17\nThis class will be removed in 0.19.\nUse\n:class:`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`\ninstead.\n'",
      "id": "sklearn.lda.LDA",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.lda.LDA.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Fit LinearDiscriminantAnalysis model according to the given\ntraining data and parameters.\n\n.. versionchanged:: 0.17\nDeprecated *store_covariance* have been moved to main constructor.\n\n.. versionchanged:: 0.17\nDeprecated *tol* have been moved to main constructor.\n",
          "id": "sklearn.lda.LDA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.lda.LDA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.lda.LDA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.lda.LDA.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Estimate log probability.\n",
          "id": "sklearn.lda.LDA.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Estimated log probabilities. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Estimate probability.\n",
          "id": "sklearn.lda.LDA.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Estimated probabilities. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.lda.LDA.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.lda.LDA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Project data to maximize class separation.\n",
          "id": "sklearn.lda.LDA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.lda.LDA",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/lda.pyc:9",
      "tags": [
        "lda"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "gaussian_process.kernels",
      "common_name": "Matern",
      "description": "' Matern kernel.\n\nThe class of Matern kernels is a generalization of the RBF and the\nabsolute exponential kernel parameterized by an additional parameter\nnu. The smaller nu, the less smooth the approximated function is.\nFor nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5\nto the absolute exponential kernel. Important intermediate values are\nnu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable\nfunctions).\n\nSee Rasmussen and Williams 2006, pp84 for details regarding the\ndifferent variants of the Matern kernel.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.kernels.Matern",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns a clone of self with given hyperparameters theta. '",
          "id": "sklearn.gaussian_process.kernels.Matern.clone_with_theta",
          "name": "clone_with_theta",
          "parameters": []
        },
        {
          "description": "'Returns the diagonal of the kernel k(X, X).\n\nThe result of this method is identical to np.diag(self(X)); however,\nit can be evaluated more efficiently since only the diagonal is\nevaluated.\n",
          "id": "sklearn.gaussian_process.kernels.Matern.diag",
          "name": "diag",
          "parameters": [
            {
              "description": "Left argument of the returned kernel k(X, Y) ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Diagonal of kernel k(X, X) '",
            "name": "K_diag",
            "shape": "n_samples_X,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters of this kernel.\n",
          "id": "sklearn.gaussian_process.kernels.Matern.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns whether the kernel is stationary. '",
          "id": "sklearn.gaussian_process.kernels.Matern.is_stationary",
          "name": "is_stationary",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this kernel.\n\nThe method works on simple kernels as well as on nested kernels.\nThe latter have parameters of the form ``<component>__<parameter>``\nso that it's possible to update each component of a nested object.\n",
          "id": "sklearn.gaussian_process.kernels.Matern.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.kernels.Matern",
      "parameters": [
        {
          "description": "The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension. ",
          "name": "length_scale",
          "shape": "n_features,",
          "type": "float"
        },
        {
          "description": "The lower and upper bound on length_scale  nu: float, default: 1.5 The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized.  '",
          "name": "length_scale_bounds",
          "type": "pair"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:1245",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The imputation fill value for each feature if axis == 0. ",
          "name": "statistics_",
          "shape": "n_features,",
          "type": "array"
        }
      ],
      "category": "preprocessing.imputation",
      "common_name": "Imputer",
      "description": "'Imputation transformer for completing missing values.\n\nRead more in the :ref:`User Guide <imputation>`.\n",
      "id": "sklearn.preprocessing.imputation.Imputer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the imputer on X.\n",
          "id": "sklearn.preprocessing.imputation.Imputer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Input data, where ``n_samples`` is the number of samples and ``n_features`` is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.imputation.Imputer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.imputation.Imputer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.imputation.Imputer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Impute all missing values in X.\n",
          "id": "sklearn.preprocessing.imputation.Imputer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input data to complete. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.imputation.Imputer",
      "parameters": [
        {
          "default": "\"NaN\"",
          "description": "The placeholder for the missing values. All occurrences of `missing_values` will be imputed. For missing values encoded as np.nan, use the string value \"NaN\". ",
          "name": "missing_values",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"mean\"",
          "description": "The imputation strategy.  - If \"mean\", then replace missing values using the mean along the axis. - If \"median\", then replace missing values using the median along the axis. - If \"most_frequent\", then replace missing using the most frequent value along the axis. ",
          "name": "strategy",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "0",
          "description": "The axis along which to impute.  - If `axis=0`, then impute along columns. - If `axis=1`, then impute along rows. ",
          "name": "axis",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the imputer. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "True",
          "description": "If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if `copy=False`:  - If X is not an array of floating values; - If X is sparse and `missing_values=0`; - If `axis=0` and X is encoded as a CSR matrix; - If `axis=1` and X is encoded as a CSC matrix. ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/imputation.pyc:64",
      "tags": [
        "preprocessing",
        "imputation"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "pipeline",
      "common_name": "Feature Union",
      "description": "\"Concatenates results of multiple transformer objects.\n\nThis estimator applies a list of transformer objects in parallel to the\ninput data, then concatenates the results. This is useful to combine\nseveral feature extraction mechanisms into a single transformer.\n\nParameters of the transformers may be set using its name and the parameter\nname separated by a '__'. A transformer may be replaced entirely by\nsetting the parameter with its name to another transformer,\nor removed by setting to ``None``.\n\nRead more in the :ref:`User Guide <feature_union>`.\n",
      "id": "sklearn.pipeline.FeatureUnion",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit all transformers using X.\n",
          "id": "sklearn.pipeline.FeatureUnion.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Input data, used to fit transformers. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "Targets for supervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples, ...",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "This estimator '",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Fit all transformers, transform the data and concatenate results.\n",
          "id": "sklearn.pipeline.FeatureUnion.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Input data to be transformed. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "Targets for supervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples, ...",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. '",
            "name": "X_t",
            "shape": "n_samples, sum_n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Get feature names from all transformers.\n",
          "id": "sklearn.pipeline.FeatureUnion.get_feature_names",
          "name": "get_feature_names",
          "parameters": [],
          "returns": {
            "description": "Names of the features produced by transform. '",
            "name": "feature_names",
            "type": "list"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.pipeline.FeatureUnion.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Set the parameters of this estimator.\n\nValid parameter keys can be listed with ``get_params()``.\n",
          "id": "sklearn.pipeline.FeatureUnion.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Transform X separately by each transformer, concatenate results.\n",
          "id": "sklearn.pipeline.FeatureUnion.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data to be transformed. ",
              "name": "X",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers. '",
            "name": "X_t",
            "shape": "n_samples, sum_n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.pipeline.FeatureUnion",
      "parameters": [
        {
          "description": "List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer.  n_jobs: int, optional Number of jobs to run in parallel (default 1).  transformer_weights: dict, optional Multiplicative weights for features per transformer. Keys are transformer names, values the weights.  \"",
          "name": "transformer_list",
          "type": "list"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/pipeline.pyc:586",
      "tags": [
        "pipeline"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Input array. ",
          "name": "X_",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "The distinct labels used in classifying instances. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Categorical distribution for each item. ",
          "name": "label_distributions_",
          "shape": "n_samples, n_classes",
          "type": "array"
        },
        {
          "description": "Label assigned to each item via the transduction. ",
          "name": "transduction_",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Number of iterations run. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "semi_supervised.label_propagation",
      "common_name": "Label Propagation",
      "description": "\"Label Propagation classifier\n\nRead more in the :ref:`User Guide <label_propagation>`.\n",
      "id": "sklearn.semi_supervised.label_propagation.LabelPropagation",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit a semi-supervised label propagation model based\n\nAll the input data is provided matrix X (labeled and unlabeled)\nand corresponding label matrix y with a dedicated marker value for\nunlabeled samples.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "A {n_samples by n_samples} size matrix will be created from this ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be transductively assigned labels ",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Performs inductive inference across the model.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Predictions for input data '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict probability for each possible outcome.\n\nCompute the probability estimates for each single sample in X\nand each possible outcome seen during training (categorical\ndistribution).\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Normalized probability distributions across class labels '",
            "name": "probabilities",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelPropagation.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.semi_supervised.label_propagation.LabelPropagation",
      "parameters": [
        {
          "description": "String identifier for kernel function to use. Only 'rbf' and 'knn' kernels are currently supported.. ",
          "name": "kernel",
          "type": "'knn', 'rbf'"
        },
        {
          "description": "Parameter for rbf kernel ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Parameter for knn kernel ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "Clamping factor ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Change maximum number of iterations allowed ",
          "name": "max_iter",
          "type": "float"
        },
        {
          "description": "Convergence tolerance: threshold to consider the system at steady state ",
          "name": "tol",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/semi_supervised/label_propagation.pyc:276",
      "tags": [
        "semi_supervised",
        "label_propagation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "A copy of the `classes` parameter where provided, or otherwise, the sorted set of classes found when fitting. ",
          "name": "classes_",
          "type": "array"
        }
      ],
      "category": "preprocessing.label",
      "common_name": "Multi Label Binarizer",
      "description": "\"Transform between iterable of iterables and a multilabel format\n\nAlthough a list of sets or tuples is a very intuitive format for multilabel\ndata, it is unwieldy to process. This transformer converts between this\nintuitive format and the supported multilabel format: a (samples x classes)\nbinary matrix indicating the presence of a class label.\n",
      "id": "sklearn.preprocessing.label.MultiLabelBinarizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the label sets binarizer, storing `classes_`\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "A set of labels (any orderable and hashable object) for each sample. If the `classes` parameter is set, `y` will not be iterated. ",
              "name": "y",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit the label sets binarizer and transform the given label sets\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "A set of labels (any orderable and hashable object) for each sample. If the `classes` parameter is set, `y` will not be iterated. ",
              "name": "y",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in `y[i]`, and 0 otherwise. '",
            "name": "y_indicator",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform the given indicator matrix into label sets\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "A matrix containing only 1s ands 0s. ",
              "name": "yt",
              "shape": "n_samples, n_classes",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The set of labels for each sample such that `y[i]` consists of `classes_[j]` for each `yt[i, j] == 1`. '",
            "name": "y",
            "type": "list"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform the given label sets\n",
          "id": "sklearn.preprocessing.label.MultiLabelBinarizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "A set of labels (any orderable and hashable object) for each sample. If the `classes` parameter is set, `y` will not be iterated. ",
              "name": "y",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "A matrix such that `y_indicator[i, j] = 1` iff `classes_[j]` is in `y[i]`, and 0 otherwise. '",
            "name": "y_indicator",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.preprocessing.label.MultiLabelBinarizer",
      "parameters": [
        {
          "description": "Indicates an ordering for the class labels ",
          "name": "classes",
          "optional": "true",
          "shape": "n_classes",
          "type": "array-like"
        },
        {
          "description": "Set to true if output binary array is desired in CSR sparse format ",
          "name": "sparse_output",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.pyc:633",
      "tags": [
        "preprocessing",
        "label"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "Subset of training points used to construct the feature map. ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "Indices of ``components_`` in the training set. ",
          "name": "component_indices_",
          "shape": "n_components",
          "type": "array"
        },
        {
          "description": "Normalization matrix needed for embedding. Square root of the kernel matrix on ``components_``.  ",
          "name": "normalization_",
          "shape": "n_components, n_components",
          "type": "array"
        }
      ],
      "category": "kernel_approximation",
      "common_name": "Nystroem",
      "description": "'Approximate a kernel map using a subset of the training data.\n\nConstructs an approximate feature map for an arbitrary kernel\nusing a subset of the data as basis.\n\nRead more in the :ref:`User Guide <nystroem_kernel_approx>`.\n",
      "id": "sklearn.kernel_approximation.Nystroem",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit estimator to data.\n\nSamples a subset of training points, computes kernel\non these and computes normalization matrix.\n",
          "id": "sklearn.kernel_approximation.Nystroem.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. '",
              "name": "X",
              "shape": "n_samples, n_feature",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.kernel_approximation.Nystroem.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.kernel_approximation.Nystroem.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.kernel_approximation.Nystroem.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply feature map to X.\n\nComputes an approximate feature map using the kernel\nbetween some training points and X.\n",
          "id": "sklearn.kernel_approximation.Nystroem.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Data to transform. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_transformed",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.kernel_approximation.Nystroem",
      "parameters": [
        {
          "description": "Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. ",
          "name": "kernel",
          "type": "string"
        },
        {
          "description": "Number of features to construct. How many data points will be used to construct the mapping. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Gamma parameter for the RBF, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels. ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. ",
          "name": "degree",
          "type": "float"
        },
        {
          "description": "Additional parameters (keyword arguments) for kernel function passed as callable object. ",
          "name": "kernel_params",
          "optional": "true",
          "type": "mapping"
        },
        {
          "description": "If int, random_state is the seed used by the random number generator; if RandomState instance, random_state is the random number generator.  ",
          "name": "random_state",
          "optional": "true",
          "type": "int, RandomState"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/kernel_approximation.pyc:361",
      "tags": [
        "kernel_approximation"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Per feature relative scaling of the data.  .. versionadded:: 0.17 *scale_* attribute. ",
          "name": "scale_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "Per feature maximum absolute value. ",
          "name": "max_abs_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.  See also -------- maxabs_scale: Equivalent function without the object oriented API.",
          "name": "n_samples_seen_",
          "type": "int"
        }
      ],
      "category": "preprocessing.data",
      "common_name": "Max Abs Scaler",
      "description": "'Scale each feature by its maximum absolute value.\n\nThis estimator scales and translates each feature individually such\nthat the maximal absolute value of each feature in the\ntraining set will be 1.0. It does not shift/center the data, and\nthus does not destroy any sparsity.\n\nThis scaler can also be applied to sparse CSR or CSC matrices.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.preprocessing.data.MaxAbsScaler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the maximum absolute value to be used for later scaling.\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Scale back the data to the original representation\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The data that should be transformed back. '",
              "name": "X",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Online computation of max absolute value of X for later scaling.\nAll of X is processed as a single batch. This is intended for cases\nwhen `fit` is not feasible due to very large number of `n_samples`\nor because X is read from a continuous stream.\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "The data used to compute the mean and standard deviation used for later scaling along the features axis.  y: Passthrough for ``Pipeline`` compatibility. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Scale the data\n",
          "id": "sklearn.preprocessing.data.MaxAbsScaler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data that should be scaled. '",
              "name": "X",
              "type": "array-like, sparse matrix"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.MaxAbsScaler",
      "parameters": [
        {
          "description": "Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:699",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Stores the position of the dataset in the embedding space ",
          "name": "embedding_",
          "shape": "n_components, n_samples",
          "type": "array-like"
        },
        {
          "description": "The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points)  ",
          "name": "stress_",
          "type": "float"
        }
      ],
      "category": "manifold.mds",
      "common_name": "MDS",
      "description": "'Multidimensional scaling\n\nRead more in the :ref:`User Guide <multidimensional_scaling>`.\n",
      "id": "sklearn.manifold.mds.MDS",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nComputes the position of the points in the embedding space\n",
          "id": "sklearn.manifold.mds.MDS.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "If None, randomly chooses the initial configuration if ndarray, initialize the SMACOF algorithm with this array. \"",
              "name": "init",
              "optional": "true",
              "shape": "n_samples,",
              "type": "None or ndarray, shape (n_samples,)"
            }
          ]
        },
        {
          "description": "\"\nFit the data from X, and returns the embedded coordinates\n",
          "id": "sklearn.manifold.mds.MDS.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "If None, randomly chooses the initial configuration if ndarray, initialize the SMACOF algorithm with this array.  \"",
              "name": "init",
              "optional": "true",
              "shape": "n_samples,",
              "type": "None or ndarray, shape (n_samples,)"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.manifold.mds.MDS.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.manifold.mds.MDS.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.manifold.mds.MDS",
      "parameters": [
        {
          "description": "compute metric or nonmetric SMACOF (Scaling by Majorizing a Complicated Function) algorithm ",
          "name": "metric",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "number of dimension in which to immerse the similarities overridden if initial array is provided. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of time the smacof algorithm will be run with different initialisation. The final results will be the best output of the n_init consecutive runs in terms of stress. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations of the SMACOF algorithm for a single run ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "level of verbosity ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "relative tolerance w.r.t stress to declare converge ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Which dissimilarity measure to use. Supported are \\'euclidean\\' and \\'precomputed\\'.  ",
          "name": "dissimilarity",
          "type": "string"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/manifold/mds.pyc:274",
      "tags": [
        "manifold",
        "mds"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.arpack",
      "common_name": "Iter Op Inv",
      "description": "'\nIterOpInv:\nhelper class to repeatedly solve [A-sigma*M]*x = b\nusing an iterative method\n'",
      "id": "sklearn.utils.arpack.IterOpInv",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Hermitian adjoint.\n\nReturns the Hermitian adjoint of self, aka the Hermitian\nconjugate or Hermitian transpose. For a complex matrix, the\nHermitian adjoint is equal to the conjugate transpose.\n\nCan be abbreviated self.H instead of self.adjoint().\n",
          "id": "sklearn.utils.arpack.IterOpInv.adjoint",
          "name": "adjoint",
          "parameters": [],
          "returns": {
            "description": "Hermitian adjoint of self. '",
            "name": "A_H",
            "type": ""
          }
        },
        {
          "description": "'Matrix-matrix or matrix-vector multiplication.\n",
          "id": "sklearn.utils.arpack.IterOpInv.dot",
          "name": "dot",
          "parameters": [
            {
              "description": "1-d or 2-d array, representing a vector or matrix. ",
              "name": "x",
              "type": "array"
            }
          ],
          "returns": {
            "description": "1-d or 2-d array (depending on the shape of x) that represents the result of applying this linear operator on x.  '",
            "name": "Ax",
            "type": "array"
          }
        },
        {
          "description": "'Matrix-matrix multiplication.\n\nPerforms the operation y=A*X where A is an MxN linear\noperator and X dense N*K matrix or ndarray.\n",
          "id": "sklearn.utils.arpack.IterOpInv.matmat",
          "name": "matmat",
          "parameters": [
            {
              "description": "An array with shape (N,K). ",
              "name": "X",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,K) depending on the type of the X argument.  Notes ----- This matmat wraps any user-specified matmat routine or overridden _matmat method to ensure that y has the correct type.  '",
            "name": "Y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Matrix-vector multiplication.\n\nPerforms the operation y=A*x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.IterOpInv.matvec",
          "name": "matvec",
          "parameters": [
            {
              "description": "An array with shape (N,) or (N,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,) or (M,1) depending on the type and shape of the x argument.  Notes ----- This matvec wraps the user-specified matvec routine or overridden _matvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Adjoint matrix-vector multiplication.\n\nPerforms the operation y = A^H * x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.IterOpInv.rmatvec",
          "name": "rmatvec",
          "parameters": [
            {
              "description": "An array with shape (M,) or (M,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (N,) or (N,1) depending on the type and shape of the x argument.  Notes ----- This rmatvec wraps the user-specified rmatvec routine or overridden _rmatvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Transpose this linear operator.\n\nReturns a LinearOperator that represents the transpose of this one.\nCan be abbreviated self.T instead of self.transpose().\n'",
          "id": "sklearn.utils.arpack.IterOpInv.transpose",
          "name": "transpose",
          "parameters": []
        }
      ],
      "name": "sklearn.utils.arpack.IterOpInv",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:1023",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.arpack",
      "common_name": "Sp Lu Inv",
      "description": "'\nSpLuInv:\nhelper class to repeatedly solve M*x=b\nusing a sparse LU-decopposition of M\n'",
      "id": "sklearn.utils.arpack.SpLuInv",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Hermitian adjoint.\n\nReturns the Hermitian adjoint of self, aka the Hermitian\nconjugate or Hermitian transpose. For a complex matrix, the\nHermitian adjoint is equal to the conjugate transpose.\n\nCan be abbreviated self.H instead of self.adjoint().\n",
          "id": "sklearn.utils.arpack.SpLuInv.adjoint",
          "name": "adjoint",
          "parameters": [],
          "returns": {
            "description": "Hermitian adjoint of self. '",
            "name": "A_H",
            "type": ""
          }
        },
        {
          "description": "'Matrix-matrix or matrix-vector multiplication.\n",
          "id": "sklearn.utils.arpack.SpLuInv.dot",
          "name": "dot",
          "parameters": [
            {
              "description": "1-d or 2-d array, representing a vector or matrix. ",
              "name": "x",
              "type": "array"
            }
          ],
          "returns": {
            "description": "1-d or 2-d array (depending on the shape of x) that represents the result of applying this linear operator on x.  '",
            "name": "Ax",
            "type": "array"
          }
        },
        {
          "description": "'Matrix-matrix multiplication.\n\nPerforms the operation y=A*X where A is an MxN linear\noperator and X dense N*K matrix or ndarray.\n",
          "id": "sklearn.utils.arpack.SpLuInv.matmat",
          "name": "matmat",
          "parameters": [
            {
              "description": "An array with shape (N,K). ",
              "name": "X",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,K) depending on the type of the X argument.  Notes ----- This matmat wraps any user-specified matmat routine or overridden _matmat method to ensure that y has the correct type.  '",
            "name": "Y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Matrix-vector multiplication.\n\nPerforms the operation y=A*x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.SpLuInv.matvec",
          "name": "matvec",
          "parameters": [
            {
              "description": "An array with shape (N,) or (N,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (M,) or (M,1) depending on the type and shape of the x argument.  Notes ----- This matvec wraps the user-specified matvec routine or overridden _matvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Adjoint matrix-vector multiplication.\n\nPerforms the operation y = A^H * x where A is an MxN linear\noperator and x is a column vector or 1-d array.\n",
          "id": "sklearn.utils.arpack.SpLuInv.rmatvec",
          "name": "rmatvec",
          "parameters": [
            {
              "description": "An array with shape (M,) or (M,1). ",
              "name": "x",
              "type": "matrix, ndarray"
            }
          ],
          "returns": {
            "description": "A matrix or ndarray with shape (N,) or (N,1) depending on the type and shape of the x argument.  Notes ----- This rmatvec wraps the user-specified rmatvec routine or overridden _rmatvec method to ensure that y has the correct shape and type.  '",
            "name": "y",
            "type": "matrix, ndarray"
          }
        },
        {
          "description": "'Transpose this linear operator.\n\nReturns a LinearOperator that represents the transpose of this one.\nCan be abbreviated self.T instead of self.transpose().\n'",
          "id": "sklearn.utils.arpack.SpLuInv.transpose",
          "name": "transpose",
          "parameters": []
        }
      ],
      "name": "sklearn.utils.arpack.SpLuInv",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/arpack.pyc:955",
      "tags": [
        "utils",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "qda",
      "common_name": "QDA",
      "description": "'\nAlias for\n:class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`.\n\n.. deprecated:: 0.17\nThis class will be removed in 0.19.\nUse\n:class:`sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`\ninstead.\n'",
      "id": "sklearn.qda.QDA",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply decision function to an array of samples.\n",
          "id": "sklearn.qda.QDA.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Array of samples (test vectors). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Decision function values related to each class, per sample. In the two-class case, the shape is [n_samples,], giving the log likelihood ratio of the positive class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model according to the given training data and parameters.\n\n.. versionchanged:: 0.17\nDeprecated *store_covariance* have been moved to main constructor.\n\n.. versionchanged:: 0.17\nDeprecated *tol* have been moved to main constructor.\n",
          "id": "sklearn.qda.QDA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers) '",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.qda.QDA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on an array of test vectors X.\n\nThe predicted class C for each sample in X is returned.\n",
          "id": "sklearn.qda.QDA.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Return posterior probabilities of classification.\n",
          "id": "sklearn.qda.QDA.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "Array of samples/test vectors. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Posterior log-probabilities of classification per class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Return posterior probabilities of classification.\n",
          "id": "sklearn.qda.QDA.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Array of samples/test vectors. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Posterior probabilities of classification per class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.qda.QDA.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.qda.QDA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.qda.QDA",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/qda.pyc:9",
      "tags": [
        "qda"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "semi_supervised.label_propagation",
      "common_name": "Base Label Propagation",
      "description": "\"Base class for label propagation module.\n",
      "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit a semi-supervised label propagation model based\n\nAll the input data is provided matrix X (labeled and unlabeled)\nand corresponding label matrix y with a dedicated marker value for\nunlabeled samples.\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "A {n_samples by n_samples} size matrix will be created from this ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be transductively assigned labels ",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Performs inductive inference across the model.\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Predictions for input data '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict probability for each possible outcome.\n\nCompute the probability estimates for each single sample in X\nand each possible outcome seen during training (categorical\ndistribution).\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Normalized probability distributions across class labels '",
            "name": "probabilities",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.semi_supervised.label_propagation.BaseLabelPropagation",
      "parameters": [
        {
          "description": "String identifier for kernel function to use. Only 'rbf' and 'knn' kernels are currently supported.. ",
          "name": "kernel",
          "type": "'knn', 'rbf'"
        },
        {
          "description": "Parameter for rbf kernel ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Clamping factor ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Change maximum number of iterations allowed ",
          "name": "max_iter",
          "type": "float"
        },
        {
          "description": "Convergence tolerance: threshold to consider the system at steady state ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Parameter for knn kernel ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores.  \"",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/semi_supervised/label_propagation.pyc:78",
      "tags": [
        "semi_supervised",
        "label_propagation"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "svm.base",
      "common_name": "Base SVC",
      "description": "'ABC for LibSVM-based classifiers.'",
      "id": "sklearn.svm.base.BaseSVC",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Distance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.base.BaseSVC.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. If decision_function_shape='ovr', the shape is (n_samples, n_classes) \"",
            "name": "X",
            "shape": "n_samples, n_classes * (n_classes-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.base.BaseSVC.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.base.BaseSVC.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.base.BaseSVC.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train] ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Class labels for samples in X. '",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.svm.base.BaseSVC.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.base.BaseSVC.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.base.BaseSVC",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/base.pyc:504",
      "tags": [
        "svm",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Leave One Group Out",
      "description": "'Leave One Group Out cross-validator\n\nProvides train/test indices to split data according to a third-party\nprovided group. This group information can be used to encode arbitrary\ndomain specific stratifications of the samples as integers.\n\nFor instance the groups could be the year of collection of the samples\nand thus allow for cross-validation against time-based splits.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n\nExamples\n--------\n>>> from sklearn.model_selection import LeaveOneGroupOut\n>>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n>>> y = np.array([1, 2, 1, 2])\n>>> groups = np.array([1, 1, 2, 2])\n>>> logo = LeaveOneGroupOut()\n>>> logo.get_n_splits(X, y, groups)\n2\n>>> print(logo)\nLeaveOneGroupOut()\n>>> for train_index, test_index in logo.split(X, y, groups):\n...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n...    X_train, X_test = X[train_index], X[test_index]\n...    y_train, y_test = y[train_index], y[test_index]\n...    print(X_train, X_test, y_train, y_test)\nTRAIN: [2 3] TEST: [0 1]\n[[5 6]\n[7 8]] [[1 2]\n[3 4]] [1 2] [1 2]\nTRAIN: [0 1] TEST: [2 3]\n[[1 2]\n[3 4]] [[5 6]\n[7 8]] [1 2] [1 2]\n\n'",
      "id": "sklearn.model_selection._split.LeaveOneGroupOut",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.LeaveOneGroupOut.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.LeaveOneGroupOut.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. ",
              "name": "y",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.LeaveOneGroupOut",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:737",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "model_selection._split",
      "common_name": "Stratified Shuffle Split",
      "description": "'Stratified ShuffleSplit cross-validator\n\nProvides train/test indices to split data in train/test sets.\n\nThis cross-validation object is a merge of StratifiedKFold and\nShuffleSplit, which returns stratified randomized folds. The folds\nare made by preserving the percentage of samples for each class.\n\nNote: like the ShuffleSplit strategy, stratified random splits\ndo not guarantee that all folds will be different, although this is\nstill very likely for sizeable datasets.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.StratifiedShuffleSplit",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the number of splitting iterations in the cross-validator\n",
          "id": "sklearn.model_selection._split.StratifiedShuffleSplit.get_n_splits",
          "name": "get_n_splits",
          "parameters": [
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "X",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "y",
              "type": "object"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "Returns the number of splitting iterations in the cross-validator. '",
            "name": "n_splits",
            "type": "int"
          }
        },
        {
          "description": "'Generate indices to split data into training and test set.\n",
          "id": "sklearn.model_selection._split.StratifiedShuffleSplit.split",
          "name": "split",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing ``y`` is sufficient to generate the splits and hence ``np.zeros(n_samples)`` may be used as a placeholder for ``X`` instead of actual training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target variable for supervised learning problems. Stratification is done based on the y labels. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Always ignored, exists for compatibility. ",
              "name": "groups",
              "type": "object"
            }
          ],
          "returns": {
            "description": "The training set indices for that split.  test : ndarray The testing set indices for that split. '",
            "name": "train",
            "type": "ndarray"
          }
        }
      ],
      "name": "sklearn.model_selection._split.StratifiedShuffleSplit",
      "parameters": [
        {
          "description": "Number of re-shuffling & splitting iterations. ",
          "name": "n_splits",
          "type": "int"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.pyc:1190",
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Indices of cluster centers ",
          "name": "cluster_centers_indices_",
          "shape": "n_clusters,",
          "type": "array"
        },
        {
          "description": "Cluster centers (if affinity != ``precomputed``). ",
          "name": "cluster_centers_",
          "shape": "n_clusters, n_features",
          "type": "array"
        },
        {
          "description": "Labels of each point ",
          "name": "labels_",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Stores the affinity matrix used in ``fit``. ",
          "name": "affinity_matrix_",
          "shape": "n_samples, n_samples",
          "type": "array"
        },
        {
          "description": "Number of iterations taken to converge. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "cluster.affinity_propagation_",
      "common_name": "Affinity Propagation",
      "description": "'Perform Affinity Propagation Clustering of data.\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n",
      "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Create affinity matrix from negative euclidean distances, then\napply affinity propagation clustering.\n",
          "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the closest cluster each sample in X belongs to.\n",
          "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "New data to predict. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Index of the cluster each sample belongs to. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.affinity_propagation_.AffinityPropagation.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.affinity_propagation_.AffinityPropagation",
      "parameters": [
        {
          "description": "Damping factor between 0.5 and 1. ",
          "name": "damping",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of iterations with no change in the number of estimated clusters that stops the convergence. ",
          "name": "convergence_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Make a copy of input data. ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities. ",
          "name": "preference",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Which affinity to use. At the moment ``precomputed`` and ``euclidean`` are supported. ``euclidean`` uses the negative squared euclidean distance between points. ",
          "name": "affinity",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Whether to be verbose.  ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/affinity_propagation_.pyc:191",
      "tags": [
        "cluster",
        "affinity_propagation_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "The name of the hyperparameter. Note that a kernel using a hyperparameter with name \"x\" must have the attributes self.x and self.x_bounds ",
          "name": "name",
          "type": "string"
        },
        {
          "description": "The type of the hyperparameter. Currently, only \"numeric\" hyperparameters are supported. ",
          "name": "value_type",
          "type": "string"
        },
        {
          "description": "The lower and upper bound on the parameter. If n_elements>1, a pair of 1d array with n_elements each may be given alternatively. If the string \"fixed\" is passed as bounds, the hyperparameter\\'s value cannot be changed. ",
          "name": "bounds",
          "type": "pair"
        },
        {
          "description": "The number of elements of the hyperparameter value. Defaults to 1, which corresponds to a scalar hyperparameter. n_elements > 1 corresponds to a hyperparameter which is vector-valued, such as, e.g., anisotropic length-scales. ",
          "name": "n_elements",
          "type": "int"
        },
        {
          "description": "Whether the value of this hyperparameter is fixed, i.e., cannot be changed during hyperparameter tuning. If None is passed, the \"fixed\" is derived based on the given bounds. ",
          "name": "fixed",
          "type": "bool"
        }
      ],
      "category": "gaussian_process.kernels",
      "common_name": "Hyperparameter",
      "description": "'A kernel hyperparameter\\'s specification in form of a namedtuple.\n\n.. versionadded:: 0.18\n\nAttributes\n----------\nname : string\nThe name of the hyperparameter. Note that a kernel using a\nhyperparameter with name \"x\" must have the attributes self.x and\nself.x_bounds\n\nvalue_type : string\nThe type of the hyperparameter. Currently, only \"numeric\"\nhyperparameters are supported.\n\nbounds : pair of floats >= 0 or \"fixed\"\nThe lower and upper bound on the parameter. If n_elements>1, a pair\nof 1d array with n_elements each may be given alternatively. If\nthe string \"fixed\" is passed as bounds, the hyperparameter\\'s value\ncannot be changed.\n\nn_elements : int, default=1\nThe number of elements of the hyperparameter value. Defaults to 1,\nwhich corresponds to a scalar hyperparameter. n_elements > 1\ncorresponds to a hyperparameter which is vector-valued,\nsuch as, e.g., anisotropic length-scales.\n\nfixed : bool, default: None\nWhether the value of this hyperparameter is fixed, i.e., cannot be\nchanged during hyperparameter tuning. If None is passed, the \"fixed\" is\nderived based on the given bounds.\n\n'",
      "id": "sklearn.gaussian_process.kernels.Hyperparameter",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.gaussian_process.kernels.Hyperparameter",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/kernels.pyc:47",
      "tags": [
        "gaussian_process",
        "kernels"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Parallel Backend Base",
      "description": "'Helper abc which defines all methods a ParallelBackend must implement'",
      "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Abort any running tasks\n\nThis is called when an exception has been raised when executing a tasks\nand all the remaining tasks will be ignored and can therefore be\naborted to spare computation resources.\n\nIf ensure_ready is True, the backend should be left in an operating\nstate as future tasks might be re-submitted via that same backend\ninstance.\n\nIf ensure_ready is False, the implementer of this method can decide\nto leave the backend in a closed / terminated state as no new task\nare expected to be submitted to this backend.\n\nSetting ensure_ready to False is an optimization that can be leveraged\nwhen aborting tasks via killing processes from a local process pool\nmanaged by the backend it-self: if we expect no new tasks, there is no\npoint in re-creating a new working pool.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.abort_everything",
          "name": "abort_everything",
          "parameters": []
        },
        {
          "description": "'Schedule a func to be run'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "'Callback indicate how long it took to run a batch'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.batch_completed",
          "name": "batch_completed",
          "parameters": []
        },
        {
          "description": "'Determine the optimal batch size'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.compute_batch_size",
          "name": "compute_batch_size",
          "parameters": []
        },
        {
          "description": "'Reconfigure the backend and return the number of workers.\n\nThis makes it possible to reuse an existing backend instance for\nsuccessive independent calls to Parallel with different parameters.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.configure",
          "name": "configure",
          "parameters": []
        },
        {
          "description": "'Determine the number of jobs that can actually run in parallel\n\nn_jobs is the number of workers requested by the callers. Passing\nn_jobs=-1 means requesting all available workers for instance matching\nthe number of CPU cores on the worker host(s).\n\nThis method should return a guesstimate of the number of workers that\ncan actually perform work concurrently. The primary use case is to make\nit possible for the caller to know in how many chunks to slice the\nwork.\n\nIn general working on larger data chunks is more efficient (less\nscheduling overhead and better use of CPU cache prefetching heuristics)\nas long as all the workers have enough work to do.\n'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.effective_n_jobs",
          "name": "effective_n_jobs",
          "parameters": []
        },
        {
          "description": "'List of exception types to be captured.'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.get_exceptions",
          "name": "get_exceptions",
          "parameters": []
        },
        {
          "description": "'Shutdown the process or thread pool'",
          "id": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib._parallel_backends.ParallelBackendBase",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc:21",
      "tags": [
        "externals",
        "joblib",
        "_parallel_backends"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.roc_curve",
      "description": "'Compute Receiver operating characteristic (ROC)\n\nNote: this implementation is restricted to the binary classification task.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n",
      "id": "sklearn.metrics.ranking.roc_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.roc_curve",
      "parameters": [
        {
          "description": "True binary labels in range {0, 1} or {-1, 1}.  If labels are not binary, pos_label should be explicitly given. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Label considered as positive and others are considered negative. ",
          "name": "pos_label",
          "type": "int"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "default": "True",
          "description": "Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves.  .. versionadded:: 0.17 parameter *drop_intermediate*. ",
          "name": "drop_intermediate",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i].  tpr : array, shape = [>2] Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i].  thresholds : array, shape = [n_thresholds] Decreasing thresholds on the decision function used to compute fpr and tpr. `thresholds[0]` represents no instances being predicted and is arbitrarily set to `max(y_score) + 1`.  See also -------- roc_auc_score : Compute Area Under the Curve (AUC) from prediction scores  Notes ----- Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both ``fpr`` and ``tpr``, which are sorted in reversed order during their calculation.  References ---------- .. [1] `Wikipedia entry for the Receiver operating characteristic <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_   Examples -------- >>> import numpy as np >>> from sklearn import metrics >>> y = np.array([1, 1, 2, 2]) >>> scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2) >>> fpr array([ 0. ,  0.5,  0.5,  1. ]) >>> tpr array([ 0.5,  0.5,  1. ,  1. ]) >>> thresholds array([ 0.8 ,  0.4 ,  0.35,  0.1 ])  '",
        "name": "fpr",
        "shape": ">2",
        "type": "array"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.isotonic.isotonic_regression",
      "description": "'Solve the isotonic regression model::\n\nmin sum w[i] (y[i] - y_[i]) ** 2\n\nsubject to y_min = y_[1] <= y_[2] ... <= y_[n] = y_max\n\nwhere:\n- y[i] are inputs (real numbers)\n- y_[i] are fitted\n- w[i] are optional strictly positive weights (default to 1.0)\n\nRead more in the :ref:`User Guide <isotonic>`.\n",
      "id": "sklearn.isotonic.isotonic_regression",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.isotonic.isotonic_regression",
      "parameters": [
        {
          "description": "The data. ",
          "name": "y",
          "type": "iterable"
        },
        {
          "description": "Weights on each point of the regression. If None, weight is set to 1 (equal weights). ",
          "name": "sample_weight",
          "optional": "true",
          "type": "iterable"
        },
        {
          "description": "If not None, set the lowest value of the fit to y_min. ",
          "name": "y_min",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If not None, set the highest value of the fit to y_max. ",
          "name": "y_max",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Whether to compute ``y_`` is increasing (if set to True) or decreasing (if set to False) ",
          "name": "increasing",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Isotonic fit of y.  References ---------- \"Active set algorithms for isotonic regression; A unifying framework\" by Michael J. Best and Nilotpal Chakravarti, section 3. '",
        "name": "y_",
        "type": "list"
      },
      "tags": [
        "isotonic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.graph.graph_laplacian",
      "description": "' Return the Laplacian matrix of a directed graph.\n\nFor non-symmetric graphs the out-degree is used in the computation.\n",
      "id": "sklearn.utils.graph.graph_laplacian",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.graph.graph_laplacian",
      "parameters": [
        {
          "description": "compressed-sparse graph, with shape (N, N).",
          "name": "csgraph",
          "type": "array"
        },
        {
          "description": "If True, then compute normalized Laplacian.",
          "name": "normed",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If True, then return diagonal as well as laplacian. ",
          "name": "return_diag",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The N x N laplacian matrix of graph. diag : ndarray The length-N diagonal of the laplacian matrix. diag is returned only if return_diag is True.  Notes ----- The Laplacian matrix of a graph is sometimes referred to as the \"Kirchoff matrix\" or the \"admittance matrix\", and is useful in many parts of spectral graph theory.  In particular, the eigen-decomposition of the laplacian matrix can give insight into many properties of the graph.  For non-symmetric directed graphs, the laplacian is computed using the out-degree of each node. '",
        "name": "lap",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "graph"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.sparsefuncs.incr_mean_variance_axis",
      "description": "'Compute incremental mean and variance along an axix on a CSR or\nCSC matrix.\n\nlast_mean, last_var are the statistics computed at the last step by this\nfunction. Both must be initilized to 0-arrays of the proper size, i.e.\nthe number of features in X. last_n is the number of samples encountered\nuntil now.\n",
      "id": "sklearn.utils.sparsefuncs.incr_mean_variance_axis",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.sparsefuncs.incr_mean_variance_axis",
      "parameters": [
        {
          "description": "Input data.  axis: int (either 0 or 1) Axis along which the axis should be computed.  last_mean: float array with shape (n_features,) Array of feature-wise means to update with the new data X.  last_var: float array with shape (n_features,) Array of feature-wise var to update with the new data X.  last_n: int Number of samples seen so far, excluded X. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        }
      ],
      "returns": {
        "description": "means: float array with shape (n_features,) Updated feature-wise means.  variances: float array with shape (n_features,) Updated feature-wise variances.  n: int Updated number of seen samples.  '",
        "name": ""
      },
      "tags": [
        "utils",
        "sparsefuncs"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_sparse_uncorrelated",
      "description": "'Generate a random regression problem with sparse uncorrelated design\n\nThis dataset is described in Celeux et al [1]. as::\n\nX ~ N(0, 1)\ny(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n\nOnly the first 4 features are informative. The remaining features are\nuseless.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_sparse_uncorrelated",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_sparse_uncorrelated",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "10",
          "description": "The number of features. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] The output values.  References ---------- .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert, \"Regularization in regression: comparing Bayesian and frequentist methods in a poorly informative situation\", 2009. '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_swiss_roll",
      "description": "'Generate a swiss roll dataset.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_swiss_roll",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_swiss_roll",
      "parameters": [
        {
          "default": "100",
          "description": "The number of sample points on the S curve. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The points.  t : array of shape [n_samples] The univariate position of the sample according to the main dimension of the points in the manifold.  Notes ----- The algorithm is from Marsland [1].  References ---------- .. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\", Chapter 10, 2009. http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py '",
        "name": "X",
        "shape": "n_samples, 3",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.shrunk_covariance_.ledoit_wolf",
      "description": "'Estimates the shrunk Ledoit-Wolf covariance matrix.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n",
      "id": "sklearn.covariance.shrunk_covariance_.ledoit_wolf",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.shrunk_covariance_.ledoit_wolf",
      "parameters": [
        {
          "description": "Data from which to compute the covariance estimate ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": "boolean"
        },
        {
          "description": "Size of the blocks into which the covariance matrix will be split. This is purely a memory optimization and does not affect results. ",
          "name": "block_size",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Shrunk covariance.  shrinkage : float Coefficient in the convex combination used for the computation of the shrunk estimate.  Notes ----- The regularized (shrunk) covariance is:  (1 - shrinkage)*cov + shrinkage * mu * np.identity(n_features)  where mu = trace(cov) / n_features  '",
        "name": "shrunk_cov",
        "shape": "n_features, n_features",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.classification_report",
      "description": "\"Build a text report showing the main classification metrics\n\nRead more in the :ref:`User Guide <classification_report>`.\n",
      "id": "sklearn.metrics.classification.classification_report",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.classification_report",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "Optional list of label indices to include in the report. ",
          "name": "labels",
          "shape": "n_labels",
          "type": "array"
        },
        {
          "description": "Optional display names matching the labels (same order). ",
          "name": "target_names",
          "type": "list"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Number of digits for formatting output floating point values ",
          "name": "digits",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Text summary of the precision, recall, F1 score for each class.  Examples -------- >>> from sklearn.metrics import classification_report >>> y_true = [0, 1, 2, 2, 2] >>> y_pred = [0, 0, 2, 2, 1] >>> target_names = ['class 0', 'class 1', 'class 2'] >>> print(classification_report(y_true, y_pred, target_names=target_names)) precision    recall  f1-score   support <BLANKLINE> class 0       0.50      1.00      0.67         1 class 1       0.00      0.00      0.00         1 class 2       1.00      0.67      0.80         3 <BLANKLINE> avg / total       0.70      0.60      0.61         5 <BLANKLINE>  \"",
        "name": "report",
        "type": "string"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.label_ranking_average_precision_score",
      "description": "'Compute ranking-based average precision\n\nLabel ranking average precision (LRAP) is the average over each ground\ntruth label assigned to each sample, of the ratio of true vs. total\nlabels with lower score.\n\nThis metric is used in multilabel ranking problem, where the goal\nis to give better rank to the labels associated to each sample.\n\nThe obtained score is always strictly greater than 0 and\nthe best value is 1.\n\nRead more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "id": "sklearn.metrics.ranking.label_ranking_average_precision_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.label_ranking_average_precision_score",
      "parameters": [
        {
          "description": "True binary labels in binary indicator format. ",
          "name": "y_true",
          "shape": "n_samples, n_labels",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples, n_labels",
          "type": "array"
        }
      ],
      "returns": {
        "description": " Examples -------- >>> import numpy as np >>> from sklearn.metrics import label_ranking_average_precision_score >>> y_true = np.array([[1, 0, 0], [0, 0, 1]]) >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]]) >>> label_ranking_average_precision_score(y_true, y_score)         # doctest: +ELLIPSIS 0.416...  '",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.paired_distances",
      "description": "'\nComputes the paired distances between X and Y.\n\nComputes the distances between (X[0], Y[0]), (X[1], Y[1]), etc...\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.paired_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.paired_distances",
      "parameters": [
        {
          "description": "Array 1 for distance computation.  Y : ndarray (n_samples, n_features) Array 2 for distance computation. ",
          "name": "X",
          "type": "ndarray"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including \"euclidean\", \"manhattan\", or \"cosine\". Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. ",
          "name": "metric",
          "type": "string"
        }
      ],
      "returns": {
        "description": " Examples -------- >>> from sklearn.metrics.pairwise import paired_distances >>> X = [[0, 1], [1, 1]] >>> Y = [[0, 1], [2, 1]] >>> paired_distances(X, Y) array([ 0.,  1.])  See also -------- pairwise_distances : pairwise distances. '",
        "name": "distances",
        "type": "ndarray"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.mean_shift_.get_bin_seeds",
      "description": "\"Finds seeds for mean_shift.\n\nFinds seeds by first binning data onto a grid whose lines are\nspaced bin_size apart, and then choosing those bins with at least\nmin_bin_freq points.\n",
      "id": "sklearn.cluster.mean_shift_.get_bin_seeds",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.mean_shift_.get_bin_seeds",
      "parameters": [
        {
          "description": "Controls the coarseness of the binning. Smaller values lead to more seeding (which is computationally more expensive). If you're not sure how to set this, set it to the value of the bandwidth used in clustering.mean_shift. ",
          "name": "bin_size",
          "type": "float"
        },
        {
          "description": "Only bins with at least min_bin_freq will be selected as seeds. Raising this value decreases the number of seeds found, which makes mean_shift computationally cheaper. ",
          "name": "min_bin_freq",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Points used as initial kernel positions in clustering.mean_shift. \"",
        "name": "bin_seeds",
        "shape": "n_samples, n_features",
        "type": "array-like"
      },
      "tags": [
        "cluster",
        "mean_shift_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.maxabs_scale",
      "description": "'Scale each feature to the [-1, 1] range without breaking the sparsity.\n\nThis estimator scales each feature individually such\nthat the maximal absolute value of each feature in the\ntraining set will be 1.0.\n\nThis scaler can also be applied to sparse CSR or CSC matrices.\n",
      "id": "sklearn.preprocessing.data.maxabs_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.maxabs_scale",
      "parameters": [
        {
          "description": "axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. ",
          "name": "axis",
          "type": "int"
        },
        {
          "description": "Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).  See also -------- MaxAbsScaler: Performs scaling to the [-1, 1] range using the``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). '",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.normalize",
      "description": "\"Scale input vectors individually to unit norm (vector length).\n\nRead more in the :ref:`User Guide <preprocessing_normalization>`.\n",
      "id": "sklearn.preprocessing.data.normalize",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.normalize",
      "parameters": [
        {
          "description": "The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0). ",
          "name": "norm",
          "optional": "true",
          "type": ""
        },
        {
          "description": "axis used to normalize the data along. If 1, independently normalize each sample, otherwise (if 0) normalize each feature. ",
          "name": "axis",
          "optional": "true",
          "type": ""
        },
        {
          "description": "set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "whether to return the computed norms  See also -------- Normalizer: Performs normalization using the ``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). \"",
          "name": "return_norm",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "urllib.quote",
      "description": "'quote(\\'abc def\\') -> \\'abc%20def\\'\n\nEach part of a URL, e.g. the path info, the query, etc., has a\ndifferent set of reserved characters that must be quoted.\n\nRFC 2396 Uniform Resource Identifiers (URI): Generic Syntax lists\nthe following reserved characters.\n\nreserved    = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\" |\n\"$\" | \",\"\n\nEach of these characters is reserved in some component of a URL,\nbut not necessarily in all of them.\n\nBy default, the quote function is intended for quoting the path\nsection of a URL.  Thus, it will not encode \\'/\\'.  This character\nis reserved, but in typical usage the quote function is being\ncalled on a path where the existing slash characters are used as\nreserved characters.\n'",
      "id": "urllib.quote",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "urllib.quote",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.mldata.fetch_mldata",
      "description": "'Fetch an mldata.org data set\n\nIf the file does not exist yet, it is downloaded from mldata.org .\n\nmldata.org does not have an enforced convention for storing data or\nnaming the columns in a data set. The default behavior of this function\nworks well with the most common cases:\n\n1) data values are stored in the column \\'data\\', and target values in the\ncolumn \\'label\\'\n2) alternatively, the first column stores target values, and the second\ndata values\n3) the data array is stored as `n_features x n_samples` , and thus needs\nto be transposed to match the `sklearn` standard\n\nKeyword arguments allow to adapt these defaults to specific data sets\n(see parameters `target_name`, `data_name`, `transpose_data`, and\nthe examples below).\n\nmldata.org data sets may have multiple columns, which are stored in the\nBunch object with their original name.\n",
      "id": "sklearn.datasets.mldata.fetch_mldata",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.mldata.fetch_mldata",
      "parameters": [
        {
          "description": "Name or index of the column containing the target values. ",
          "name": "target_name",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Name or index of the column containing the data. ",
          "name": "data_name",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If True, transpose the downloaded data array. ",
          "name": "transpose_data",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Specify another download and cache folder for the data sets. By default all scikit learn data is stored in \\'~/scikit_learn_data\\' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": "data : Bunch Dictionary-like object, the interesting attributes are: \\'data\\', the data to learn, \\'target\\', the classification labels, \\'DESCR\\', the full description of the dataset, and \\'COL_NAMES\\', the original names of the dataset columns.  Examples -------- Load the \\'iris\\' dataset from mldata.org:  >>> from sklearn.datasets.mldata import fetch_mldata >>> import tempfile >>> test_data_home = tempfile.mkdtemp()  >>> iris = fetch_mldata(\\'iris\\', data_home=test_data_home) >>> iris.target.shape (150,) >>> iris.data.shape (150, 4)  Load the \\'leukemia\\' dataset from mldata.org, which needs to be transposed to respects the scikit-learn axes convention:  >>> leuk = fetch_mldata(\\'leukemia\\', transpose_data=True, ...                     data_home=test_data_home) >>> leuk.data.shape (72, 7129)  Load an alternative \\'iris\\' dataset, which has different names for the columns:  >>> iris2 = fetch_mldata(\\'datasets-UCI iris\\', target_name=1, ...                      data_name=0, data_home=test_data_home) >>> iris3 = fetch_mldata(\\'datasets-UCI iris\\', ...                      target_name=\\'class\\', data_name=\\'double0\\', ...                      data_home=test_data_home)  >>> import shutil >>> shutil.rmtree(test_data_home) '",
        "name": ""
      },
      "tags": [
        "datasets",
        "mldata"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_lu.lu_factor",
      "description": "'\nCompute pivoted LU decomposition of a matrix.\n\nThe decomposition is::\n\nA = P L U\n\nwhere P is a permutation matrix, L lower triangular with unit\ndiagonal elements, and U upper triangular.\n",
      "id": "scipy.linalg.decomp_lu.lu_factor",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_lu.lu_factor",
      "parameters": [
        {
          "description": "Matrix to decompose",
          "name": "a",
          "type": ""
        },
        {
          "description": "Whether to overwrite data in A (may increase performance)",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrix contains only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Matrix containing U in its upper triangle, and L in its lower triangle. The unit diagonal elements of L are not stored. piv : (N,) ndarray Pivot indices representing the permutation matrix P: row i of matrix was interchanged with row piv[i].  See also -------- lu_solve : solve an equation system using the LU factorization of a matrix  Notes ----- This is a wrapper to the ``*GETRF`` routines from LAPACK.  '",
        "name": "lu",
        "type": ""
      },
      "tags": [
        "linalg",
        "decomp_lu"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_sparse_coded_signal",
      "description": "'Generate a signal as a sparse combination of dictionary elements.\n\nReturns a matrix Y = DX, such as D is (n_features, n_components),\nX is (n_components, n_samples) and each column of X has exactly\nn_nonzero_coefs non-zero elements.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_sparse_coded_signal",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_sparse_coded_signal",
      "parameters": [
        {
          "description": "number of samples to generate  n_components:  int, number of components in the dictionary ",
          "name": "n_samples",
          "type": "int"
        },
        {
          "description": "number of features of the dataset to generate ",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "number of active (non-zero) coefficients in each sample ",
          "name": "n_nonzero_coefs",
          "type": "int"
        },
        {
          "default": "None",
          "description": "seed used by the pseudo random number generator ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The encoded signal (Y).  dictionary : array of shape [n_features, n_components] The dictionary with normalized components (D).  code : array of shape [n_components, n_samples] The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X).  '",
        "name": "data",
        "shape": "n_features, n_samples",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.california_housing.fetch_california_housing",
      "description": "\"Loader for the California housing dataset from StatLib.\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.california_housing.fetch_california_housing",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.california_housing.fetch_california_housing",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in '~/scikit_learn_data' subfolders.  download_if_missing: optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": " dataset.data : ndarray, shape [20640, 8] Each row corresponding to the 8 feature values in order.  dataset.target : numpy array of shape (20640,) Each value corresponds to the average house value in units of 100,000.  dataset.feature_names : array of length 8 Array of ordered feature names used in the dataset.  dataset.DESCR : string Description of the California housing dataset.  Notes ------  This dataset consists of 20,640 samples and 9 features. \"",
        "name": "dataset",
        "type": "dict-like"
      },
      "tags": [
        "datasets",
        "california_housing"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "tokenize.generate_tokens",
      "description": "'\nThe generate_tokens() generator requires one argument, readline, which\nmust be a callable object which provides the same interface as the\nreadline() method of built-in file objects. Each call to the function\nshould return one line of input as a string.  Alternately, readline\ncan be a callable function terminating with StopIteration:\nreadline = open(myfile).next    # Example of alternate readline\n\nThe generator produces 5-tuples with these members: the token type; the\ntoken string; a 2-tuple (srow, scol) of ints specifying the row and\ncolumn where the token begins in the source; a 2-tuple (erow, ecol) of\nints specifying the row and column where the token ends in the source;\nand the line on which the token was found. The line passed is the\nlogical line; continuation lines are included.\n'",
      "id": "tokenize.generate_tokens",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "tokenize.generate_tokens",
      "parameters": [],
      "tags": [],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.parallel.parallel_backend",
      "description": "\"Change the default backend used by Parallel inside a with block.\n\nIf ``backend`` is a string it must match a previously registered\nimplementation using the ``register_parallel_backend`` function.\n\nAlternatively backend can be passed directly as an instance.\n\nBy default all available workers will be used (``n_jobs=-1``) unless the\ncaller passes an explicit value for the ``n_jobs`` parameter.\n\nThis is an alternative to passing a ``backend='backend_name'`` argument to\nthe ``Parallel`` class constructor. It is particularly useful when calling\ninto library code that uses joblib internally but does not expose the\nbackend argument in its own API.\n\n>>> from operator import neg\n>>> with parallel_backend('threading'):\n...     print(Parallel()(delayed(neg)(i + 1) for i in range(5)))\n...\n[-1, -2, -3, -4, -5]\n\nWarning: this function is experimental and subject to change in a future\nversion of joblib.\n\n.. versionadded:: 0.10\n\n\"",
      "id": "sklearn.externals.joblib.parallel.parallel_backend",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.parallel.parallel_backend",
      "parameters": [],
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_lu.lu_solve",
      "description": "'Solve an equation system, a x = b, given the LU factorization of a\n",
      "id": "scipy.linalg.decomp_lu.lu_solve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_lu.lu_solve",
      "parameters": [
        {
          "description": "Right-hand side",
          "name": "b",
          "type": "array"
        },
        {
          "description": "Type of system to solve:  =====  ========= trans  system =====  ========= 0      a x   = b 1      a^T x = b 2      a^H x = b =====  =========",
          "name": "trans",
          "optional": "true",
          "type": "0, 1, 2"
        },
        {
          "description": "Whether to overwrite data in b (may increase performance)",
          "name": "overwrite_b",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrices contain only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Solution to the system  See also -------- lu_factor : LU factorize a matrix  '",
        "name": "x",
        "type": "array"
      },
      "tags": [
        "linalg",
        "decomp_lu"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.class_weight.compute_class_weight",
      "description": "'Estimate class weights for unbalanced datasets.\n",
      "id": "sklearn.utils.class_weight.compute_class_weight",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.class_weight.compute_class_weight",
      "parameters": [
        {
          "description": "If \\'balanced\\', class weights will be given by ``n_samples / (n_classes * np.bincount(y))``. If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform. ",
          "name": "class_weight",
          "type": "dict"
        },
        {
          "description": "Array of the classes occurring in the data, as given by ``np.unique(y_org)`` with ``y_org`` the original class labels. ",
          "name": "classes",
          "type": "ndarray"
        },
        {
          "description": "Array of original class labels per sample; ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Array with class_weight_vect[i] the weight for i-th class  References ---------- The \"balanced\" heuristic is inspired by Logistic Regression in Rare Events Data, King, Zen, 2001. '",
        "name": "class_weight_vect",
        "shape": "n_classes,",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "class_weight"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._split.check_cv",
      "description": "'Input checker utility for building a cross-validator\n",
      "id": "sklearn.model_selection._split.check_cv",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._split.check_cv",
      "parameters": [
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if classifier is True and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The target variable for supervised learning problems. ",
          "name": "y",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Whether the task is a classification task, in which case stratified KFold will be used. ",
          "name": "classifier",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "The return value is a cross-validator which generates the train/test splits via the ``split`` method. '",
        "name": "checked_cv",
        "type": "a"
      },
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle.load",
      "description": "\"Reconstruct a Python object from a file persisted with joblib.dump.\n",
      "id": "sklearn.externals.joblib.numpy_pickle.load",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle.load",
      "parameters": [
        {
          "description": "The path of the file from which to load the object mmap_mode: {None, 'r+', 'r', 'w+', 'c'}, optional If not None, the arrays are memory-mapped from the disk. This mode has no effect for compressed files. Note that in this case the reconstructed object might not longer match exactly the originally pickled object. ",
          "name": "filename",
          "type": "str"
        }
      ],
      "returns": {
        "description": "The object stored in the file.  See Also -------- joblib.dump : function to save an object  Notes -----  This function can load numpy array files saved separately during the dump. If the mmap_mode argument is given, it is passed to np.load and arrays are loaded as memmaps. As a consequence, the reconstructed object might not match the original pickled object. Note that if the file was saved with compression, the arrays cannot be memmaped. \"",
        "name": "result: any Python object"
      },
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.image.img_to_graph",
      "description": "'Graph of the pixel-to-pixel gradient connections\n\nEdges are weighted with the gradient values.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.image.img_to_graph",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.image.img_to_graph",
      "parameters": [
        {
          "description": "2D or 3D image",
          "name": "img",
          "type": "ndarray"
        },
        {
          "description": "An optional mask of the image, to consider only part of the pixels.",
          "name": "mask",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "The class to use to build the returned adjacency matrix.",
          "name": "return_as",
          "optional": "true",
          "type": "np"
        },
        {
          "description": "The data of the returned sparse matrix. By default it is the dtype of img  Notes ----- For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance.  Going forward, np.ndarray returns an np.ndarray, as expected.  For compatibility, user code relying on this method should wrap its calls in ``np.asarray`` to avoid type issues. '",
          "name": "dtype",
          "optional": "true",
          "type": ""
        }
      ],
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.univariate_selection.f_regression",
      "description": "'Univariate linear regression tests.\n\nQuick linear model for testing the effect of a single regressor,\nsequentially for many regressors.\n\nThis is done in 2 steps:\n\n1. The cross correlation between each regressor and the target is computed,\nthat is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\nstd(y)).\n2. It is converted to an F score then to a p-value.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.f_regression",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.univariate_selection.f_regression",
      "parameters": [
        {
          "description": "The set of regressors that will be tested sequentially. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "The data matrix ",
          "name": "y",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "If true, X and y will be centered. ",
          "name": "center",
          "type": ""
        }
      ],
      "returns": {
        "description": "F values of features.  pval : array, shape=(n_features,) p-values of F-scores.  See also -------- f_classif: ANOVA F-value between label/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. '",
        "name": "F",
        "shape": "n_features,",
        "type": "array"
      },
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.contingency_matrix",
      "description": "'Build a contingency matrix describing the relationship between labels.\n",
      "id": "sklearn.metrics.cluster.supervised.contingency_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.contingency_matrix",
      "parameters": [
        {
          "description": "Ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "Cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "If a float, that value is added to all values in the contingency matrix. This helps to stop NaN propagation. If ``None``, nothing is adjusted. ",
          "name": "eps",
          "optional": "true",
          "type": ""
        },
        {
          "description": "If True, return a sparse CSR continency matrix. If ``eps is not None``, and ``sparse is True``, will throw ValueError.  .. versionadded:: 0.18 ",
          "name": "sparse",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Matrix :math:`C` such that :math:`C_{i, j}` is the number of samples in true class :math:`i` and in predicted class :math:`j`. If ``eps is None``, the dtype of this array will be integer. If ``eps`` is given, the dtype will be float. Will be a ``scipy.sparse.csr_matrix`` if ``sparse=True``. '",
        "name": "contingency",
        "shape": "n_classes_true, n_classes_pred",
        "type": "array-like, sparse"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.check_paired_arrays",
      "description": "' Set X and Y appropriately and checks inputs for paired distances\n\nAll paired distance metrics should use this function first to assert that\nthe given parameters are correct and safe to use.\n\nSpecifically, this function first ensures that both X and Y are arrays,\nthen checks that they are at least two dimensional while ensuring that\ntheir elements are floats. Finally, the function checks that the size\nof the dimensions of the two arrays are equal.\n",
      "id": "sklearn.metrics.pairwise.check_paired_arrays",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.check_paired_arrays",
      "parameters": [
        {
          "description": " Y : {array-like, sparse matrix}, shape (n_samples_b, n_features) ",
          "name": "X",
          "shape": "n_samples_a, n_features",
          "type": "array-like, sparse matrix"
        }
      ],
      "returns": {
        "description": "An array equal to X, guaranteed to be a numpy array.  safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features) An array equal to Y if Y was not None, guaranteed to be a numpy array. If Y was None, safe_Y will be a pointer to X.  '",
        "name": "safe_X",
        "shape": "n_samples_a, n_features",
        "type": "array-like, sparse matrix"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.coverage_error",
      "description": "'Coverage error measure\n\nCompute how far we need to go through the ranked scores to cover all\ntrue labels. The best value is equal to the average number\nof labels in ``y_true`` per sample.\n\nTies in ``y_scores`` are broken by giving maximal rank that would have\nbeen assigned to all tied values.\n\nRead more in the :ref:`User Guide <coverage_error>`.\n",
      "id": "sklearn.metrics.ranking.coverage_error",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.coverage_error",
      "parameters": [
        {
          "description": "True binary labels in binary indicator format. ",
          "name": "y_true",
          "shape": "n_samples, n_labels",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples, n_labels",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In Data mining and knowledge discovery handbook (pp. 667-685). Springer US.  '",
        "name": "coverage_error",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.construct.eye",
      "description": "'Sparse matrix with ones on diagonal\n\nReturns a sparse (m x n) matrix where the k-th diagonal\nis all ones and everything else is zeros.\n",
      "id": "scipy.sparse.construct.eye",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.construct.eye",
      "parameters": [
        {
          "description": "Number of rows in the matrix.",
          "name": "m",
          "type": "int"
        },
        {
          "description": "Number of columns. Default: `m`.",
          "name": "n",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Diagonal to place ones on. Default: 0 (main diagonal).",
          "name": "k",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Data type of the matrix.",
          "name": "dtype",
          "optional": "true",
          "type": "dtype"
        },
        {
          "description": "Sparse format of the result, e.g. format=\"csr\", etc.  Examples -------- >>> from scipy import sparse >>> sparse.eye(3).toarray() array([[ 1.,  0.,  0.], [ 0.,  1.,  0.], [ 0.,  0.,  1.]]) >>> sparse.eye(3, dtype=np.int8) <3x3 sparse matrix of type \\'<type \\'numpy.int8\\'>\\' with 3 stored elements (1 diagonals) in DIAgonal format>  '",
          "name": "format",
          "optional": "true",
          "type": "str"
        }
      ],
      "tags": [
        "sparse",
        "construct"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.bicluster.consensus_score",
      "description": "'The similarity of two sets of biclusters.\n\nSimilarity between individual biclusters is computed. Then the\nbest matching between sets is found using the Hungarian algorithm.\nThe final score is the sum of similarities divided by the size of\nthe larger set.\n\nRead more in the :ref:`User Guide <biclustering>`.\n",
      "id": "sklearn.metrics.cluster.bicluster.consensus_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.bicluster.consensus_score",
      "parameters": [
        {
          "description": "Tuple of row and column indicators for a set of biclusters. ",
          "name": "a",
          "type": ""
        },
        {
          "description": "Another set of biclusters like ``a``. ",
          "name": "b",
          "type": ""
        },
        {
          "description": "May be the string \"jaccard\" to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns).  References ----------  * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis for bicluster acquisition <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.  '",
          "name": "similarity",
          "optional": "true",
          "type": "string"
        }
      ],
      "tags": [
        "metrics",
        "cluster",
        "bicluster"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.cosine_similarity",
      "description": "'Compute cosine similarity between samples in X and Y.\n\nCosine similarity, or the cosine kernel, computes similarity as the\nnormalized dot product of X and Y:\n\nK(X, Y) = <X, Y> / (||X||*||Y||)\n\nOn L2-normalized data, this function is equivalent to linear_kernel.\n\nRead more in the :ref:`User Guide <cosine_similarity>`.\n",
      "id": "sklearn.metrics.pairwise.cosine_similarity",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.cosine_similarity",
      "parameters": [
        {
          "description": "Input data.  Y : ndarray or sparse array, shape: (n_samples_Y, n_features) Input data. If ``None``, the output will be the pairwise similarities between all samples in ``X``. ",
          "name": "X",
          "type": "ndarray"
        },
        {
          "description": "Whether to return dense output even when the input is sparse. If ``False``, the output is sparse if both input arrays are sparse.  .. versionadded:: 0.17 parameter ``dense_output`` for dense output. ",
          "name": "dense_output",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "An array with shape (n_samples_X, n_samples_Y). '",
        "name": "kernel matrix",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.special.basic.comb",
      "description": "'The number of combinations of N things taken k at a time.\n\nThis is often expressed as \"N choose k\".\n",
      "id": "scipy.special.basic.comb",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.special.basic.comb",
      "parameters": [
        {
          "description": "Number of things.",
          "name": "N",
          "type": "int"
        },
        {
          "description": "Number of elements taken.",
          "name": "k",
          "type": "int"
        },
        {
          "description": "If `exact` is False, then floating point precision is used, otherwise exact long integer is computed.",
          "name": "exact",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If `repetition` is True, then the number of combinations with repetition is computed. ",
          "name": "repetition",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The total number of combinations.  Notes ----- - Array arguments accepted only for exact=False case. - If k > N, N < 0, or k < 0, then a 0 is returned.  Examples -------- >>> from scipy.special import comb >>> k = np.array([3, 4]) >>> n = np.array([10, 10]) >>> comb(n, k, exact=False) array([ 120.,  210.]) >>> comb(10, 3, exact=True) 120L >>> comb(10, 3, exact=True, repetition=True) 220L  '",
        "name": "val",
        "type": "int"
      },
      "tags": [
        "special",
        "basic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.locally_linear.barycenter_kneighbors_graph",
      "description": "\"Computes the barycenter weighted graph of k-Neighbors for points in X\n",
      "id": "sklearn.manifold.locally_linear.barycenter_kneighbors_graph",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.locally_linear.barycenter_kneighbors_graph",
      "parameters": [
        {
          "description": "Sample data, shape = (n_samples, n_features), in the form of a numpy array, sparse array, precomputed tree, or NearestNeighbors object. ",
          "name": "X",
          "type": "array-like, sparse matrix, BallTree, KDTree, NearestNeighbors"
        },
        {
          "description": "Number of neighbors for each sample. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "Amount of regularization when solving the least-squares problem. Only relevant if mode='barycenter'. If None, use the default. ",
          "name": "reg",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "A[i, j] is assigned the weight of edge that connects i to j.  See also -------- sklearn.neighbors.kneighbors_graph sklearn.neighbors.radius_neighbors_graph \"",
        "name": "A",
        "shape": "n_samples, n_samples",
        "type": "sparse"
      },
      "tags": [
        "manifold",
        "locally_linear"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.mlcomp.load_mlcomp",
      "description": "\"Load a datasets as downloaded from http://mlcomp.org\n",
      "id": "sklearn.datasets.mlcomp.load_mlcomp",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.mlcomp.load_mlcomp",
      "parameters": [
        {
          "description": "dataset to load ",
          "name": "name_or_id",
          "type": "the"
        },
        {
          "description": "",
          "name": "set_",
          "type": "select"
        },
        {
          "description": "are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead.  **kwargs : domain specific kwargs to be passed to the dataset loader.  Read more in the :ref:`User Guide <datasets>`. ",
          "name": "mlcomp_root",
          "type": "the"
        }
      ],
      "returns": {
        "description": "data : Bunch Dictionary-like object, the interesting attributes are: 'filenames', the files holding the raw to learn, 'target', the classification labels (integer index), 'target_names', the meaning of the labels, and 'DESCR', the full description of the dataset.  Note on the lookup process: depending on the type of name_or_id, will choose between integer id lookup or metadata name lookup by looking at the unzipped archives and metadata file.  TODO: implement zip dataset loading too \"",
        "name": ""
      },
      "tags": [
        "datasets",
        "mlcomp"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.linear_assignment_.linear_assignment",
      "description": "'Solve the linear assignment problem using the Hungarian algorithm.\n\nThe problem is also known as maximum weight matching in bipartite graphs.\nThe method is also known as the Munkres or Kuhn-Munkres algorithm.\n",
      "id": "sklearn.utils.linear_assignment_.linear_assignment",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.linear_assignment_.linear_assignment",
      "parameters": [
        {
          "description": "The cost matrix of the bipartite graph ",
          "name": "X",
          "type": "array"
        }
      ],
      "returns": {
        "description": "The pairs of (row, col) indices in the original array giving the original ordering.  References ----------  1. http://www.public.iastate.edu/~ddoty/HungarianAlgorithm.html  2. Harold W. Kuhn. The Hungarian Method for the assignment problem. *Naval Research Logistics Quarterly*, 2:83-97, 1955.  3. Harold W. Kuhn. Variants of the Hungarian method for assignment problems. *Naval Research Logistics Quarterly*, 3: 253-258, 1956.  4. Munkres, J. Algorithms for the Assignment and Transportation Problems. *Journal of the Society of Industrial and Applied Mathematics*, 5(1):32-38, March, 1957.  5. https://en.wikipedia.org/wiki/Hungarian_algorithm '",
        "name": "indices",
        "type": "array"
      },
      "tags": [
        "utils",
        "linear_assignment_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.binarize",
      "description": "'Boolean thresholding of array-like or scipy.sparse matrix\n\nRead more in the :ref:`User Guide <preprocessing_binarization>`.\n",
      "id": "sklearn.preprocessing.data.binarize",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.binarize",
      "parameters": [
        {
          "description": "The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices. ",
          "name": "threshold",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR / CSC matrix and if axis is 1).  See also -------- Binarizer: Performs binarization using the ``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). '",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.deprecation.log_multivariate_normal_density",
      "description": "\"DEPRECATED: The function log_multivariate_normal_density is deprecated in 0.18 and will be removed in 0.20.\n\nCompute the log probability under a multivariate Gaussian distribution.\n",
      "id": "sklearn.utils.deprecation.log_multivariate_normal_density",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.deprecation.log_multivariate_normal_density",
      "parameters": [
        {
          "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "List of n_features-dimensional mean vectors for n_components Gaussians. Each row corresponds to a single mean vector. ",
          "name": "means",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "List of n_components covariance parameters for each Gaussian. The shape depends on `covariance_type`: (n_components, n_features)      if 'spherical', (n_features, n_features)    if 'tied', (n_components, n_features)    if 'diag', (n_components, n_features, n_features) if 'full' ",
          "name": "covars",
          "type": "array"
        },
        {
          "description": "Type of the covariance parameters.  Must be one of 'spherical', 'tied', 'diag', 'full'.  Defaults to 'diag'. ",
          "name": "covariance_type",
          "type": "string"
        }
      ],
      "returns": {
        "description": "Array containing the log probabilities of each data point in X under each of the n_components multivariate Gaussian distributions. \"",
        "name": "lpr",
        "shape": "n_samples, n_components",
        "type": "array"
      },
      "tags": [
        "utils",
        "deprecation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.image.grid_to_graph",
      "description": "'Graph of the pixel-to-pixel connections\n\nEdges exist if 2 voxels are connected.\n",
      "id": "sklearn.feature_extraction.image.grid_to_graph",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.image.grid_to_graph",
      "parameters": [
        {
          "description": "Dimension in x axis",
          "name": "n_x",
          "type": "int"
        },
        {
          "description": "Dimension in y axis",
          "name": "n_y",
          "type": "int"
        },
        {
          "description": "Dimension in z axis",
          "name": "n_z",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "An optional mask of the image, to consider only part of the pixels.",
          "name": "mask",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "The class to use to build the returned adjacency matrix.",
          "name": "return_as",
          "optional": "true",
          "type": "np"
        },
        {
          "description": "The data of the returned sparse matrix. By default it is int  Notes ----- For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance.  Going forward, np.ndarray returns an np.ndarray, as expected.  For compatibility, user code relying on this method should wrap its calls in ``np.asarray`` to avoid type issues. '",
          "name": "dtype",
          "optional": "true",
          "type": "dtype"
        }
      ],
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores, None if `score_func` returned scores only.  See also -------- f_classif: ANOVA F-value between label/feature for classification tasks. mutual_info_classif: Mutual information for a discrete target. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. mutual_info_regression: Mutual information for a continuous target. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate.",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Generic Univariate Select",
      "description": "\"Univariate feature selector with configurable strategy.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.GenericUnivariateSelect",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes 'percentile' or 'kbest' it can return a single array scores. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "Feature selection mode. ",
          "name": "mode",
          "type": "'percentile', 'k_best', 'fpr', 'fdr', 'fwe'"
        },
        {
          "description": "Parameter of the corresponding mode. ",
          "name": "param",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:658",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores.  See also -------- f_classif: ANOVA F-value between label/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. mutual_info_classif: f_regression: F-value between label/feature for regression tasks. mutual_info_regression: Mutual information between features and the target. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFdr: Select features based on an estimated false discovery rate. SelectFwe: Select features based on family-wise error rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Select Fpr",
      "description": "'Filter: Select the pvalues below alpha based on a FPR test.\n\nFPR test stands for False Positive Rate test. It controls the total\namount of false detections.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.SelectFpr",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFpr.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.SelectFpr",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below \"See also\"). The default function only works with classification tasks. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "The highest p-value for features to be kept. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:493",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Mean or median or quantile of the training targets or constant value given by the user. ",
          "name": "constant_",
          "shape": "n_outputs",
          "type": "float"
        },
        {
          "description": "Number of outputs.  outputs_2d_ : bool, True if the output at fit is 2d, else false.",
          "name": "n_outputs_",
          "type": "int"
        }
      ],
      "category": "dummy",
      "common_name": "Dummy Regressor",
      "description": "'\nDummyRegressor is a regressor that makes predictions using\nsimple rules.\n\nThis regressor is useful as a simple baseline to compare with other\n(real) regressors. Do not use it for real problems.\n\nRead more in the :ref:`User Guide <dummy_estimators>`.\n",
      "id": "sklearn.dummy.DummyRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the random regressor.\n",
          "id": "sklearn.dummy.DummyRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.dummy.DummyRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nPerform classification on test vectors X.\n",
          "id": "sklearn.dummy.DummyRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Input vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values for X. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.dummy.DummyRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.dummy.DummyRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.dummy.DummyRegressor",
      "parameters": [
        {
          "description": "Strategy to use to generate predictions.  * \"mean\": always predicts the mean of the training set * \"median\": always predicts the median of the training set * \"quantile\": always predicts a specified quantile of the training set, provided with the quantile parameter. * \"constant\": always predicts a constant value that is provided by the user. ",
          "name": "strategy",
          "type": "str"
        },
        {
          "description": "The explicit constant as predicted by the \"constant\" strategy. This parameter is useful only for the \"constant\" strategy. ",
          "name": "constant",
          "shape": "n_outputs",
          "type": "int"
        },
        {
          "description": "The quantile to predict using the \"quantile\" strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum. ",
          "name": "quantile",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/dummy.pyc:323",
      "tags": [
        "dummy"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "feature_extraction.hashing",
      "common_name": "Feature Hasher",
      "description": "'Implements feature hashing, aka the hashing trick.\n\nThis class turns sequences of symbolic feature names (strings) into\nscipy.sparse matrices, using a hash function to compute the matrix column\ncorresponding to a name. The hash function employed is the signed 32-bit\nversion of Murmurhash3.\n\nFeature names of type byte string are used as-is. Unicode strings are\nconverted to UTF-8 first, but no Unicode normalization is done.\nFeature values must be (finite) numbers.\n\nThis class is a low-memory alternative to DictVectorizer and\nCountVectorizer, intended for large-scale (online) learning and situations\nwhere memory is tight, e.g. when running prediction code on embedded\ndevices.\n\nRead more in the :ref:`User Guide <feature_hashing>`.\n",
      "id": "sklearn.feature_extraction.hashing.FeatureHasher",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"No-op.\n\nThis method doesn't do anything. It exists purely for compatibility\nwith the scikit-learn transformer API.\n",
          "id": "sklearn.feature_extraction.hashing.FeatureHasher.fit",
          "name": "fit",
          "parameters": [],
          "returns": {
            "description": " \"",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_extraction.hashing.FeatureHasher.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.hashing.FeatureHasher.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.hashing.FeatureHasher.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform a sequence of instances to a scipy.sparse matrix.\n",
          "id": "sklearn.feature_extraction.hashing.FeatureHasher.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Samples. Each sample must be iterable an (e.g., a list or tuple) containing/generating feature names (and optionally values, see the input_type constructor argument) which will be hashed. raw_X need not support the len function, so it can be the result of a generator; n_samples is determined on the fly.",
              "name": "raw_X",
              "type": "iterable"
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "Feature matrix, for use with estimators or further transformers.  '",
            "name": "X",
            "shape": "n_samples, self.n_features",
            "type": "scipy"
          }
        }
      ],
      "name": "sklearn.feature_extraction.hashing.FeatureHasher",
      "parameters": [
        {
          "description": "The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners.",
          "name": "n_features",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type.",
          "name": "dtype",
          "optional": "true",
          "type": "numpy"
        },
        {
          "description": "Either \"dict\" (the default) to accept dictionaries over (feature_name, value); \"pair\" to accept pairs of (feature_name, value); or \"string\" to accept single strings. feature_name should be a string, while value should be a number. In the case of \"string\", a value of 1 is implied. The feature_name is hashed to find the appropriate column for the feature. The value\\'s sign might be flipped in the output (but see non_negative, below).",
          "name": "input_type",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Whether output matrices should contain non-negative values only; effectively calls abs on the matrix prior to returning it. When True, output values can be interpreted as frequencies. When False, output values will have expected value zero. ",
          "name": "non_negative",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/hashing.pyc:18",
      "tags": [
        "feature_extraction",
        "hashing"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The unchanged dictionary atoms  See also -------- DictionaryLearning MiniBatchDictionaryLearning SparsePCA MiniBatchSparsePCA sparse_encode",
          "name": "components_",
          "type": "array"
        }
      ],
      "category": "decomposition.dict_learning",
      "common_name": "Sparse Coder",
      "description": "\"Sparse coding\n\nFinds a sparse representation of data against a fixed, precomputed\ndictionary.\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\nX ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n",
      "id": "sklearn.decomposition.dict_learning.SparseCoder",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Do nothing and return the estimator unchanged\n\nThis method is just there to implement the usual API and hence\nwork in pipelines.\n'",
          "id": "sklearn.decomposition.dict_learning.SparseCoder.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.dict_learning.SparseCoder.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.dict_learning.SparseCoder.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.dict_learning.SparseCoder.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.\n",
          "id": "sklearn.decomposition.dict_learning.SparseCoder.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Test data to be transformed, must have the same number of features as the data used to train the model. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Transformed data  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.decomposition.dict_learning.SparseCoder",
      "parameters": [
        {
          "description": "The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm. ",
          "name": "dictionary",
          "type": "array"
        },
        {
          "description": "Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'`` ",
          "name": "transform_algorithm",
          "type": "'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'"
        },
        {
          "description": "Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case. ",
          "name": "transform_n_nonzero_coefs",
          "type": "int"
        },
        {
          "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`. ",
          "name": "transform_alpha",
          "type": "float"
        },
        {
          "description": "Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. ",
          "name": "split_sign",
          "type": "bool"
        },
        {
          "description": "number of parallel jobs to run ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc:831",
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering"
      ],
      "attributes": [
        {
          "description": "Spectral embedding of the training matrix. ",
          "name": "embedding_",
          "shape": "n_samples, n_components",
          "type": "array"
        },
        {
          "description": "Affinity_matrix constructed from samples or precomputed. ",
          "name": "affinity_matrix_",
          "shape": "n_samples, n_samples",
          "type": "array"
        }
      ],
      "category": "manifold.spectral_embedding_",
      "common_name": "Spectral Embedding",
      "description": "'Spectral embedding for non-linear dimensionality reduction.\n\nForms an affinity matrix given by the specified function and\napplies spectral decomposition to the corresponding graph laplacian.\nThe resulting transformation is given by the value of the\neigenvectors for each data point.\n\nRead more in the :ref:`User Guide <spectral_embedding>`.\n",
      "id": "sklearn.manifold.spectral_embedding_.SpectralEmbedding",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.manifold.spectral_embedding_.SpectralEmbedding.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples is the number of samples and n_features is the number of features.  If affinity is \"precomputed\" X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit the model from data in X and transform X.\n",
          "id": "sklearn.manifold.spectral_embedding_.SpectralEmbedding.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vector, where n_samples is the number of samples and n_features is the number of features.  If affinity is \"precomputed\" X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.manifold.spectral_embedding_.SpectralEmbedding.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.manifold.spectral_embedding_.SpectralEmbedding.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.manifold.spectral_embedding_.SpectralEmbedding",
      "parameters": [
        {
          "description": "The dimension of the projected subspace. ",
          "name": "n_components",
          "type": "integer"
        },
        {
          "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. ",
          "name": "eigen_solver",
          "type": "None, \\'arpack\\', \\'lobpcg\\', or \\'amg\\'"
        },
        {
          "description": "A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition when eigen_solver == \\'amg\\'. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "How to construct the affinity matrix. - \\'nearest_neighbors\\' : construct affinity matrix by knn graph - \\'rbf\\' : construct affinity matrix by rbf kernel - \\'precomputed\\' : interpret X as precomputed affinity matrix - callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples). ",
          "name": "affinity",
          "type": "string"
        },
        {
          "description": "Kernel coefficient for rbf kernel. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of nearest neighbors for nearest_neighbors graph building. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/manifold/spectral_embedding_.pyc:324",
      "tags": [
        "manifold",
        "spectral_embedding_"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "neighbors.base",
      "common_name": "Radius Neighbors Mixin",
      "description": "'Mixin for radius-based neighbors searches'",
      "id": "sklearn.neighbors.base.RadiusNeighborsMixin",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n",
          "id": "sklearn.neighbors.base.RadiusNeighborsMixin.radius_neighbors",
          "name": "radius_neighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "type": "array-like"
            },
            {
              "description": "Limiting distance of neighbors to return. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the ``metric`` constructor parameter.  ind : array, shape (n_samples,) of arrays An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size ``radius`` around the query points.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1, 1, 1]:  >>> import numpy as np >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.6) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> rng = neigh.radius_neighbors([[1., 1., 1.]]) >>> print(np.asarray(rng[0][0])) # doctest: +ELLIPSIS [ 1.5  0.5] >>> print(np.asarray(rng[1][0])) # doctest: +ELLIPSIS [1 2]  The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.  Notes ----- Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, `radius_neighbors` returns arrays of objects, where each object is a 1D array of indices or distances. \"",
            "name": "dist",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n",
          "id": "sklearn.neighbors.base.RadiusNeighborsMixin.radius_neighbors_graph",
          "name": "radius_neighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Radius of neighborhoods. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.5) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.radius_neighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.neighbors.base.RadiusNeighborsMixin",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/base.pyc:499",
      "tags": [
        "neighbors",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Features got by optimizing the Huber loss. ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Bias. ",
          "name": "intercept_",
          "type": "float"
        },
        {
          "description": "The value by which ``|y - X'w - c|`` is scaled down. ",
          "name": "scale_",
          "type": "float"
        },
        {
          "description": "Number of iterations that fmin_l_bfgs_b has run for. Not available if SciPy version is 0.9 and below.  outliers_: array, shape (n_samples,) A boolean mask which is set to True where the samples are identified as outliers. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.huber",
      "common_name": "Huber Regressor",
      "description": "\"Linear regression model that is robust to outliers.\n\nThe Huber Regressor optimizes the squared loss for the samples where\n``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\nwhere ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\nto be optimized. The parameter sigma makes sure that if y is scaled up\nor down by a certain factor, one does not need to rescale epsilon to\nachieve the same robustness. Note that this does not take into account\nthe fact that the different features of X may be of different scales.\n\nThis makes sure that the loss function is not heavily influenced by the\noutliers while not completely ignoring their effect.\n\nRead more in the :ref:`User Guide <huber_regression>`\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.linear_model.huber.HuberRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model according to the given training data.\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target vector relative to X. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Weight given to each sample. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.huber.HuberRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.huber.HuberRegressor",
      "parameters": [
        {
          "description": "The parameter epsilon controls the number of samples that should be classified as outliers. The smaller the epsilon, the more robust it is to outliers. ",
          "name": "epsilon",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b should run for. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "Regularization parameter. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "This is useful if the stored attributes of a previously used model has to be reused. If set to False, then the coefficients will be rewritten for every call to fit. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "description": "Whether or not to fit the intercept. This can be set to False if the data is already centered around the origin. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The iteration will stop when ``max{|proj g_i | i = 1, ..., n}`` <= ``tol`` where pg_i is the i-th component of the projected gradient. ",
          "name": "tol",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/huber.pyc:125",
      "tags": [
        "linear_model",
        "huber"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Number of active features across every target. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.omp",
      "common_name": "Orthogonal Matching Pursuit",
      "description": "\"Orthogonal Matching Pursuit model (OMP)\n",
      "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values.  ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuit.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.omp.OrthogonalMatchingPursuit",
      "parameters": [
        {
          "description": "Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. ",
          "name": "n_nonzero_coefs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to `False`. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when `n_targets` or `n_samples` is very large. Note that if you already have such matrices, you can pass them directly to the fit method.  Read more in the :ref:`User Guide <omp>`. ",
          "name": "precompute",
          "type": "True, False, 'auto'"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/omp.pyc:543",
      "tags": [
        "linear_model",
        "omp"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "linear_model.stochastic_gradient",
      "common_name": "Base SGD Classifier",
      "description": "None",
      "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_classes, n_features",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of the training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of the target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained by via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to contain all labels in `classes`. ",
              "name": "classes",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.stochastic_gradient.BaseSGDClassifier",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc:302",
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Per feature adjustment for minimum. ",
          "name": "min_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "Per feature relative scaling of the data.  .. versionadded:: 0.17 *scale_* attribute. ",
          "name": "scale_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "Per feature minimum seen in the data  .. versionadded:: 0.17 *data_min_* instead of deprecated *data_min*. ",
          "name": "data_min_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "Per feature maximum seen in the data  .. versionadded:: 0.17 *data_max_* instead of deprecated *data_max*. ",
          "name": "data_max_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "Per feature range ``(data_max_ - data_min_)`` seen in the data  .. versionadded:: 0.17 *data_range_* instead of deprecated *data_range*.  See also -------- minmax_scale: Equivalent function without the object oriented API.",
          "name": "data_range_",
          "shape": "n_features,",
          "type": "ndarray"
        }
      ],
      "category": "preprocessing.data",
      "common_name": "Min Max Scaler",
      "description": "'Transforms features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, i.e. between\nzero and one.\n\nThe transformation is given by::\n\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "id": "sklearn.preprocessing.data.MinMaxScaler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the minimum and maximum to be used for later scaling.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The data used to compute the per-feature minimum and maximum used for later scaling along the features axis. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Undo the scaling of X according to feature_range.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Input data that will be transformed. It cannot be sparse. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Online computation of min and max on X for later scaling.\nAll of X is processed as a single batch. This is intended for cases\nwhen `fit` is not feasible due to very large number of `n_samples`\nor because X is read from a continuous stream.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "The data used to compute the mean and standard deviation used for later scaling along the features axis. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "'",
              "name": "y",
              "type": ""
            }
          ]
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Scaling features of X according to feature_range.\n",
          "id": "sklearn.preprocessing.data.MinMaxScaler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data that will be transformed. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.MinMaxScaler",
      "parameters": [
        {
          "description": "Desired range of transformed data. ",
          "name": "feature_range",
          "type": "tuple"
        },
        {
          "description": "Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array). ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:186",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Holds the label for each class. ",
          "name": "classes_",
          "shape": "n_class",
          "type": "array"
        },
        {
          "description": "Represents the type of the target data as evaluated by utils.multiclass.type_of_target. Possible type are 'continuous', 'continuous-multioutput', 'binary', 'multiclass', 'multiclass-multioutput', 'multilabel-indicator', and 'unknown'. ",
          "name": "y_type_",
          "type": "str"
        },
        {
          "description": "True if the input data to transform is given as a sparse matrix, False otherwise. ",
          "name": "sparse_input_",
          "type": "boolean"
        }
      ],
      "category": "preprocessing.label",
      "common_name": "Label Binarizer",
      "description": "\"Binarize labels in a one-vs-all fashion\n\nSeveral regression and binary classification algorithms are\navailable in the scikit. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nAt learning time, this simply consists in learning one regressor\nor binary classifier per class. In doing so, one needs to convert\nmulti-class labels to binary labels (belong or does not belong\nto the class). LabelBinarizer makes this process easy with the\ntransform method.\n\nAt prediction time, one assigns the class for which the corresponding\nmodel gave the greatest confidence. LabelBinarizer makes this easy\nwith the inverse_transform method.\n\nRead more in the :ref:`User Guide <preprocessing_targets>`.\n",
      "id": "sklearn.preprocessing.label.LabelBinarizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit label binarizer\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Transform binary labels back to multi-class labels\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Target values. All sparse matrices are converted to CSR before inverse transformation. ",
              "name": "Y",
              "shape": "n_samples, n_classes",
              "type": "numpy"
            },
            {
              "description": "Threshold used in the binary and multi-label cases.  Use 0 when: - Y contains the output of decision_function (classifier) Use 0.5 when: - Y contains the output of predict_proba  If None, the threshold is assumed to be half way between neg_label and pos_label. ",
              "name": "threshold",
              "type": "float"
            }
          ],
          "returns": {
            "description": " Notes ----- In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear model's decision_function method directly as the input of inverse_transform. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "numpy"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform multi-class labels to binary labels\n\nThe output of transform is sometimes referred to by some authors as the\n1-of-K coding scheme.\n",
          "id": "sklearn.preprocessing.label.LabelBinarizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "(n_samples, n_classes) Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification. Sparse matrix can be CSR, CSC, COO, DOK, or LIL. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Shape will be [n_samples, 1] for binary problems. '",
            "name": "Y",
            "shape": "n_samples, n_classes",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.preprocessing.label.LabelBinarizer",
      "parameters": [
        {
          "description": "Value with which negative labels must be encoded. ",
          "name": "neg_label",
          "type": "int"
        },
        {
          "description": "Value with which positive labels must be encoded. ",
          "name": "pos_label",
          "type": "int"
        },
        {
          "description": "True if the returned array from transform is desired to be in sparse CSR format. ",
          "name": "sparse_output",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/label.pyc:177",
      "tags": [
        "preprocessing",
        "label"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Estimators used for predictions. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "Array containing labels.",
          "name": "classes_",
          "shape": "n_classes",
          "type": "numpy"
        }
      ],
      "category": "multiclass",
      "common_name": "One Vs One Classifier",
      "description": "\"One-vs-one multiclass strategy\n\nThis strategy consists in fitting one classifier per class pair.\nAt prediction time, the class which received the most votes is selected.\nSince it requires to fit `n_classes * (n_classes - 1) / 2` classifiers,\nthis method is usually slower than one-vs-the-rest, due to its\nO(n_classes^2) complexity. However, this method may be advantageous for\nalgorithms such as kernel algorithms which don't scale well with\n`n_samples`. This is because each individual learning problem only involves\na small subset of the data whereas, with one-vs-the-rest, the complete\ndataset is used `n_classes` times.\n\nRead more in the :ref:`User Guide <ovo_classification>`.\n",
      "id": "sklearn.multiclass.OneVsOneClassifier",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Decision function for the OneVsOneClassifier.\n\nThe decision values for the samples are computed by adding the\nnormalized sum of pair-wise classification confidence levels to the\nvotes in order to disambiguate between the decision values when the\nvotes for all the classes are equal leading to a tie.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "Y",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit underlying estimators.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-class targets. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Partially fit underlying estimators\n\nShould be used when memory is inefficient to train all data. Chunks\nof data can be passed in several iteration, where the first call\nshould have an array of all target variables.\n\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-class targets. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is only required in the first call of partial_fit and can be omitted in the subsequent calls. ",
              "name": "classes",
              "shape": "n_classes, ",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Estimate the best class label for each sample in X.\n\nThis is implemented as ``argmax(decision_function(X), axis=1)`` which\nwill return the label of the class with most votes by estimators\npredicting the outcome of a decision for each possible class pair.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Predicted multi-class targets. '",
            "name": "y",
            "shape": "n_samples",
            "type": "numpy"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multiclass.OneVsOneClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multiclass.OneVsOneClassifier",
      "parameters": [
        {
          "description": "An estimator object implementing `fit` and one of `decision_function` or `predict_proba`. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc:432",
      "tags": [
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "cluster._feature_agglomeration",
      "common_name": "Agglomeration Transform",
      "description": "'\nA class for feature agglomeration via the transform interface\n'",
      "id": "sklearn.cluster._feature_agglomeration.AgglomerationTransform",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.cluster._feature_agglomeration.AgglomerationTransform.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'\nInverse the transformation.\nReturn a vector of size nb_features with the values of Xred assigned\nto each group of features\n",
          "id": "sklearn.cluster._feature_agglomeration.AgglomerationTransform.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The values to be assigned to each cluster of samples ",
              "name": "Xred",
              "shape": "n_samples, n_clusters",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "A vector of size n_samples with the values of Xred assigned to each of the cluster of samples. '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'\nCompute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n",
          "id": "sklearn.cluster._feature_agglomeration.AgglomerationTransform.mean",
          "name": "mean",
          "parameters": [
            {
              "description": "Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.  .. versionadded: 1.7.0  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.",
              "name": "axis",
              "optional": "true",
              "type": ""
            },
            {
              "description": "Type to use in computing the mean.  For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype.",
              "name": "dtype",
              "optional": "true",
              "type": "data-type"
            },
            {
              "description": "Alternate output array in which to place the result.  The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See `doc.ufuncs` for details.",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            },
            {
              "description": "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. ",
              "name": "keepdims",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned.  See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar  Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements.  Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.mean(a) 2.5 >>> np.mean(a, axis=0) array([ 2.,  3.]) >>> np.mean(a, axis=1) array([ 1.5,  3.5])  In single precision, `mean` can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.mean(a) 0.546875  Computing the mean in float64 is more accurate:  >>> np.mean(a, dtype=np.float64) 0.55000000074505806  '",
            "name": "m",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nTransform a new matrix using the built clustering\n",
          "id": "sklearn.cluster._feature_agglomeration.AgglomerationTransform.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "A M by N array of M observations in N dimensions or a length M array of M one-dimensional observations. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The pooled values for each feature cluster. '",
            "name": "Y",
            "shape": "n_samples, n_clusters",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.cluster._feature_agglomeration.AgglomerationTransform",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/_feature_agglomeration.pyc:17",
      "tags": [
        "cluster",
        "_feature_agglomeration"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Root of the CFTree. ",
          "name": "root_",
          "type": ""
        },
        {
          "description": "Start pointer to all the leaves. ",
          "name": "dummy_leaf_",
          "type": ""
        },
        {
          "description": "Centroids of all subclusters read directly from the leaves. ",
          "name": "subcluster_centers_",
          "type": "ndarray"
        },
        {
          "description": "Labels assigned to the centroids of the subclusters after they are clustered globally. ",
          "name": "subcluster_labels_",
          "type": "ndarray"
        },
        {
          "description": "Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data. ",
          "name": "labels_",
          "shape": "n_samples,",
          "type": "ndarray"
        }
      ],
      "category": "cluster.birch",
      "common_name": "Birch",
      "description": "'Implements the Birch clustering algorithm.\n\nEvery new sample is inserted into the root of the Clustering Feature\nTree. It is then clubbed together with the subcluster that has the\ncentroid closest to the new sample. This is done recursively till it\nends up at the subcluster of the leaf of the tree has the closest centroid.\n\nRead more in the :ref:`User Guide <birch>`.\n",
      "id": "sklearn.cluster.birch.Birch",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nBuild a CF Tree for the input data.\n",
          "id": "sklearn.cluster.birch.Birch.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Input data. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.birch.Birch.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.cluster.birch.Birch.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.birch.Birch.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nOnline learning. Prevents rebuilding of CFTree from scratch.\n",
          "id": "sklearn.cluster.birch.Birch.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Input data. If X is not provided, only the global clustering step is done. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'\nPredict data using the ``centroids_`` of subclusters.\n\nAvoid computation of the row norms of X.\n",
          "id": "sklearn.cluster.birch.Birch.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Labelled data. '",
            "name": "labels: ndarray, shape(n_samples)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.birch.Birch.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'\nTransform X into subcluster centroids dimension.\n\nEach dimension represents the distance from the sample point to each\ncluster centroid.\n",
          "id": "sklearn.cluster.birch.Birch.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_trans",
            "shape": "n_samples, n_clusters",
            "type": "array-like, sparse matrix"
          }
        }
      ],
      "name": "sklearn.cluster.birch.Birch",
      "parameters": [
        {
          "description": "The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. ",
          "name": "threshold",
          "type": "float"
        },
        {
          "description": "Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then the node has to be split. The corresponding parent also has to be split and if the number of subclusters in the parent is greater than the branching factor, then it has to be split recursively. ",
          "name": "branching_factor",
          "type": "int"
        },
        {
          "description": "Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples. If None, this final clustering step is not performed and the subclusters are returned as they are. If a model is provided, the model is fit treating the subclusters as new samples and the initial data is mapped to the label of the closest subcluster. If an int is provided, the model fit is AgglomerativeClustering with n_clusters set to the int. ",
          "name": "n_clusters",
          "type": "int"
        },
        {
          "description": "Whether or not to compute labels for each fit. ",
          "name": "compute_labels",
          "type": "bool"
        },
        {
          "description": "Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten. ",
          "name": "copy",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/birch.pyc:324",
      "tags": [
        "cluster",
        "birch"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "common_name": "Memmaping Pool",
      "description": "'Process pool that shares large arrays to avoid memory copy.\n\nThis drop-in replacement for `multiprocessing.pool.Pool` makes\nit possible to work efficiently with shared memory in a numpy\ncontext.\n\nExisting instances of numpy.memmap are preserved: the child\nsuprocesses will have access to the same shared memory in the\noriginal mode except for the \\'w+\\' mode that is automatically\ntransformed as \\'r+\\' to avoid zeroing the original data upon\ninstantiation.\n\nFurthermore large arrays from the parent process are automatically\ndumped to a temporary folder on the filesystem such as child\nprocesses to access their content via memmaping (file system\nbacked shared memory).\n\nNote: it is important to call the terminate method to collect\nthe temporary folder used by the pool.\n",
      "id": "sklearn.externals.joblib.pool.MemmapingPool",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nEquivalent of `apply()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.apply",
          "name": "apply",
          "parameters": []
        },
        {
          "description": "'\nAsynchronous equivalent of `apply()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.apply_async",
          "name": "apply_async",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.close",
          "name": "close",
          "parameters": []
        },
        {
          "description": "'\nEquivalent of `itertools.imap()` -- can be MUCH slower than `Pool.map()`\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.imap",
          "name": "imap",
          "parameters": []
        },
        {
          "description": "'\nLike `imap()` method but ordering of results is arbitrary\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.imap_unordered",
          "name": "imap_unordered",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.join",
          "name": "join",
          "parameters": []
        },
        {
          "description": "'\nEquivalent of `map()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.map",
          "name": "map",
          "parameters": []
        },
        {
          "description": "'\nAsynchronous equivalent of `map()` builtin\n'",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.map_async",
          "name": "map_async",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.pool.MemmapingPool.terminate",
          "name": "terminate",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.pool.MemmapingPool",
      "parameters": [
        {
          "description": "Number of worker processes running concurrently in the pool. initializer: callable, optional Callable executed on worker process creation. initargs: tuple, optional Arguments passed to the initializer callable. temp_folder: str, optional Folder to be used by the pool for memmaping large arrays for sharing memory with worker processes. If None, this will try in order: - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable, - /dev/shm if the folder exists and is writable: this is a RAMdisk filesystem available by default on modern Linux distributions, - the default system temporary folder that can be overridden with TMP, TMPDIR or TEMP environment variables, typically /tmp under Unix operating systems. max_nbytes int or None, optional, 1e6 by default Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Use None to disable memmaping of large arrays. mmap_mode: {\\'r+\\', \\'r\\', \\'w+\\', \\'c\\'} Memmapping mode for numpy arrays passed to workers. See \\'max_nbytes\\' parameter documentation for more details. forward_reducers: dictionary, optional Reducers used to pickle objects passed from master to worker processes: see below. backward_reducers: dictionary, optional Reducers used to pickle return values from workers back to the master process. verbose: int, optional Make it possible to monitor how the communication of numpy arrays with the subprocess is handled (pickling or memmaping) prewarm: bool or str, optional, \"auto\" by default. If True, force a read on newly memmaped array to make sure that OS pre- cache it in memory. This can be useful to avoid concurrent disk access when the same data array is passed to different worker processes. If \"auto\" (by default), prewarm is set to True, unless the Linux shared memory partition /dev/shm is available and used as temp_folder.  `forward_reducers` and `backward_reducers` are expected to be dictionaries with key/values being `(type, callable)` pairs where `callable` is a function that give an instance of `type` will return a tuple `(constructor, tuple_of_objects)` to rebuild an instance out of the pickled `tuple_of_objects` as would return a `__reduce__` method. See the standard library documentation on pickling for more details.  '",
          "name": "processes",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/pool.pyc:441",
      "tags": [
        "externals",
        "joblib",
        "pool"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. (stored only if store_precision is True)  `shrinkage` : float, 0 <= shrinkage <= 1 Coefficient in the convex combination used for the computation of the shrunk estimate. ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        }
      ],
      "category": "covariance.shrunk_covariance_",
      "common_name": "Shrunk Covariance",
      "description": "'Covariance estimator with shrinkage\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n",
      "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "' Fits the shrunk covariance model\naccording to the given training data and parameters.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.shrunk_covariance_.ShrunkCovariance",
      "parameters": [
        {
          "description": "Specify if the estimated precision is stored ",
          "name": "store_precision",
          "type": "boolean"
        },
        {
          "description": "Coefficient in the convex combination used for the computation of the shrunk estimate. ",
          "name": "shrinkage",
          "type": "float"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/shrunk_covariance_.pyc:66",
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The class labels.  calibrated_classifiers_: list (len() equal to cv or 1 if cv == \"prefit\") The list of calibrated classifiers, one for each crossvalidation fold, which has been fitted on all but the validation fold and calibrated on the validation fold. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        }
      ],
      "category": "calibration",
      "common_name": "Calibrated Classifier CV",
      "description": "'Probability calibration with isotonic regression or sigmoid.\n\nWith this class, the base_estimator is fit on the train set of the\ncross-validation generator and the test set is used for calibration.\nThe probabilities for each of the folds are then averaged\nfor prediction. In case that cv=\"prefit\" is passed to __init__,\nit is assumed that base_estimator has been fitted already and all\ndata is used for calibration. Note that data for fitting the\nclassifier and for calibrating it must be disjoint.\n\nRead more in the :ref:`User Guide <calibration>`.\n",
      "id": "sklearn.calibration.CalibratedClassifierCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the calibrated model\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the target of new samples. Can be different from the\nprediction of the uncalibrated classifier.\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted class. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Posterior probabilities of classification\n\nThis function returns posterior probabilities of classification\naccording to each class on an array of test vectors X.\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted probas. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.calibration.CalibratedClassifierCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.calibration.CalibratedClassifierCV",
      "parameters": [
        {
          "description": "The classifier whose output decision function needs to be calibrated to offer more accurate predict_proba outputs. If cv=prefit, the classifier must have been fit already on data. ",
          "name": "base_estimator",
          "type": "instance"
        },
        {
          "description": "The method to use for calibration. Can be \\'sigmoid\\' which corresponds to Platt\\'s method or \\'isotonic\\' which is a non-parametric approach. It is not advised to use isotonic calibration with too few calibration samples ``(<<1000)`` since it tends to overfit. Use sigmoids (Platt\\'s calibration) in this case. ",
          "name": "method",
          "type": ""
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. If ``y`` is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  If \"prefit\" is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration. ",
          "name": "cv",
          "optional": "true",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/calibration.pyc:30",
      "tags": [
        "calibration"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Components with maximum variance. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Percentage of variance explained by each of the selected components. k is not set then all components are stored and the sum of explained variances is equal to 1.0 ",
          "name": "explained_variance_ratio_",
          "type": "array"
        },
        {
          "description": "Per-feature empirical mean, estimated from the training set. ",
          "name": "mean_",
          "type": "array"
        }
      ],
      "category": "decomposition.pca",
      "common_name": "Randomized PCA",
      "description": "\"Principal component analysis (PCA) using randomized SVD\n\n.. deprecated:: 0.18\nThis class will be removed in 0.20.\nUse :class:`PCA` with parameter svd_solver 'randomized' instead.\nThe new implementation DOES NOT store whiten ``components_``.\nApply transform to get them.\n\nLinear dimensionality reduction using approximated Singular Value\nDecomposition of the data and keeping only the most significant\nsingular vectors to project the data to a lower dimensional space.\n\nRead more in the :ref:`User Guide <RandomizedPCA>`.\n",
      "id": "sklearn.decomposition.pca.RandomizedPCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model with X by extracting the first principal components.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit the model with X and apply the dimensionality reduction on X.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "New data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform data back to its original space.\n\nReturns an array X_original whose transform would be X.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "New data, where n_samples in the number of samples and n_components is the number of components. ",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- If whitening is enabled, inverse_transform does not compute the exact inverse operation of transform. '",
            "name": "X_original array-like, shape (n_samples, n_features)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply dimensionality reduction on X.\n\nX is projected on the first principal components previous extracted\nfrom a training set.\n",
          "id": "sklearn.decomposition.pca.RandomizedPCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.decomposition.pca.RandomizedPCA",
      "parameters": [
        {
          "description": "Maximum number of components to keep. When not given or None, this is set to n_features (the second dimension of the training data). ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "Number of iterations for the power method.  .. versionchanged:: 0.18 ",
          "name": "iterated_power",
          "type": "int"
        },
        {
          "description": "When True (False by default) the `components_` vectors are multiplied by the square root of (n_samples) and divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.  Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. ",
          "name": "whiten",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Pseudo Random Number generator seed control. If None, use the numpy.random singleton. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/pca.pyc:537",
      "tags": [
        "decomposition",
        "pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "Percentage of variance explained by each of the selected components. ",
          "name": "explained_variance_ratio_",
          "type": "array"
        },
        {
          "description": "The variance of the training samples transformed by a projection to each component. ",
          "name": "explained_variance_",
          "type": "array"
        }
      ],
      "category": "decomposition.truncated_svd",
      "common_name": "Truncated SVD",
      "description": "'Dimensionality reduction using truncated SVD (aka LSA).\n\nThis transformer performs linear dimensionality reduction by means of\ntruncated singular value decomposition (SVD). Contrary to PCA, this\nestimator does not center the data before computing the singular value\ndecomposition. This means it can work with scipy.sparse matrices\nefficiently.\n\nIn particular, truncated SVD works on term count/tf-idf matrices as\nreturned by the vectorizers in sklearn.feature_extraction.text. In that\ncontext, it is known as latent semantic analysis (LSA).\n\nThis estimator supports two algorithms: a fast randomized SVD solver, and\na \"naive\" algorithm that uses ARPACK as an eigensolver on (X * X.T) or\n(X.T * X), whichever is more efficient.\n\nRead more in the :ref:`User Guide <LSA>`.\n",
      "id": "sklearn.decomposition.truncated_svd.TruncatedSVD",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit LSI model on training data X.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns the transformer object. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit LSI model to X and perform dimensionality reduction on X.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Reduced version of X. This will always be a dense array. '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform X back to its original space.\n\nReturns an array X_original whose transform would be X.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "New data. ",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Note that this is always a dense array. '",
            "name": "X_original",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Perform dimensionality reduction on X.\n",
          "id": "sklearn.decomposition.truncated_svd.TruncatedSVD.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Reduced version of X. This will always be a dense array. '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.decomposition.truncated_svd.TruncatedSVD",
      "parameters": [
        {
          "description": "Desired dimensionality of output data. Must be strictly less than the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or \"randomized\" for the randomized algorithm due to Halko (2009). ",
          "name": "algorithm",
          "type": "string"
        },
        {
          "description": "Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in `randomized_svd` to handle sparse matrices that may have large slowly decaying spectrum. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "(Seed for) pseudo-random number generator. If not given, the numpy.random singleton is used. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/truncated_svd.pyc:25",
      "tags": [
        "decomposition",
        "truncated_svd"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Covariance matrices of each class. ",
          "name": "covariances_",
          "shape": "n_features, n_features",
          "type": "list"
        },
        {
          "description": "Class means. ",
          "name": "means_",
          "shape": "n_classes, n_features",
          "type": "array-like"
        },
        {
          "description": "Class priors (sum to 1). ",
          "name": "priors_",
          "shape": "n_classes",
          "type": "array-like"
        },
        {
          "description": "For each class k an array of shape [n_features, n_k], with ``n_k = min(n_features, number of elements in class k)`` It is the rotation of the Gaussian distribution, i.e. its principal axis. ",
          "name": "rotations_",
          "type": "list"
        },
        {
          "description": "For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system. ",
          "name": "scalings_",
          "type": "list"
        },
        {
          "description": "If True the covariance matrices are computed and stored in the `self.covariances_` attribute.  .. versionadded:: 0.17 ",
          "name": "store_covariances",
          "type": "boolean"
        },
        {
          "description": "Threshold used for rank estimation.  .. versionadded:: 0.17 ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        }
      ],
      "category": "discriminant_analysis",
      "common_name": "Quadratic Discriminant Analysis",
      "description": "\"\nQuadratic Discriminant Analysis\n\nA classifier with a quadratic decision boundary, generated\nby fitting class conditional densities to the data\nand using Bayes' rule.\n\nThe model fits a Gaussian density to each class.\n\n.. versionadded:: 0.17\n*QuadraticDiscriminantAnalysis*\n\nRead more in the :ref:`User Guide <lda_qda>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply decision function to an array of samples.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Array of samples (test vectors). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Decision function values related to each class, per sample. In the two-class case, the shape is [n_samples,], giving the log likelihood ratio of the positive class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model according to the given training data and parameters.\n\n.. versionchanged:: 0.17\nDeprecated *store_covariance* have been moved to main constructor.\n\n.. versionchanged:: 0.17\nDeprecated *tol* have been moved to main constructor.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers) '",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on an array of test vectors X.\n\nThe predicted class C for each sample in X is returned.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Return posterior probabilities of classification.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "Array of samples/test vectors. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Posterior log-probabilities of classification per class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Return posterior probabilities of classification.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Array of samples/test vectors. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Posterior probabilities of classification per class. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "description": "Priors on classes ",
          "name": "priors",
          "optional": "true",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Regularizes the covariance estimate as ``(1-reg_param)*Sigma + reg_param*np.eye(n_features)`` ",
          "name": "reg_param",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.pyc:549",
      "tags": [
        "discriminant_analysis"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "linear_model.stochastic_gradient",
      "common_name": "Base SGD Regressor",
      "description": "None",
      "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nPredict using the linear model\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_features,",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "1,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.stochastic_gradient.BaseSGDRegressor",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc:835",
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features. ",
          "name": "coef_",
          "shape": "n_features, ",
          "type": "array"
        },
        {
          "description": "Sum of residuals. Squared Euclidean 2-norm for each target passed during the fit. If the linear regression problem is under-determined (the number of linearly independent rows of the training matrix is less than its number of linearly independent columns), this is an empty array. If the target vector passed during the fit is 1-dimensional, this is a (1,) shape array.  .. versionadded:: 0.18 ",
          "name": "residues_",
          "shape": "n_targets,",
          "type": "array"
        },
        {
          "description": "Independent term in the linear model. ",
          "name": "intercept_",
          "type": "array"
        }
      ],
      "category": "linear_model.base",
      "common_name": "Linear Regression",
      "description": "'\nOrdinary least squares Linear Regression.\n",
      "id": "sklearn.linear_model.base.LinearRegression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.base.LinearRegression.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'\nFit linear model.\n",
          "id": "sklearn.linear_model.base.LinearRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples,n_features",
              "type": "numpy"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples, n_targets",
              "type": "numpy"
            },
            {
              "description": "Individual weights for each sample  .. versionadded:: 0.17 parameter *sample_weight* support to LinearRegression. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.base.LinearRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.base.LinearRegression.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.base.LinearRegression.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.base.LinearRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.base.LinearRegression",
      "parameters": [
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. This will only provide speedup for n_targets > 1 and sufficient large problems. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/base.pyc:417",
      "tags": [
        "linear_model",
        "base"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Coordinates of cluster centers.  labels_ : Labels of each point. ",
          "name": "cluster_centers_",
          "type": "array"
        }
      ],
      "category": "cluster.mean_shift_",
      "common_name": "Mean Shift",
      "description": "'Mean shift clustering using a flat kernel.\n\nMean shift clustering aims to discover \"blobs\" in a smooth density of\nsamples. It is a centroid-based algorithm, which works by updating\ncandidates for centroids to be the mean of the points within a given\nregion. These candidates are then filtered in a post-processing stage to\neliminate near-duplicates to form the final set of centroids.\n\nSeeding is performed using a binning technique for scalability.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n",
      "id": "sklearn.cluster.mean_shift_.MeanShift",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Perform clustering.\n",
          "id": "sklearn.cluster.mean_shift_.MeanShift.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Samples to cluster. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.mean_shift_.MeanShift.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.mean_shift_.MeanShift.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the closest cluster each sample in X belongs to.\n",
          "id": "sklearn.cluster.mean_shift_.MeanShift.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "New data to predict. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Index of the cluster each sample belongs to. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.mean_shift_.MeanShift.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.mean_shift_.MeanShift",
      "parameters": [
        {
          "description": "Bandwidth used in the RBF kernel.  If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below). ",
          "name": "bandwidth",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters. ",
          "name": "seeds",
          "optional": "true",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None. ",
          "name": "bin_seeding",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1. ",
          "name": "min_bin_freq",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. ",
          "name": "cluster_all",
          "type": "boolean"
        },
        {
          "description": "The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/mean_shift_.pyc:282",
      "tags": [
        "cluster",
        "mean_shift_"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "naive_bayes",
      "common_name": "Base Discrete NB",
      "description": "'Abstract base class for naive Bayes on discrete/categorical data\n\nAny estimator based on this class should provide:\n\n__init__\n_joint_log_likelihood(X) as per BaseNB\n'",
      "id": "sklearn.naive_bayes.BaseDiscreteNB",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Naive Bayes classifier according to X, y\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls. ",
              "name": "classes",
              "optional": "true",
              "shape": "n_classes",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'\nPerform classification on an array of test vectors X.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log-probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.naive_bayes.BaseDiscreteNB.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.naive_bayes.BaseDiscreteNB",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc:437",
      "tags": [
        "naive_bayes"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "Biases of the hidden units. ",
          "name": "intercept_hidden_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "Biases of the visible units. ",
          "name": "intercept_visible_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units. ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": "array-like"
        }
      ],
      "category": "neural_network.rbm",
      "common_name": "Bernoulli RBM",
      "description": "'Bernoulli Restricted Boltzmann Machine (RBM).\n\nA Restricted Boltzmann Machine with binary visible units and\nbinary hidden units. Parameters are estimated using Stochastic Maximum\nLikelihood (SML), also known as Persistent Contrastive Divergence (PCD)\n[2].\n\nThe time complexity of this implementation is ``O(d ** 2)`` assuming\nd ~ n_features ~ n_components.\n\nRead more in the :ref:`User Guide <rbm>`.\n",
      "id": "sklearn.neural_network.rbm.BernoulliRBM",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model to the data X.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The fitted model. '",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform one Gibbs sampling step.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.gibbs",
          "name": "gibbs",
          "parameters": [
            {
              "description": "Values of the visible layer to start from. ",
              "name": "v",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Values of the visible layer after one Gibbs step. '",
            "name": "v_new",
            "shape": "n_samples, n_features",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the model to the data X which should contain a partial\nsegment of the data.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The fitted model. '",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Compute the pseudo-likelihood of X.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "Values of the visible layer. Must be all-boolean (not checked). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Value of the pseudo-likelihood (proxy for likelihood).  Notes ----- This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference. '",
            "name": "pseudo_likelihood",
            "shape": "n_samples,",
            "type": "array-like"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Compute the hidden layer activation probabilities, P(h=1|v=X).\n",
          "id": "sklearn.neural_network.rbm.BernoulliRBM.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data to be transformed. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Latent representations of the data. '",
            "name": "h",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.neural_network.rbm.BernoulliRBM",
      "parameters": [
        {
          "description": "Number of binary hidden units. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The learning rate for weight updates. It is *highly* recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of examples per minibatch. ",
          "name": "batch_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of iterations/sweeps over the training dataset to perform during training. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The verbosity level. The default, zero, means silent mode. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/rbm.pyc:28",
      "tags": [
        "neural_network",
        "rbm"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Class labels for each output. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Number of label for each output. ",
          "name": "n_classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Probability of each class for each output. ",
          "name": "class_prior_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Number of outputs.  outputs_2d_ : bool, True if the output at fit is 2d, else false. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input y is passed in sparse format. ",
          "name": "sparse_output_",
          "type": "bool"
        }
      ],
      "category": "dummy",
      "common_name": "Dummy Classifier",
      "description": "'\nDummyClassifier is a classifier that makes predictions using simple rules.\n\nThis classifier is useful as a simple baseline to compare with other\n(real) classifiers. Do not use it for real problems.\n\nRead more in the :ref:`User Guide <dummy_estimators>`.\n",
      "id": "sklearn.dummy.DummyClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the random classifier.\n",
          "id": "sklearn.dummy.DummyClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.dummy.DummyClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on test vectors X.\n",
          "id": "sklearn.dummy.DummyClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Input vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values for X. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log probability estimates for the test vectors X.\n",
          "id": "sklearn.dummy.DummyClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "Input vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns the log probability of the sample for each class in the model, where classes are ordered arithmetically for each output. '",
            "name": "P",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vectors X.\n",
          "id": "sklearn.dummy.DummyClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Input vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each class in the model, where classes are ordered arithmetically, for each output. '",
            "name": "P",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.dummy.DummyClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.dummy.DummyClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.dummy.DummyClassifier",
      "parameters": [
        {
          "description": "Strategy to use to generate predictions.  * \"stratified\": generates predictions by respecting the training set\\'s class distribution. * \"most_frequent\": always predicts the most frequent label in the training set. * \"prior\": always predicts the class that maximizes the class prior (like \"most_frequent\") and ``predict_proba`` returns the class prior. * \"uniform\": generates predictions uniformly at random. * \"constant\": always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class  .. versionadded:: 0.17 Dummy Classifier now supports prior fitting strategy using parameter *prior*. ",
          "name": "strategy",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator to use. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The explicit constant as predicted by the \"constant\" strategy. This parameter is useful only for the \"constant\" strategy. ",
          "name": "constant",
          "shape": "n_outputs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/dummy.pyc:21",
      "tags": [
        "dummy"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "The unmixing matrix. ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": ""
        },
        {
          "description": "The mixing matrix. ",
          "name": "mixing_",
          "shape": "n_features, n_components",
          "type": "array"
        },
        {
          "description": "If the algorithm is \"deflation\", n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.fastica_",
      "common_name": "Fast ICA",
      "description": "'FastICA: a fast algorithm for Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n",
      "id": "sklearn.decomposition.fastica_.FastICA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model to X.\n",
          "id": "sklearn.decomposition.fastica_.FastICA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Fit the model and recover the sources from X.\n",
          "id": "sklearn.decomposition.fastica_.FastICA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.fastica_.FastICA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform the sources back to the mixed data (apply mixing matrix).\n",
          "id": "sklearn.decomposition.fastica_.FastICA.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Sources, where n_samples is the number of samples and n_components is the number of components.",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            },
            {
              "description": "If False, data passed to fit are overwritten. Defaults to True. ",
              "name": "copy",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new",
            "shape": "n_samples, n_features",
            "type": "array-like"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.fastica_.FastICA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Recover the sources from X (apply the unmixing matrix).\n",
          "id": "sklearn.decomposition.fastica_.FastICA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Data to transform, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "If False, data passed to fit are overwritten. Defaults to True. ",
              "name": "copy",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.decomposition.fastica_.FastICA",
      "parameters": [
        {
          "description": "Number of components to use. If none is passed, all are used. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Apply parallel or deflational algorithm for FastICA. ",
          "name": "algorithm",
          "type": "\\'parallel\\', \\'deflation\\'"
        },
        {
          "description": "If whiten is false, the data is already considered to be whitened, and no whitening is performed. ",
          "name": "whiten",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The functional form of the G function used in the approximation to neg-entropy. Could be either \\'logcosh\\', \\'exp\\', or \\'cube\\'. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:  def my_g(x): return x ** 3, 3 * x ** 2 ",
          "name": "fun",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Arguments to send to the functional form. If empty and if fun=\\'logcosh\\', fun_args will take value {\\'alpha\\' : 1.0}. ",
          "name": "fun_args",
          "optional": "true",
          "type": "dictionary"
        },
        {
          "description": "Maximum number of iterations during fit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Tolerance on update at each iteration. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The mixing matrix to be used to initialize the algorithm. ",
          "name": "w_init",
          "type": ""
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/fastica_.pyc:376",
      "tags": [
        "decomposition",
        "fastica_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Eigenvalues of the centered kernel matrix in decreasing order. If `n_components` and `remove_zero_eig` are not set, then all values are stored. ",
          "name": "lambdas_",
          "type": "array"
        },
        {
          "description": "Eigenvectors of the centered kernel matrix. If `n_components` and `remove_zero_eig` are not set, then all components are stored. ",
          "name": "alphas_",
          "type": "array"
        },
        {
          "description": "Inverse transform matrix. Set if `fit_inverse_transform` is True.  X_transformed_fit_ : array, (n_samples, n_components) Projection of the fitted data on the kernel principal components.  X_fit_ : (n_samples, n_features) The data used to fit the model. If `copy_X=False`, then `X_fit_` is a reference. This attribute is used for the calls to transform. ",
          "name": "dual_coef_",
          "type": "array"
        }
      ],
      "category": "decomposition.kernel_pca",
      "common_name": "Kernel PCA",
      "description": "'Kernel Principal component analysis (KPCA)\n\nNon-linear dimensionality reduction through the use of kernels (see\n:ref:`metrics`).\n\nRead more in the :ref:`User Guide <kernel_PCA>`.\n",
      "id": "sklearn.decomposition.kernel_pca.KernelPCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit the model from data in X and transform X.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform X back to original space.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " References ---------- \"Learning to Find Pre-Images\", G BakIr et al, 2004. '",
            "name": "X_new: array-like, shape (n_samples, n_features)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform X.\n",
          "id": "sklearn.decomposition.kernel_pca.KernelPCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.decomposition.kernel_pca.KernelPCA",
      "parameters": [
        {
          "description": "Number of components. If None, all non-zero components are kept. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Kernel. Default=\"linear\". ",
          "name": "kernel",
          "type": ""
        },
        {
          "description": "Degree for poly kernels. Ignored by other kernels. ",
          "name": "degree",
          "type": "int"
        },
        {
          "description": "Kernel coefficient for rbf and poly kernels. Ignored by other kernels.  coef0 : float, default=1 Independent term in poly and sigmoid kernels. Ignored by other kernels. ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. ",
          "name": "kernel_params",
          "type": "mapping"
        },
        {
          "description": "Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True). ",
          "name": "alpha",
          "type": "int"
        },
        {
          "description": "Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point) ",
          "name": "fit_inverse_transform",
          "type": "bool"
        },
        {
          "description": "Select eigensolver to use. If n_components is much less than the number of training samples, arpack may be more efficient than the dense eigensolver. ",
          "name": "eigen_solver",
          "type": "string"
        },
        {
          "description": "Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless. ",
          "name": "remove_zero_eig",
          "type": "boolean"
        },
        {
          "description": "A pseudo random number generator used for the initialization of the residuals when eigen_solver == \\'arpack\\'.  .. versionadded:: 0.18 ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The number of parallel jobs to run. If `-1`, then the number of jobs is set to the number of CPU cores.  .. versionadded:: 0.18  copy_X : boolean, default=True If True, input X is copied and stored by the model in the `X_fit_` attribute. If no further changes will be done to X, setting `copy_X=False` saves memory by storing a reference.  .. versionadded:: 0.18 ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/kernel_pca.pyc:18",
      "tags": [
        "decomposition",
        "kernel_pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Minimum value of input array `X_` for left bound.  X_max_ : float Maximum value of input array `X_` for right bound. ",
          "name": "X_min_",
          "type": "float"
        },
        {
          "description": "The stepwise interpolating function that covers the domain `X_`. ",
          "name": "f_",
          "type": "function"
        }
      ],
      "category": "isotonic",
      "common_name": "Isotonic Regression",
      "description": "'Isotonic regression model.\n\nThe isotonic regression optimization problem is defined by::\n\nmin sum w_i (y[i] - y_[i]) ** 2\n\nsubject to y_[i] <= y_[j] whenever X[i] <= X[j]\nand min(y_) = y_min, max(y_) = y_max\n\nwhere:\n- ``y[i]`` are inputs (real numbers)\n- ``y_[i]`` are fitted\n- ``X`` specifies the order.\nIf ``X`` is non-decreasing then ``y_`` is non-decreasing.\n- ``w[i]`` are optional strictly positive weights (default to 1.0)\n\nRead more in the :ref:`User Guide <isotonic>`.\n",
      "id": "sklearn.isotonic.IsotonicRegression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.isotonic.IsotonicRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Training target. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Weights. If set to None, all weights will be set to 1 (equal weights). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns an instance of self.  Notes ----- X is stored for future use, as `transform` needs X to interpolate new input data. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.isotonic.IsotonicRegression.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.isotonic.IsotonicRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict new data by linear interpolation.\n",
          "id": "sklearn.isotonic.IsotonicRegression.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data to transform. ",
              "name": "T",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "T_",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.isotonic.IsotonicRegression.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.isotonic.IsotonicRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform new data by linear interpolation\n",
          "id": "sklearn.isotonic.IsotonicRegression.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Data to transform. ",
              "name": "T",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The transformed data '",
            "name": "T_",
            "shape": "n_samples,",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.isotonic.IsotonicRegression",
      "parameters": [
        {
          "description": "If not None, set the lowest value of the fit to y_min. ",
          "name": "y_min",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If not None, set the highest value of the fit to y_max. ",
          "name": "y_max",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If boolean, whether or not to fit the isotonic regression with y increasing or decreasing.  The string value \"auto\" determines whether y should increase or decrease based on the Spearman correlation estimate\\'s sign. ",
          "name": "increasing",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The ``out_of_bounds`` parameter handles how x-values outside of the training domain are handled.  When set to \"nan\", predicted y-values will be NaN.  When set to \"clip\", predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to \"raise\", allow ``interp1d`` to throw ValueError.  ",
          "name": "out_of_bounds",
          "optional": "true",
          "type": "string"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/isotonic.pyc:141",
      "tags": [
        "isotonic"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Results of the clustering. `rows[i, r]` is True if cluster `i` contains row `r`. Available only after calling ``fit``. ",
          "name": "rows_",
          "shape": "n_row_clusters, n_rows",
          "type": "array-like"
        },
        {
          "description": "Results of the clustering, like `rows`. ",
          "name": "columns_",
          "shape": "n_column_clusters, n_columns",
          "type": "array-like"
        },
        {
          "description": "The bicluster label of each row. ",
          "name": "row_labels_",
          "shape": "n_rows,",
          "type": "array-like"
        },
        {
          "description": "The bicluster label of each column. ",
          "name": "column_labels_",
          "shape": "n_cols,",
          "type": "array-like"
        }
      ],
      "category": "cluster.bicluster",
      "common_name": "Spectral Coclustering",
      "description": "\"Spectral Co-Clustering algorithm (Dhillon, 2001).\n\nClusters rows and columns of an array `X` to solve the relaxed\nnormalized cut of the bipartite graph created from `X` as follows:\nthe edge between row vertex `i` and column vertex `j` has weight\n`X[i, j]`.\n\nThe resulting bicluster structure is block-diagonal, since each\nrow and each column belongs to exactly one bicluster.\n\nSupports sparse matrices, as long as they are nonnegative.\n\nRead more in the :ref:`User Guide <spectral_coclustering>`.\n",
      "id": "sklearn.cluster.bicluster.SpectralCoclustering",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Creates a biclustering for X.\n",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.fit",
          "name": "fit",
          "parameters": [
            {
              "description": " '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "\"Row and column indices of the i'th bicluster.\n\nOnly works if ``rows_`` and ``columns_`` attributes exist.\n",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.get_indices",
          "name": "get_indices",
          "parameters": [],
          "returns": {
            "description": "Indices of rows in the dataset that belong to the bicluster. col_ind : np.array, dtype=np.intp Indices of columns in the dataset that belong to the bicluster.  \"",
            "name": "row_ind",
            "type": "np"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Shape of the i'th bicluster.\n",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.get_shape",
          "name": "get_shape",
          "parameters": [],
          "returns": {
            "description": "Number of rows and columns (resp.) in the bicluster. \"",
            "name": "shape",
            "type": ""
          }
        },
        {
          "description": "'Returns the submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if ``rows_`` and\n``columns_`` attributes exist.\n\n'",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.get_submatrix",
          "name": "get_submatrix",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.bicluster.SpectralCoclustering.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.bicluster.SpectralCoclustering",
      "parameters": [
        {
          "description": "The number of biclusters to find. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Selects the algorithm for finding singular vectors. May be 'randomized' or 'arpack'. If 'randomized', use :func:`sklearn.utils.extmath.randomized_svd`, which may be faster for large matrices. If 'arpack', use :func:`sklearn.utils.arpack.svds`, which is more accurate, but possibly slower in some cases. ",
          "name": "svd_method",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of vectors to use in calculating the SVD. Corresponds to `ncv` when `svd_method=arpack` and `n_oversamples` when `svd_method` is 'randomized`. ",
          "name": "n_svd_vecs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to use mini-batch k-means, which is faster but may get different results. ",
          "name": "mini_batch",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Method for initialization of k-means algorithm; defaults to 'k-means++'. ",
          "name": "init",
          "type": "'k-means++', 'random' or an ndarray"
        },
        {
          "description": "Number of random initializations that are tried with the k-means algorithm.  If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A pseudo random number generator used by the K-Means initialization. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/bicluster.pyc:180",
      "tags": [
        "cluster",
        "bicluster"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Concrete number of components computed when n_components=\"auto\". ",
          "name": "n_component_",
          "type": "int"
        },
        {
          "description": "Random matrix used for the projection. ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": ""
        },
        {
          "description": "Concrete density computed from when density = \"auto\".  See Also -------- GaussianRandomProjection ",
          "name": "density_",
          "type": "float"
        }
      ],
      "category": "random_projection",
      "common_name": "Sparse Random Projection",
      "description": "'Reduce dimensionality through sparse random projection\n\nSparse random matrix is an alternative to dense random\nprojection matrix that guarantees similar embedding quality while being\nmuch more memory efficient and allowing faster computation of the\nprojected data.\n\nIf we note `s = 1 / density` the components of the random matrix are\ndrawn from:\n\n- -sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n-  0                              with probability 1 - 1 / s\n- +sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n\nRead more in the :ref:`User Guide <sparse_random_matrix>`.\n",
      "id": "sklearn.random_projection.SparseRandomProjection",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Generate a sparse random projection matrix\n",
          "id": "sklearn.random_projection.SparseRandomProjection.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": " '",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.random_projection.SparseRandomProjection.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.random_projection.SparseRandomProjection.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.random_projection.SparseRandomProjection.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Project the data by using matrix product with the random matrix\n",
          "id": "sklearn.random_projection.SparseRandomProjection.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input data to project into a smaller dimensional space. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "",
              "name": "y",
              "type": "is"
            }
          ],
          "returns": {
            "description": "Projected array.  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "numpy"
          }
        }
      ],
      "name": "sklearn.random_projection.SparseRandomProjection",
      "parameters": [
        {
          "description": "Dimensionality of the target projection space.  n_components can be automatically adjusted according to the number of samples in the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the quality of the embedding is controlled by the ``eps`` parameter.  It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Ratio of non-zero component in the random projection matrix.  If density = \\'auto\\', the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).  Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001. ",
          "name": "density",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to \\'auto\\'.  Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space. ",
          "name": "eps",
          "optional": "true",
          "type": "strictly"
        },
        {
          "default": "False",
          "description": "If True, ensure that the output of the random projection is a dense numpy array even if the input and random projection matrix are both sparse. In practice, if the number of components is small the number of zero components in the projected data will be very small and it will be more CPU and memory efficient to use a dense representation.  If False, the projected data uses a sparse representation if the input is sparse. ",
          "name": "dense_output",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Control the pseudo random number generator used to generate the matrix at fit time. ",
          "name": "random_state",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/random_projection.pyc:501",
      "tags": [
        "random_projection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.hinge_loss",
      "description": "\"Average hinge loss (non-regularized)\n\nIn binary class case, assuming labels in y_true are encoded with +1 and -1,\nwhen a prediction mistake is made, ``margin = y_true * pred_decision`` is\nalways negative (since the signs disagree), implying ``1 - margin`` is\nalways greater than 1.  The cumulated hinge loss is therefore an upper\nbound of the number of mistakes made by the classifier.\n\nIn multiclass case, the function expects that either all the labels are\nincluded in y_true or an optional labels argument is provided which\ncontains all the labels. The multilabel margin is calculated according\nto Crammer-Singer's method. As in the binary case, the cumulated hinge loss\nis an upper bound of the number of mistakes made by the classifier.\n\nRead more in the :ref:`User Guide <hinge_loss>`.\n",
      "id": "sklearn.metrics.classification.hinge_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.hinge_loss",
      "parameters": [
        {
          "description": "True target, consisting of integers of two values. The positive label must be greater than the negative label. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Predicted decisions, as output by decision_function (floats). ",
          "name": "pred_decision",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Contains all the labels for the problem. Used in multiclass hinge loss. ",
          "name": "labels",
          "optional": "true",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- .. [1] `Wikipedia entry on the Hinge loss <https://en.wikipedia.org/wiki/Hinge_loss>`_  .. [2] Koby Crammer, Yoram Singer. On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines. Journal of Machine Learning Research 2, (2001), 265-292  .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models by Robert C. Moore, John DeNero. <http://www.ttic.edu/sigml/symposium2011/papers/ Moore+DeNero_Regularization.pdf>`_  Examples -------- >>> from sklearn import svm >>> from sklearn.metrics import hinge_loss >>> X = [[0], [1]] >>> y = [-1, 1] >>> est = svm.LinearSVC(random_state=0) >>> est.fit(X, y) LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=0, tol=0.0001, verbose=0) >>> pred_decision = est.decision_function([[-2], [3], [0.5]]) >>> pred_decision  # doctest: +ELLIPSIS array([-2.18...,  2.36...,  0.09...]) >>> hinge_loss([-1, 1, 1], pred_decision)  # doctest: +ELLIPSIS 0.30...  In the multiclass case:  >>> X = np.array([[0], [1], [2], [3]]) >>> Y = np.array([0, 1, 2, 3]) >>> labels = np.array([0, 1, 2, 3]) >>> est = svm.LinearSVC() >>> est.fit(X, Y) LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True, intercept_scaling=1, loss='squared_hinge', max_iter=1000, multi_class='ovr', penalty='l2', random_state=None, tol=0.0001, verbose=0) >>> pred_decision = est.decision_function([[-1], [2], [3]]) >>> y_true = [0, 2, 3] >>> hinge_loss(y_true, pred_decision, labels)  #doctest: +ELLIPSIS 0.56... \"",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._validation.cross_val_predict",
      "description": "\"Generate cross-validated estimates for each input data point\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._validation.cross_val_predict",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._validation.cross_val_predict",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like The data to fit. Can be, for example a list, or an array at least 2d. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
          "name": "groups",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Parameters to pass to the fit method of the estimator. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Invokes the passed method name of the passed estimator. ",
          "name": "method",
          "optional": "true",
          "type": "string"
        }
      ],
      "returns": {
        "description": "This is the result of calling ``method``  Examples -------- >>> from sklearn import datasets, linear_model >>> from sklearn.model_selection import cross_val_predict >>> diabetes = datasets.load_diabetes() >>> X = diabetes.data[:150] >>> y = diabetes.target[:150] >>> lasso = linear_model.Lasso() >>> y_pred = cross_val_predict(lasso, X, y) \"",
        "name": "predictions",
        "type": "ndarray"
      },
      "tags": [
        "model_selection",
        "_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.tree.export.export_graphviz",
      "description": "\"Export a decision tree in DOT format.\n\nThis function generates a GraphViz representation of the decision tree,\nwhich is then written into `out_file`. Once exported, graphical renderings\ncan be generated using, for example::\n\n$ dot -Tps tree.dot -o tree.ps      (PostScript format)\n$ dot -Tpng tree.dot -o tree.png    (PNG format)\n\nThe sample counts that are shown are weighted with any sample_weights that\nmight be present.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "id": "sklearn.tree.export.export_graphviz",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.tree.export.export_graphviz",
      "parameters": [
        {
          "description": "The decision tree to be exported to GraphViz. ",
          "name": "decision_tree",
          "type": "decision"
        },
        {
          "default": "'tree.dot'",
          "description": "Handle or name of the output file. If ``None``, the result is returned as a string. This will the default from version 0.20. ",
          "name": "out_file",
          "optional": "true",
          "type": "file"
        },
        {
          "default": "None",
          "description": "The maximum depth of the representation. If None, the tree is fully generated. ",
          "name": "max_depth",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Names of each of the features. ",
          "name": "feature_names",
          "optional": "true",
          "type": "list"
        },
        {
          "default": "None",
          "description": "Names of each of the target classes in ascending numerical order. Only relevant for classification and not supported for multi-output. If ``True``, shows a symbolic representation of the class name. ",
          "name": "class_names",
          "optional": "true",
          "type": "list"
        },
        {
          "default": "'all'",
          "description": "Whether to show informative labels for impurity, etc. Options include 'all' to show at every node, 'root' to show only at the top root node, or 'none' to not show at any node. ",
          "name": "label",
          "optional": "true",
          "type": "'all', 'root', 'none'"
        },
        {
          "default": "False",
          "description": "When set to ``True``, paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output. ",
          "name": "filled",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``True``, draw all leaf nodes at the bottom of the tree. ",
          "name": "leaves_parallel",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "True",
          "description": "When set to ``True``, show the impurity at each node. ",
          "name": "impurity",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``True``, show the ID number on each node. ",
          "name": "node_ids",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``True``, change the display of 'values' and/or 'samples' to be proportions and percentages respectively. ",
          "name": "proportion",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``True``, orient tree left to right rather than top-down. ",
          "name": "rotate",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``True``, draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman. ",
          "name": "rounded",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to ``False``, ignore special characters for PostScript compatibility. ",
          "name": "special_characters",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "String representation of the input tree in GraphViz dot format. Only returned if ``out_file`` is None.  .. versionadded:: 0.18  Examples -------- >>> from sklearn.datasets import load_iris >>> from sklearn import tree  >>> clf = tree.DecisionTreeClassifier() >>> iris = load_iris()  >>> clf = clf.fit(iris.data, iris.target) >>> tree.export_graphviz(clf, ...     out_file='tree.dot')                # doctest: +SKIP \"",
        "name": "dot_data",
        "type": "string"
      },
      "tags": [
        "tree",
        "export"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._validation.cross_val_score",
      "description": "\"Evaluate a score by cross-validation\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._validation.cross_val_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._validation.cross_val_score",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like The data to fit. Can be, for example a list, or an array at least 2d. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
          "name": "groups",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Parameters to pass to the fit method of the estimator. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Array of scores of the estimator for each run of the cross validation.  Examples -------- >>> from sklearn import datasets, linear_model >>> from sklearn.model_selection import cross_val_score >>> diabetes = datasets.load_diabetes() >>> X = diabetes.data[:150] >>> y = diabetes.target[:150] >>> lasso = linear_model.Lasso() >>> print(cross_val_score(lasso, X, y))  # doctest: +ELLIPSIS [ 0.33150734  0.08022311  0.03531764]  See Also --------- :func:`sklearn.metrics.make_scorer`: Make a scorer from a performance metric or loss function.  \"",
        "name": "scores",
        "shape": "len(list(cv",
        "type": "array"
      },
      "tags": [
        "model_selection",
        "_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cross_validation.cross_val_score",
      "description": "\"Evaluate a score by cross-validation\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.cross_val_score` instead.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.cross_val_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cross_validation.cross_val_score",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like The data to fit. Can be, for example a list, or an array at least 2d. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Parameters to pass to the fit method of the estimator. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Array of scores of the estimator for each run of the cross validation.  Examples -------- >>> from sklearn import datasets, linear_model >>> from sklearn.cross_validation import cross_val_score >>> diabetes = datasets.load_diabetes() >>> X = diabetes.data[:150] >>> y = diabetes.target[:150] >>> lasso = linear_model.Lasso() >>> print(cross_val_score(lasso, X, y))  # doctest:  +ELLIPSIS [ 0.33150734  0.08022311  0.03531764]  See Also --------- :func:`sklearn.metrics.make_scorer`: Make a scorer from a performance metric or loss function.  \"",
        "name": "scores",
        "shape": "len(list(cv",
        "type": "array"
      },
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. (stored only if store_precision is True) ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Coefficient in the convex combination used for the computation of the shrunk estimate. ",
          "name": "shrinkage_",
          "type": "float"
        }
      ],
      "category": "covariance.shrunk_covariance_",
      "common_name": "Ledoit Wolf",
      "description": "'LedoitWolf Estimator\n\nLedoit-Wolf is a particular form of shrinkage, where the shrinkage\ncoefficient is computed using O. Ledoit and M. Wolf\\'s formula as\ndescribed in \"A Well-Conditioned Estimator for Large-Dimensional\nCovariance Matrices\", Ledoit and Wolf, Journal of Multivariate\nAnalysis, Volume 88, Issue 2, February 2004, pages 365-411.\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n",
      "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "' Fits the Ledoit-Wolf shrunk covariance model\naccording to the given training data and parameters.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features.",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.shrunk_covariance_.LedoitWolf.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.shrunk_covariance_.LedoitWolf",
      "parameters": [
        {
          "description": "Specify if the estimated precision is stored. ",
          "name": "store_precision",
          "type": "bool"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation. ",
          "name": "assume_centered",
          "type": "bool"
        },
        {
          "description": "Size of the blocks into which the covariance matrix will be split during its Ledoit-Wolf estimation. This is purely a memory optimization and does not affect results. ",
          "name": "block_size",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/shrunk_covariance_.pyc:311",
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "ensemble.forest",
      "common_name": "Base Forest",
      "description": "'Base class for forests of trees.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n'",
      "id": "sklearn.ensemble.forest.BaseForest",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.BaseForest.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.BaseForest.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.BaseForest.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.BaseForest.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.BaseForest.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.BaseForest.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.BaseForest.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.BaseForest",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:127",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "preprocessing._function_transformer",
      "common_name": "Function Transformer",
      "description": "\"Constructs a transformer from an arbitrary callable.\n\nA FunctionTransformer forwards its X (and optionally y) arguments to a\nuser-defined function or function object and returns the result of this\nfunction. This is useful for stateless transformations such as taking the\nlog of frequencies, doing custom scaling, etc.\n\nA FunctionTransformer will not do any checks on its function's output.\n\nNote: If a lambda is used as the function, then the resulting\ntransformer will not be pickleable.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <function_transformer>`.\n",
      "id": "sklearn.preprocessing._function_transformer.FunctionTransformer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "None",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.inverse_transform",
          "name": "inverse_transform",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "None",
          "id": "sklearn.preprocessing._function_transformer.FunctionTransformer.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.preprocessing._function_transformer.FunctionTransformer",
      "parameters": [
        {
          "description": "The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function. ",
          "name": "func",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function. ",
          "name": "inverse_func",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Indicate that the input X array should be checked before calling func. If validate is false, there will be no input validation. If it is true, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If this conversion is not possible or X contains NaN or infinity, an exception is raised. ",
          "name": "validate",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised. ",
          "name": "accept_sparse",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Indicate that transform should forward the y argument to the inner callable. ",
          "name": "pass_y",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Dictionary of additional keyword arguments to pass to func. ",
          "name": "kw_args",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Dictionary of additional keyword arguments to pass to inverse_func.  \"",
          "name": "inv_kw_args",
          "optional": "true",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/_function_transformer.pyc:11",
      "tags": [
        "preprocessing",
        "_function_transformer"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Input array. ",
          "name": "X_",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "The distinct labels used in classifying instances. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Categorical distribution for each item. ",
          "name": "label_distributions_",
          "shape": "n_samples, n_classes",
          "type": "array"
        },
        {
          "description": "Label assigned to each item via the transduction. ",
          "name": "transduction_",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Number of iterations run. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "semi_supervised.label_propagation",
      "common_name": "Label Spreading",
      "description": "\"LabelSpreading model for semi-supervised learning\n\nThis model is similar to the basic Label Propgation algorithm,\nbut uses affinity matrix based on the normalized graph Laplacian\nand soft clamping across the labels.\n\nRead more in the :ref:`User Guide <label_propagation>`.\n",
      "id": "sklearn.semi_supervised.label_propagation.LabelSpreading",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit a semi-supervised label propagation model based\n\nAll the input data is provided matrix X (labeled and unlabeled)\nand corresponding label matrix y with a dedicated marker value for\nunlabeled samples.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "A {n_samples by n_samples} size matrix will be created from this ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be transductively assigned labels ",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Performs inductive inference across the model.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Predictions for input data '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict probability for each possible outcome.\n\nCompute the probability estimates for each single sample in X\nand each possible outcome seen during training (categorical\ndistribution).\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Normalized probability distributions across class labels '",
            "name": "probabilities",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.semi_supervised.label_propagation.LabelSpreading.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.semi_supervised.label_propagation.LabelSpreading",
      "parameters": [
        {
          "description": "String identifier for kernel function to use. Only 'rbf' and 'knn' kernels are currently supported. ",
          "name": "kernel",
          "type": "'knn', 'rbf'"
        },
        {
          "description": "parameter for rbf kernel ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "parameter for knn kernel ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "clamping factor ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "maximum number of iterations allowed ",
          "name": "max_iter",
          "type": "float"
        },
        {
          "description": "Convergence tolerance: threshold to consider the system at steady state ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/semi_supervised/label_propagation.pyc:362",
      "tags": [
        "semi_supervised",
        "label_propagation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.kde",
      "common_name": "Kernel Density",
      "description": "\"Kernel Density Estimation\n\nRead more in the :ref:`User Guide <kernel_density>`.\n",
      "id": "sklearn.neighbors.kde.KernelDensity",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the Kernel Density model on the data.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Generate random samples from the model.\n\nCurrently, this is implemented only for gaussian and tophat kernels.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "A random number generator instance. ",
              "name": "random_state",
              "type": ""
            }
          ],
          "returns": {
            "description": "List of samples. '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the total log probability under the model.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Total log-likelihood of the data in X. '",
            "name": "logprob",
            "type": "float"
          }
        },
        {
          "description": "'Evaluate the density model on the data.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "An array of points to query.  Last dimension should match dimension of training data (n_features). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The array of log(density) evaluations. '",
            "name": "density",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.kde.KernelDensity.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.kde.KernelDensity",
      "parameters": [
        {
          "description": "The bandwidth of the kernel. ",
          "name": "bandwidth",
          "type": "float"
        },
        {
          "description": "The tree algorithm to use.  Valid options are ['kd_tree'|'ball_tree'|'auto'].  Default is 'auto'. ",
          "name": "algorithm",
          "type": "string"
        },
        {
          "description": "The kernel to use.  Valid kernels are ['gaussian'|'tophat'|'epanechnikov'|'exponential'|'linear'|'cosine'] Default is 'gaussian'. ",
          "name": "kernel",
          "type": "string"
        },
        {
          "description": "The distance metric to use.  Note that not all metrics are valid with all algorithms.  Refer to the documentation of :class:`BallTree` and :class:`KDTree` for a description of available algorithms.  Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is 'euclidean'. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The desired absolute tolerance of the result.  A larger tolerance will generally lead to faster execution. Default is 0. ",
          "name": "atol",
          "type": "float"
        },
        {
          "description": "The desired relative tolerance of the result.  A larger tolerance will generally lead to faster execution.  Default is 1E-8. ",
          "name": "rtol",
          "type": "float"
        },
        {
          "description": "If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach. ",
          "name": "breadth_first",
          "type": "boolean"
        },
        {
          "description": "Specify the leaf size of the underlying tree.  See :class:`BallTree` or :class:`KDTree` for details.  Default is 40. ",
          "name": "leaf_size",
          "type": "int"
        },
        {
          "description": "Additional parameters to be passed to the tree for use with the metric.  For more information, see the documentation of :class:`BallTree` or :class:`KDTree`. \"",
          "name": "metric_params",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/kde.pyc:24",
      "tags": [
        "neighbors",
        "kde"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Scores of features. ",
          "name": "scores_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "p-values of feature scores.  See also -------- f_classif: ANOVA F-value between label/feature for classification tasks. chi2: Chi-squared stats of non-negative features for classification tasks. f_regression: F-value between label/feature for regression tasks. SelectPercentile: Select features based on percentile of the highest scores. SelectKBest: Select features based on the k highest scores. SelectFpr: Select features based on a false positive rate test. SelectFdr: Select features based on an estimated false discovery rate. GenericUnivariateSelect: Univariate feature selector with configurable mode.",
          "name": "pvalues_",
          "shape": "n_features,",
          "type": "array-like"
        }
      ],
      "category": "feature_selection.univariate_selection",
      "common_name": "Select Fwe",
      "description": "'Filter: Select the p-values corresponding to Family-wise error rate\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.SelectFwe",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run score function on (X, y) and get the appropriate features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.univariate_selection.SelectFwe.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.univariate_selection.SelectFwe",
      "parameters": [
        {
          "description": "Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below \"See also\"). The default function only works with classification tasks. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "The highest uncorrected p-value for features to keep. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/univariate_selection.pyc:606",
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Stores the embedding vectors ",
          "name": "embedding_vectors_",
          "shape": "n_components, n_samples",
          "type": "array-like"
        },
        {
          "description": "Reconstruction error associated with `embedding_vectors_` ",
          "name": "reconstruction_error_",
          "type": "float"
        },
        {
          "description": "Stores nearest neighbors instance, including BallTree or KDtree if applicable. ",
          "name": "nbrs_",
          "type": ""
        }
      ],
      "category": "manifold.locally_linear",
      "common_name": "Locally Linear Embedding",
      "description": "\"Locally Linear Embedding\n\nRead more in the :ref:`User Guide <locally_linear_embedding>`.\n",
      "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the embedding vectors for data X\n",
          "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Compute the embedding vectors for data X and transform X.\n",
          "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "X_new: array-like, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'\nTransform new points into embedding space.\n",
          "id": "sklearn.manifold.locally_linear.LocallyLinearEmbedding.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs) '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.manifold.locally_linear.LocallyLinearEmbedding",
      "parameters": [
        {
          "description": "number of neighbors to consider for each point. ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "number of coordinates for the manifold ",
          "name": "n_components",
          "type": "integer"
        },
        {
          "description": "regularization constant, multiplies the trace of the local covariance matrix of the distances. ",
          "name": "reg",
          "type": "float"
        },
        {
          "description": "",
          "name": "eigen_solver",
          "type": "string"
        },
        {
          "description": "",
          "name": "auto",
          "type": "algorithm"
        },
        {
          "description": "For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results. ",
          "name": "arpack",
          "type": "use"
        },
        {
          "description": "decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems. ",
          "name": "dense",
          "type": "use"
        },
        {
          "description": "Tolerance for 'arpack' method Not used if eigen_solver=='dense'. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "maximum number of iterations for the arpack solver. Not used if eigen_solver=='dense'. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "",
          "name": "method",
          "type": "string"
        },
        {
          "description": "reference [1]",
          "name": "standard",
          "type": "use"
        },
        {
          "description": "``n_neighbors > n_components * (1 + (n_components + 1) / 2`` see reference [2]",
          "name": "hessian",
          "type": "use"
        },
        {
          "description": "see reference [3]",
          "name": "modified",
          "type": "use"
        },
        {
          "description": "see reference [4] ",
          "name": "ltsa",
          "type": "use"
        },
        {
          "description": "Tolerance for Hessian eigenmapping method. Only used if ``method == 'hessian'`` ",
          "name": "hessian_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Tolerance for modified LLE method. Only used if ``method == 'modified'`` ",
          "name": "modified_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "algorithm to use for nearest neighbors search, passed to neighbors.NearestNeighbors instance  random_state: numpy.RandomState or int, optional The generator or seed used to determine the starting vector for arpack iterations.  Defaults to numpy.random. ",
          "name": "neighbors_algorithm",
          "type": "string"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/manifold/locally_linear.pyc:508",
      "tags": [
        "manifold",
        "locally_linear"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Sparse components extracted from the data. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Vector of errors at each iteration. ",
          "name": "error_",
          "type": "array"
        },
        {
          "description": "Number of iterations run.  See also -------- PCA SparsePCA DictionaryLearning",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.sparse_pca",
      "common_name": "Mini Batch Sparse PCA",
      "description": "\"Mini-batch Sparse Principal Components Analysis\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n",
      "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Least Squares projection of the data onto the sparse components.\n\nTo avoid instability issues in case the system is under-determined,\nregularization can be applied (Ridge regression) via the\n`ridge_alpha` parameter.\n\nNote that Sparse PCA components orthogonality is not enforced as in PCA\nhence one cannot use a simple linear projection.\n",
          "id": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Test data to be transformed, must have the same number of features as the data used to train the model.  ridge_alpha: float, default: 0.01 Amount of ridge shrinkage to apply in order to improve conditioning. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_new array, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.decomposition.sparse_pca.MiniBatchSparsePCA",
      "parameters": [
        {
          "description": "number of sparse atoms to extract ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Sparsity controlling parameter. Higher values lead to sparser components. ",
          "name": "alpha",
          "type": "int"
        },
        {
          "description": "Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method. ",
          "name": "ridge_alpha",
          "type": "float"
        },
        {
          "description": "number of iterations to perform for each mini batch ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "callable that gets invoked every five iterations ",
          "name": "callback",
          "type": "callable"
        },
        {
          "description": "the number of features to take in each mini batch  verbose : degree of output the procedure will print ",
          "name": "batch_size",
          "type": "int"
        },
        {
          "description": "whether to shuffle the data before splitting it in batches ",
          "name": "shuffle",
          "type": "boolean"
        },
        {
          "description": "number of parallel jobs to run, or -1 to autodetect. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. ",
          "name": "method",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/sparse_pca.pyc:170",
      "tags": [
        "decomposition",
        "sparse_pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Sparse components extracted from the data. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Vector of errors at each iteration. ",
          "name": "error_",
          "type": "array"
        },
        {
          "description": "Number of iterations run.  See also -------- PCA MiniBatchSparsePCA DictionaryLearning",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.sparse_pca",
      "common_name": "Sparse PCA",
      "description": "\"Sparse Principal Components Analysis (SparsePCA)\n\nFinds the set of sparse components that can optimally reconstruct\nthe data.  The amount of sparseness is controllable by the coefficient\nof the L1 penalty, given by the parameter alpha.\n\nRead more in the :ref:`User Guide <SparsePCA>`.\n",
      "id": "sklearn.decomposition.sparse_pca.SparsePCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.decomposition.sparse_pca.SparsePCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.sparse_pca.SparsePCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.sparse_pca.SparsePCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.sparse_pca.SparsePCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Least Squares projection of the data onto the sparse components.\n\nTo avoid instability issues in case the system is under-determined,\nregularization can be applied (Ridge regression) via the\n`ridge_alpha` parameter.\n\nNote that Sparse PCA components orthogonality is not enforced as in PCA\nhence one cannot use a simple linear projection.\n",
          "id": "sklearn.decomposition.sparse_pca.SparsePCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Test data to be transformed, must have the same number of features as the data used to train the model.  ridge_alpha: float, default: 0.01 Amount of ridge shrinkage to apply in order to improve conditioning. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_new array, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.decomposition.sparse_pca.SparsePCA",
      "parameters": [
        {
          "description": "Number of sparse atoms to extract. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Sparsity controlling parameter. Higher values lead to sparser components. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method. ",
          "name": "ridge_alpha",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations to perform. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "Tolerance for the stopping condition. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. ",
          "name": "method",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Number of parallel jobs to run.  U_init : array of shape (n_samples, n_components), Initial values for the loadings for warm restart scenarios.  V_init : array of shape (n_components, n_features), Initial values for the components for warm restart scenarios.  verbose : Degree of verbosity of the printed output. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/sparse_pca.pyc:14",
      "tags": [
        "decomposition",
        "sparse_pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Indices of support vectors. ",
          "name": "support_",
          "shape": "n_SV",
          "type": "array-like"
        },
        {
          "description": "Support vectors. ",
          "name": "support_vectors_",
          "shape": "nSV, n_features",
          "type": "array-like"
        },
        {
          "description": "Coefficients of the support vectors in the decision function. ",
          "name": "dual_coef_",
          "shape": "n_classes-1, n_SV",
          "type": "array"
        },
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_` ",
          "name": "coef_",
          "shape": "n_classes-1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "n_classes-1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "One Class SVM",
      "description": "\"Unsupervised Outlier Detection.\n\nEstimate the support of a high-dimensional distribution.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_outlier_detection>`.\n",
      "id": "sklearn.svm.classes.OneClassSVM",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Distance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.classes.OneClassSVM.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the samples. '",
            "name": "X",
            "shape": "n_samples,",
            "type": "array-like"
          }
        },
        {
          "description": "'\nDetects the soft boundary of the set of samples X.\n",
          "id": "sklearn.svm.classes.OneClassSVM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Set of samples, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ----- If X is not a C-ordered contiguous array it is copied.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.OneClassSVM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform regression on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.classes.OneClassSVM.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is (n_samples_test, n_samples_train). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.OneClassSVM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.OneClassSVM",
      "parameters": [
        {
          "default": "'rbf'",
          "description": "Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. ",
          "name": "kernel",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken. ",
          "name": "nu",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "3",
          "description": "Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. ",
          "name": "degree",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "'auto'",
          "description": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Tolerance for stopping criterion. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Whether to use the shrinking heuristic. ",
          "name": "shrinking",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Specify the size of the kernel cache (in MB). ",
          "name": "cache_size",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "default": "-1",
          "description": "Hard limit on iterations within solver, or -1 for no limit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data for probability estimation. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:923",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The median value for each feature in the training set. ",
          "name": "center_",
          "type": "array"
        },
        {
          "description": "The (scaled) interquartile range for each feature in the training set.  .. versionadded:: 0.17 *scale_* attribute.  See also -------- robust_scale: Equivalent function without the object oriented API.  :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'. ",
          "name": "scale_",
          "type": "array"
        }
      ],
      "category": "preprocessing.data",
      "common_name": "Robust Scaler",
      "description": "\"Scale features using statistics that are robust to outliers.\n\nThis Scaler removes the median and scales the data according to\nthe quantile range (defaults to IQR: Interquartile Range).\nThe IQR is the range between the 1st quartile (25th quantile)\nand the 3rd quartile (75th quantile).\n\nCentering and scaling happen independently on each feature (or each\nsample, depending on the `axis` argument) by computing the relevant\nstatistics on the samples in the training set. Median and  interquartile\nrange are then stored to be used on later data using the `transform`\nmethod.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators. Typically this is done by removing the mean\nand scaling to unit variance. However, outliers can often influence the\nsample mean / variance in a negative way. In such cases, the median and\nthe interquartile range often give better results.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "id": "sklearn.preprocessing.data.RobustScaler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the median and quantiles to be used for scaling.\n",
          "id": "sklearn.preprocessing.data.RobustScaler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The data used to compute the median and quantiles used for later scaling along the features axis. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.RobustScaler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.RobustScaler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Scale back the data to the original representation\n",
          "id": "sklearn.preprocessing.data.RobustScaler.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The data used to scale along the specified axis. '",
              "name": "X",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.RobustScaler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Center and scale the data\n",
          "id": "sklearn.preprocessing.data.RobustScaler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data used to scale along the specified axis. '",
              "name": "X",
              "type": "array-like"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.RobustScaler",
      "parameters": [
        {
          "description": "If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. ",
          "name": "with_centering",
          "type": "boolean"
        },
        {
          "description": "If True, scale the data to interquartile range. ",
          "name": "with_scaling",
          "type": "boolean"
        },
        {
          "description": "Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate ``scale_``.  .. versionadded:: 0.18 ",
          "name": "quantile_range",
          "type": "tuple"
        },
        {
          "description": "If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:898",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "bayesian"
      ],
      "attributes": [
        {
          "description": "Components with maximum variance. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "The log likelihood at each iteration. ",
          "name": "loglike_",
          "type": "list"
        },
        {
          "description": "The estimated noise variance for each feature. ",
          "name": "noise_variance_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Number of iterations run. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.factor_analysis",
      "common_name": "Factor Analysis",
      "description": "\"Factor Analysis (FA)\n\nA simple linear generative model with Gaussian latent variables.\n\nThe observations are assumed to be caused by a linear transformation of\nlower dimensional latent factors and added Gaussian noise.\nWithout loss of generality the factors are distributed according to a\nGaussian with zero mean and unit covariance. The noise is also zero mean\nand has an arbitrary diagonal covariance matrix.\n\nIf we would restrict the model further, by assuming that the Gaussian\nnoise is even isotropic (all diagonal entries are the same) we would obtain\n:class:`PPCA`.\n\nFactorAnalysis performs a maximum likelihood estimate of the so-called\n`loading` matrix, the transformation of the latent variables to the\nobserved ones, using expectation-maximization (EM).\n\nRead more in the :ref:`User Guide <FA>`.\n",
      "id": "sklearn.decomposition.factor_analysis.FactorAnalysis",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the FactorAnalysis model to X using EM\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Compute data covariance with the FactorAnalysis model.\n\n``cov = components_.T * components_ + diag(noise_variance)``\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.get_covariance",
          "name": "get_covariance",
          "parameters": [],
          "returns": {
            "description": "Estimated covariance of data. '",
            "name": "cov",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Compute data precision matrix with the FactorAnalysis model.\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "Estimated precision of data. '",
            "name": "precision",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the average log-likelihood of the samples\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.score",
          "name": "score",
          "parameters": [
            {
              "description": "The data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Average log-likelihood of the samples under the current model '",
            "name": "ll: float"
          }
        },
        {
          "description": "'Compute the log-likelihood of each sample\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "The data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log-likelihood of each sample under the current model '",
            "name": "ll: array, shape (n_samples,)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply dimensionality reduction to X using the model.\n\nCompute the expected mean of the latent variables.\nSee Barber, 21.2.33 (or Bishop, 12.66).\n",
          "id": "sklearn.decomposition.factor_analysis.FactorAnalysis.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The latent variables of X. '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.decomposition.factor_analysis.FactorAnalysis",
      "parameters": [
        {
          "description": "Dimensionality of latent space, the number of components of ``X`` that are obtained after ``transform``. If None, n_components is set to the number of features. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Stopping tolerance for EM algorithm. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Whether to make a copy of X. If ``False``, the input X gets overwritten during fitting. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "Maximum number of iterations. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features) ",
          "name": "noise_variance_init",
          "shape": "n_features,",
          "type": ""
        },
        {
          "description": "Which SVD method to use. If 'lapack' use standard SVD from scipy.linalg, if 'randomized' use fast ``randomized_svd`` function. Defaults to 'randomized'. For most applications 'randomized' will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for `iterated_power`. If this is not sufficient, for maximum precision you should choose 'lapack'. ",
          "name": "svd_method",
          "type": "'lapack', 'randomized'"
        },
        {
          "description": "Number of iterations for the power method. 3 by default. Only used if ``svd_method`` equals 'randomized' ",
          "name": "iterated_power",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Pseudo number generator state used for random sampling. Only used if ``svd_method`` equals 'randomized' ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/factor_analysis.pyc:36",
      "tags": [
        "decomposition",
        "factor_analysis"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The classes labels. ",
          "name": "classes_",
          "shape": "n_predictions",
          "type": "array-like"
        }
      ],
      "category": "ensemble.voting_classifier",
      "common_name": "Voting Classifier",
      "description": "\"Soft Voting/Majority Rule classifier for unfitted estimators.\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <voting_classifier>`.\n",
      "id": "sklearn.ensemble.voting_classifier.VotingClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "' Fit the estimators.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Return estimator parameter names for GridSearch support'",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.get_params",
          "name": "get_params",
          "parameters": []
        },
        {
          "description": "' Predict class labels for X.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class labels. '",
            "name": "maj",
            "shape": "n_samples",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "\"Return class labels or probabilities for X for each estimator.\n",
          "id": "sklearn.ensemble.voting_classifier.VotingClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "array-like = [n_classifiers, n_samples, n_classes] Class probabilities calculated by each classifier. If `voting='hard'`: array-like = [n_samples, n_classifiers] Class labels predicted by each classifier. \"",
            "name": "If `voting='soft'`:"
          }
        }
      ],
      "name": "sklearn.ensemble.voting_classifier.VotingClassifier",
      "parameters": [
        {
          "description": "Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones of those original estimators that will be stored in the class attribute `self.estimators_`. ",
          "name": "estimators",
          "type": "list"
        },
        {
          "description": "If 'hard', uses predicted class labels for majority rule voting. Else if 'soft', predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. ",
          "name": "voting",
          "type": "str"
        },
        {
          "default": "`None`",
          "description": "Sequence of weights (`float` or `int`) to weight the occurrences of predicted class labels (`hard` voting) or class probabilities before averaging (`soft` voting). Uses uniform weights if `None`. ",
          "name": "weights",
          "optional": "true",
          "shape": "n_classifiers",
          "type": "array-like"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for ``fit``. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/voting_classifier.pyc:35",
      "tags": [
        "ensemble",
        "voting_classifier"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of iterations run.  See Also -------- graph_lasso, GraphLassoCV",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "covariance.graph_lasso_",
      "common_name": "Graph Lasso",
      "description": "\"Sparse inverse covariance estimation with an l1-penalized estimator.\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n",
      "id": "sklearn.covariance.graph_lasso_.GraphLasso",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "None",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLasso.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.graph_lasso_.GraphLasso",
      "parameters": [
        {
          "description": "The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. ",
          "name": "alpha",
          "type": "positive"
        },
        {
          "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable. ",
          "name": "mode",
          "type": "'cd', 'lars'"
        },
        {
          "description": "The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. ",
          "name": "tol",
          "type": "positive"
        },
        {
          "description": "The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. ",
          "name": "enet_tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The maximum number of iterations. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "If verbose is True, the objective function and dual gap are plotted at each iteration. ",
          "name": "verbose",
          "type": "boolean"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/graph_lasso_.pyc:270",
      "tags": [
        "covariance",
        "graph_lasso_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": ""
        },
        {
          "description": "Estimated pseudo-inverse matrix. (stored only if store_precision is True) ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": ""
        }
      ],
      "category": "covariance.empirical_covariance_",
      "common_name": "Empirical Covariance",
      "description": "'Maximum likelihood covariance estimator\n\nRead more in the :ref:`User Guide <covariance>`.\n",
      "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "'Fits the Maximum Likelihood Estimator covariance model\naccording to the given training data and parameters.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.empirical_covariance_.EmpiricalCovariance",
      "parameters": [
        {
          "description": "Specifies if the estimated precision is stored. ",
          "name": "store_precision",
          "type": "bool"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation. ",
          "name": "assume_centered",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/empirical_covariance_.pyc:88",
      "tags": [
        "covariance",
        "empirical_covariance_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "dictionary atoms extracted from the data ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "vector of errors at each iteration ",
          "name": "error_",
          "type": "array"
        },
        {
          "description": "Number of iterations run. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.dict_learning",
      "common_name": "Dictionary Learning",
      "description": "\"Dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n(U,V)\nwith || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n",
      "id": "sklearn.decomposition.dict_learning.DictionaryLearning",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.decomposition.dict_learning.DictionaryLearning.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the object itself '",
            "name": "self: object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.dict_learning.DictionaryLearning.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.dict_learning.DictionaryLearning.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.dict_learning.DictionaryLearning.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.\n",
          "id": "sklearn.decomposition.dict_learning.DictionaryLearning.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Test data to be transformed, must have the same number of features as the data used to train the model. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Transformed data  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.decomposition.dict_learning.DictionaryLearning",
      "parameters": [
        {
          "description": "number of dictionary elements to extract ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "sparsity controlling parameter ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "maximum number of iterations to perform ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "tolerance for numerical error ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse.  .. versionadded:: 0.17 *cd* coordinate descent method to improve speed. ",
          "name": "fit_algorithm",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection ``dictionary * X'``  .. versionadded:: 0.17 *lasso_cd* coordinate descent method to improve speed. ",
          "name": "transform_algorithm",
          "type": "'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'"
        },
        {
          "description": "Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case. ",
          "name": "transform_n_nonzero_coefs",
          "type": "int"
        },
        {
          "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`. ",
          "name": "transform_alpha",
          "type": "float"
        },
        {
          "description": "Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. ",
          "name": "split_sign",
          "type": "bool"
        },
        {
          "description": "number of parallel jobs to run ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "initial value for the code, for warm restart ",
          "name": "code_init",
          "shape": "n_samples, n_components",
          "type": "array"
        },
        {
          "description": "initial values for the dictionary, for warm restart  verbose : degree of verbosity of the printed output ",
          "name": "dict_init",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc:916",
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Indices for active features, meaning values that actually occur in the training set. Only available when n_values is ``\\'auto\\'``. ",
          "name": "active_features_",
          "type": "array"
        },
        {
          "description": "Indices to feature ranges. Feature ``i`` in the original data is mapped to features from ``feature_indices_[i]`` to ``feature_indices_[i+1]`` (and then potentially masked by `active_features_` afterwards) ",
          "name": "feature_indices_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Maximum number of values per feature. ",
          "name": "n_values_",
          "shape": "n_features,",
          "type": "array"
        }
      ],
      "category": "preprocessing.data",
      "common_name": "One Hot Encoder",
      "description": "'Encode categorical integer features using a one-hot aka one-of-K scheme.\n\nThe input to this transformer should be a matrix of integers, denoting\nthe values taken on by categorical (discrete) features. The output will be\na sparse matrix where each column corresponds to one possible value of one\nfeature. It is assumed that input features take on values in the range\n[0, n_values).\n\nThis encoding is needed for feeding categorical data to many scikit-learn\nestimators, notably linear models and SVMs with the standard kernels.\n\nNote: a one-hot encoding of y labels should use a LabelBinarizer\ninstead.\n\nRead more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
      "id": "sklearn.preprocessing.data.OneHotEncoder",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit OneHotEncoder to X.\n",
          "id": "sklearn.preprocessing.data.OneHotEncoder.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Input array of type int. ",
              "name": "X",
              "shape": "n_samples, n_feature",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Fit OneHotEncoder to X, then transform X.\n\nEquivalent to self.fit(X).transform(X), but more convenient and more\nefficient. See fit for the parameters, transform for the return value.\n'",
          "id": "sklearn.preprocessing.data.OneHotEncoder.fit_transform",
          "name": "fit_transform",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.OneHotEncoder.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.OneHotEncoder.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform X using one-hot encoding.\n",
          "id": "sklearn.preprocessing.data.OneHotEncoder.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input array of type int. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed input. '",
            "name": "X_out",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.preprocessing.data.OneHotEncoder",
      "parameters": [
        {
          "description": "Number of values per feature.  - \\'auto\\' : determine value range from training data. - int : number of categorical values per feature. Each feature value should be in ``range(n_values)`` - array : ``n_values[i]`` is the number of categorical values in ``X[:, i]``. Each feature value should be in ``range(n_values[i])`` ",
          "name": "n_values",
          "type": ""
        },
        {
          "description": "Specify what features are treated as categorical.  - \\'all\\' (default): All features are treated as categorical. - array of indices: Array of categorical feature indices. - mask: Array of length n_features and with dtype=bool.  Non-categorical features are always stacked to the right of the matrix. ",
          "name": "categorical_features",
          "type": ""
        },
        {
          "description": "Desired dtype of output. ",
          "name": "dtype",
          "type": "number"
        },
        {
          "description": "Will return sparse matrix if set True else will return an array. ",
          "name": "sparse",
          "type": "boolean"
        },
        {
          "description": "Whether to raise an error or ignore if a unknown categorical feature is present during transform. ",
          "name": "handle_unknown",
          "type": "str"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:1728",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "tree.tree",
      "common_name": "Base Decision Tree",
      "description": "'Base class for decision trees.\n\nWarning: This class should not be used directly.\nUse derived classes instead.\n'",
      "id": "sklearn.tree.tree.BaseDecisionTree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nReturns the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within ``[0; self.tree_.node_count)``, possibly with gaps in the numbering. \"",
            "name": "X_leaves",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Return the decision path in the tree\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  \"",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "None",
          "id": "sklearn.tree.tree.BaseDecisionTree.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "The predicted classes, or the predict values. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.tree.tree.BaseDecisionTree.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.tree.tree.BaseDecisionTree",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc:74",
      "tags": [
        "tree",
        "tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Indices of core samples. ",
          "name": "core_sample_indices_",
          "shape": "n_core_samples",
          "type": "array"
        },
        {
          "description": "Copy of each core sample found by training. ",
          "name": "components_",
          "shape": "n_core_samples, n_features",
          "type": "array"
        },
        {
          "description": "Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1. ",
          "name": "labels_",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "category": "cluster.dbscan_",
      "common_name": "DBSCAN",
      "description": "'Perform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise.\nFinds core samples of high density and expands clusters from them.\nGood for data which contains clusters of similar density.\n\nRead more in the :ref:`User Guide <dbscan>`.\n",
      "id": "sklearn.cluster.dbscan_.DBSCAN",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Perform DBSCAN clustering from features or distance matrix.\n",
          "id": "sklearn.cluster.dbscan_.DBSCAN.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "A feature array, or array of distances between samples if ``metric='precomputed'``.",
              "name": "X",
              "shape": "n_samples, n_samples",
              "type": "array"
            },
            {
              "description": "Weight of each sample, such that a sample with a weight of at least ``min_samples`` is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1. \"",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array"
            }
          ]
        },
        {
          "description": "\"Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.dbscan_.DBSCAN.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "A feature array, or array of distances between samples if ``metric='precomputed'``.",
              "name": "X",
              "shape": "n_samples, n_samples",
              "type": "array"
            },
            {
              "description": "Weight of each sample, such that a sample with a weight of at least ``min_samples`` is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array"
            }
          ],
          "returns": {
            "description": "cluster labels \"",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.dbscan_.DBSCAN.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.dbscan_.DBSCAN.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.dbscan_.DBSCAN",
      "parameters": [
        {
          "description": "The maximum distance between two samples for them to be considered as in the same neighborhood. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. ",
          "name": "min_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.calculate_distance for its metric parameter. If metric is \"precomputed\", X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only \"nonzero\" elements may be considered neighbors for DBSCAN.  .. versionadded:: 0.17 metric *precomputed* to accept precomputed sparse matrix. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. ",
          "name": "algorithm",
          "optional": "true",
          "type": "\\'auto\\', \\'ball_tree\\', \\'kd_tree\\', \\'brute\\'"
        },
        {
          "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The power of the Minkowski metric to be used to calculate distance between points. ",
          "name": "p",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/dbscan_.pyc:156",
      "tags": [
        "cluster",
        "dbscan_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "cluster labels for each point ",
          "name": "labels_",
          "type": "array"
        },
        {
          "description": "Number of leaves in the hierarchical tree. ",
          "name": "n_leaves_",
          "type": "int"
        },
        {
          "description": "The estimated number of connected components in the graph. ",
          "name": "n_components_",
          "type": "int"
        },
        {
          "description": "The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i - n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i` ",
          "name": "children_",
          "shape": "n_nodes-1, 2",
          "type": "array-like"
        }
      ],
      "category": "cluster.hierarchical",
      "common_name": "Agglomerative Clustering",
      "description": "'\nAgglomerative Clustering\n\nRecursively merges the pair of clusters that minimally increases\na given linkage distance.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "id": "sklearn.cluster.hierarchical.AgglomerativeClustering",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the hierarchical clustering on the data\n",
          "id": "sklearn.cluster.hierarchical.AgglomerativeClustering.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The samples a.k.a. observations. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.hierarchical.AgglomerativeClustering.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.hierarchical.AgglomerativeClustering.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.hierarchical.AgglomerativeClustering.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.hierarchical.AgglomerativeClustering",
      "parameters": [
        {
          "description": "The number of clusters to find. ",
          "name": "n_clusters",
          "type": "int"
        },
        {
          "description": "Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. ",
          "name": "connectivity",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\", or \\'precomputed\\'. If linkage is \"ward\", only \"euclidean\" is accepted. ",
          "name": "affinity",
          "type": "string"
        },
        {
          "description": "Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. ",
          "name": "memory",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. ",
          "name": "compute_full_tree",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.  - ward minimizes the variance of the clusters being merged. - average uses the average of the distances of each observation of the two sets. - complete or maximum linkage uses the maximum distances between all observations of the two sets. ",
          "name": "linkage",
          "optional": "true",
          "type": "\"ward\", \"complete\", \"average\""
        },
        {
          "description": "This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument ``axis=1``, and reduce it to an array of size [M]. ",
          "name": "pooling_func",
          "type": "callable"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/hierarchical.pyc:585",
      "tags": [
        "cluster",
        "hierarchical"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "mixture.gmm",
      "common_name": "GMM",
      "description": "'\nLegacy Gaussian Mixture Model\n\n.. deprecated:: 0.18\nThis class will be removed in 0.20.\nUse :class:`sklearn.mixture.GaussianMixture` instead.\n\n'",
      "id": "sklearn.mixture.gmm.GMM",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Akaike information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.gmm.GMM.aic",
          "name": "aic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "aic: float (the lower the better)"
          }
        },
        {
          "description": "'Bayesian information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.gmm.GMM.bic",
          "name": "bic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "bic: float (the lower the better)"
          }
        },
        {
          "description": "\"Estimate model parameters with the EM algorithm.\n\nA initialization step is performed before entering the\nexpectation-maximization (EM) algorithm. If you want to avoid\nthis step, set the keyword argument init_params to the empty\nstring '' when creating the GMM object. Likewise, if you would\nlike just to do an initialization, set n_iter=0.\n",
          "id": "sklearn.mixture.gmm.GMM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Fit and then predict labels for data.\n\nWarning: Due to the final maximization step in the EM algorithm,\nwith low iterations the prediction may not be 100%  accurate.\n\n.. versionadded:: 0.17\n*fit_predict* method in Gaussian Mixture Model.\n",
          "id": "sklearn.mixture.gmm.GMM.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.gmm.GMM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict label for data.\n",
          "id": "sklearn.mixture.gmm.GMM.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data under each Gaussian\nin the model.\n",
          "id": "sklearn.mixture.gmm.GMM.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "responsibilities",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Generate random samples from the model.\n",
          "id": "sklearn.mixture.gmm.GMM.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "List of samples '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the log probability under the model.\n",
          "id": "sklearn.mixture.gmm.GMM.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Return the per-sample likelihood of the data under the model.\n\nCompute the log probability of X under the model and\nreturn the posterior distribution (responsibilities) of each\nmixture component for each element of X.\n",
          "id": "sklearn.mixture.gmm.GMM.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X.  responsibilities : array_like, shape (n_samples, n_components) Posterior probabilities of each mixture component for each observation '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.gmm.GMM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.gmm.GMM",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/gmm.pyc:664",
      "tags": [
        "mixture",
        "gmm"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Estimators used for predictions. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "Array containing labels. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "numpy"
        },
        {
          "description": "Binary array containing the code of each class. ",
          "name": "code_book_",
          "shape": "n_classes, code_size",
          "type": "numpy"
        }
      ],
      "category": "multiclass",
      "common_name": "Output Code Classifier",
      "description": "'(Error-Correcting) Output-Code multiclass strategy\n\nOutput-code based strategies consist in representing each class with a\nbinary code (an array of 0s and 1s). At fitting time, one binary\nclassifier per bit in the code book is fitted.  At prediction time, the\nclassifiers are used to project new points in the class space and the class\nclosest to the points is chosen. The main advantage of these strategies is\nthat the number of classifiers used can be controlled by the user, either\nfor compressing the model (0 < code_size < 1) or for making the model more\nrobust to errors (code_size > 1). See the documentation for more details.\n\nRead more in the :ref:`User Guide <ecoc>`.\n",
      "id": "sklearn.multiclass.OutputCodeClassifier",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit underlying estimators.\n",
          "id": "sklearn.multiclass.OutputCodeClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-class targets. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multiclass.OutputCodeClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict multi-class targets using underlying estimators.\n",
          "id": "sklearn.multiclass.OutputCodeClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Predicted multi-class targets. '",
            "name": "y",
            "shape": "n_samples",
            "type": "numpy"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.multiclass.OutputCodeClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multiclass.OutputCodeClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multiclass.OutputCodeClassifier",
      "parameters": [
        {
          "description": "An estimator object implementing `fit` and one of `decision_function` or `predict_proba`. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Percentage of the number of classes to be used to create the code book. A number between 0 and 1 will require fewer classifiers than one-vs-the-rest. A number greater than 1 will require more classifiers than one-vs-the-rest. ",
          "name": "code_size",
          "type": "float"
        },
        {
          "description": "The generator used to initialize the codebook. Defaults to numpy.random. ",
          "name": "random_state",
          "optional": "true",
          "type": "numpy"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc:614",
      "tags": [
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "mixture.dpgmm",
      "common_name": "DPGMM",
      "description": "\"Dirichlet Process Gaussian Mixture Models\n\n.. deprecated:: 0.18\nThis class will be removed in 0.20.\nUse :class:`sklearn.mixture.BayesianGaussianMixture` with\nparameter ``weight_concentration_prior_type='dirichlet_process'``\ninstead.\n\n\"",
      "id": "sklearn.mixture.dpgmm.DPGMM",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Akaike information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.aic",
          "name": "aic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "aic: float (the lower the better)"
          }
        },
        {
          "description": "'Bayesian information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.bic",
          "name": "bic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "bic: float (the lower the better)"
          }
        },
        {
          "description": "\"Estimate model parameters with the EM algorithm.\n\nA initialization step is performed before entering the\nexpectation-maximization (EM) algorithm. If you want to avoid\nthis step, set the keyword argument init_params to the empty\nstring '' when creating the GMM object. Likewise, if you would\nlike just to do an initialization, set n_iter=0.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Fit and then predict labels for data.\n\nWarning: Due to the final maximization step in the EM algorithm,\nwith low iterations the prediction may not be 100%  accurate.\n\n.. versionadded:: 0.17\n*fit_predict* method in Gaussian Mixture Model.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'returns a lower bound on model evidence based on X and membership'",
          "id": "sklearn.mixture.dpgmm.DPGMM.lower_bound",
          "name": "lower_bound",
          "parameters": []
        },
        {
          "description": "'Predict label for data.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data under each Gaussian\nin the model.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "responsibilities",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Generate random samples from the model.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "List of samples '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the log probability under the model.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Return the likelihood of the data under the model.\n\nCompute the bound on log probability of X under the model\nand return the posterior distribution (responsibilities) of\neach mixture component for each element of X.\n\nThis is done by computing the parameters for the mean-field of\nz for each observation.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X responsibilities : array_like, shape (n_samples, n_components) Posterior probabilities of each mixture component for each observation '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.dpgmm.DPGMM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.dpgmm.DPGMM",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/dpgmm.pyc:631",
      "tags": [
        "mixture",
        "dpgmm"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Coefficients of the regression model (mean of distribution) ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "estimated precision of the noise. ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "estimated precision of the weights. ",
          "name": "lambda_",
          "type": "float"
        },
        {
          "description": "if computed, value of the objective function (to be maximized) ",
          "name": "scores_",
          "type": "float"
        }
      ],
      "category": "linear_model.bayes",
      "common_name": "Bayesian Ridge",
      "description": "'Bayesian ridge regression\n\nFit a Bayesian ridge model and optimize the regularization parameters\nlambda (precision of the weights) and alpha (precision of the noise).\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n",
      "id": "sklearn.linear_model.bayes.BayesianRidge",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data",
              "name": "X",
              "shape": "n_samples,n_features",
              "type": "numpy"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.bayes.BayesianRidge.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.bayes.BayesianRidge",
      "parameters": [
        {
          "description": "Maximum number of iterations.  Default is 300. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stop the algorithm if w has converged. Default is 1.e-3.  alpha_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6  alpha_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.  lambda_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.  lambda_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6 ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If True, compute the objective function at each step of the model. Default is False ",
          "name": "compute_score",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). Default is True. ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Verbose mode when fitting the model.  ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/bayes.pyc:22",
      "tags": [
        "linear_model",
        "bayes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the ``SelectFromModel``, i.e when prefit is False.  `threshold_`: float The threshold value used for feature selection.",
          "name": "`estimator_`",
          "type": "an"
        }
      ],
      "category": "feature_selection.from_model",
      "common_name": "Select From Model",
      "description": "'Meta-transformer for selecting features based on importance weights.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.feature_selection.from_model.SelectFromModel",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the SelectFromModel meta-transformer.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (integers that correspond to classes in classification, real numbers in regression).  **fit_params : Other estimator specific parameters ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "'Fit the SelectFromModel meta-transformer only once.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (integers that correspond to classes in classification, real numbers in regression).  **fit_params : Other estimator specific parameters ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.from_model.SelectFromModel.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.from_model.SelectFromModel",
      "parameters": [
        {
          "description": "The base estimator from which the transformer is built. This can be both a fitted (if ``prefit`` is set to True) or a non-fitted estimator. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, \"mean\" is used by default. ",
          "name": "threshold",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Whether a prefit model is expected to be passed into the constructor directly or not. If True, ``transform`` must be called directly and SelectFromModel cannot be used with ``cross_val_score``, ``GridSearchCV`` and similar utilities that clone the estimator. Otherwise train the model using ``fit`` and then ``transform`` to do feature selection. ",
          "name": "prefit",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/from_model.pyc:145",
      "tags": [
        "feature_selection",
        "from_model"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The number of selected features. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The mask of selected features. ",
          "name": "support_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The feature ranking, such that ``ranking_[i]`` corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1. ",
          "name": "ranking_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The external estimator fit on the reduced dataset. ",
          "name": "estimator_",
          "type": "object"
        }
      ],
      "category": "feature_selection.rfe",
      "common_name": "RFE",
      "description": "'Feature ranking with recursive feature elimination.\n\nGiven an external estimator that assigns weights to features (e.g., the\ncoefficients of a linear model), the goal of recursive feature elimination\n(RFE) is to select features by recursively considering smaller and smaller\nsets of features. First, the estimator is trained on the initial set of\nfeatures and weights are assigned to each one of them. Then, features whose\nabsolute weights are the smallest are pruned from the current set features.\nThat procedure is recursively repeated on the pruned set until the desired\nnumber of features to select is eventually reached.\n\nRead more in the :ref:`User Guide <rfe>`.\n",
      "id": "sklearn.feature_selection.rfe.RFE",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the RFE model and then the underlying estimator on the selected\nfeatures.\n",
          "id": "sklearn.feature_selection.rfe.RFE.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values. '",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.rfe.RFE.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.rfe.RFE.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.rfe.RFE.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.rfe.RFE.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.rfe.RFE.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.rfe.RFE.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.rfe.RFE",
      "parameters": [
        {
          "description": "A supervised learning estimator with a `fit` method that updates a `coef_` attribute that holds the fitted parameters. Important features must correspond to high absolute values in the `coef_` array.  For instance, this is the case for most supervised learning algorithms such as Support Vector Classifiers and Generalized Linear Models from the `svm` and `linear_model` modules. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "The number of features to select. If `None`, half of the features are selected. ",
          "name": "n_features_to_select",
          "type": "int"
        },
        {
          "default": "1",
          "description": "If greater than or equal to 1, then `step` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then `step` corresponds to the percentage (rounded down) of features to remove at each iteration. ",
          "name": "step",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Controls verbosity of output. ",
          "name": "verbose",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc:33",
      "tags": [
        "feature_selection",
        "rfe"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "feature_extraction.text",
      "common_name": "Tfidf Transformer",
      "description": "u'Transform a count matrix to a normalized tf or tf-idf representation\n\nTf means term-frequency while tf-idf means term-frequency times inverse\ndocument-frequency. This is a common term weighting scheme in information\nretrieval, that has also found good use in document classification.\n\nThe goal of using tf-idf instead of the raw frequencies of occurrence of a\ntoken in a given document is to scale down the impact of tokens that occur\nvery frequently in a given corpus and that are hence empirically less\ninformative than features that occur in a small fraction of the training\ncorpus.\n\nThe formula that is used to compute the tf-idf of term t is\ntf-idf(d, t) = tf(t) * idf(d, t), and the idf is computed as\nidf(d, t) = log [ n / df(d, t) ] + 1 (if ``smooth_idf=False``),\nwhere n is the total number of documents and df(d, t) is the\ndocument frequency; the document frequency is the number of documents d\nthat contain term t. The effect of adding \"1\" to the idf in the equation\nabove is that terms with zero idf, i.e., terms  that occur in all documents\nin a training set, will not be entirely ignored.\n(Note that the idf formula above differs from the standard\ntextbook notation that defines the idf as\nidf(d, t) = log [ n / (df(d, t) + 1) ]).\n\nIf ``smooth_idf=True`` (the default), the constant \"1\" is added to the\nnumerator and denominator of the idf as if an extra document was seen\ncontaining every term in the collection exactly once, which prevents\nzero divisions: idf(d, t) = log [ (1 + n) / 1 + df(d, t) ] + 1.\n\nFurthermore, the formulas used to compute tf and idf depend\non parameter settings that correspond to the SMART notation used in IR\nas follows:\n\nTf is \"n\" (natural) by default, \"l\" (logarithmic) when\n``sublinear_tf=True``.\nIdf is \"t\" when use_idf is given, \"n\" (none) otherwise.\nNormalization is \"c\" (cosine) when ``norm=\\'l2\\'``, \"n\" (none)\nwhen ``norm=None``.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.text.TfidfTransformer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "u'Learn the idf vector (global term weights)\n",
          "id": "sklearn.feature_extraction.text.TfidfTransformer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "a matrix of term/token counts '",
              "name": "X",
              "type": "sparse"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_extraction.text.TfidfTransformer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.text.TfidfTransformer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.text.TfidfTransformer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "u'Transform a count matrix to a tf or tf-idf representation\n",
          "id": "sklearn.feature_extraction.text.TfidfTransformer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "a matrix of term/token counts ",
              "name": "X",
              "type": "sparse"
            },
            {
              "description": "Whether to copy X and operate on the copy or perform in-place operations. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "vectors",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.feature_extraction.text.TfidfTransformer",
      "parameters": [
        {
          "description": "Norm used to normalize term vectors. None for no normalization. ",
          "name": "norm",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Enable inverse-document-frequency reweighting. ",
          "name": "use_idf",
          "type": "boolean"
        },
        {
          "description": "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions. ",
          "name": "smooth_idf",
          "type": "boolean"
        },
        {
          "description": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf). ",
          "name": "sublinear_tf",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc:941",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cross_validation.cross_val_predict",
      "description": "\"Generate cross-validated estimates for each input data point\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.cross_val_predict` instead.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.cross_val_predict",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cross_validation.cross_val_predict",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like The data to fit. Can be, for example a list, or an array at least 2d. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Parameters to pass to the fit method of the estimator. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "This is the result of calling 'predict'  Examples -------- >>> from sklearn import datasets, linear_model >>> from sklearn.cross_validation import cross_val_predict >>> diabetes = datasets.load_diabetes() >>> X = diabetes.data[:150] >>> y = diabetes.target[:150] >>> lasso = linear_model.Lasso() >>> y_pred = cross_val_predict(lasso, X, y) \"",
        "name": "preds",
        "type": "ndarray"
      },
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cross_validation.check_cv",
      "description": "'Input checker utility for building a CV in a user friendly way.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.check_cv` instead.\n",
      "id": "sklearn.cross_validation.check_cv",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cross_validation.check_cv",
      "parameters": [
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if classifier is True and ``y`` is binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here.  X : array-like The data the cross-val object will be applied on. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The target variable for a supervised learning problem. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "Whether the task is a classification task, in which case stratified KFold will be used. ",
          "name": "classifier",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "The return value is guaranteed to be a cv generator instance, whatever the input type. '",
        "name": "checked_cv: a cross-validation generator instance."
      },
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.svm.bounds.l1_min_c",
      "description": "'\nReturn the lowest bound for C such that for C in (l1_min_C, infinity)\nthe model is guaranteed not to be empty. This applies to l1 penalized\nclassifiers, such as LinearSVC with penalty=\\'l1\\' and\nlinear_model.LogisticRegression with penalty=\\'l1\\'.\n\nThis value is valid if class_weight parameter in fit() is not set.\n",
      "id": "sklearn.svm.bounds.l1_min_c",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.svm.bounds.l1_min_c",
      "parameters": [
        {
          "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Target vector relative to X ",
          "name": "y",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Specifies the loss function. With \\'squared_hinge\\' it is the squared hinge loss (a.k.a. L2 loss). With \\'log\\' it is the loss of logistic regression models. \\'l2\\' is accepted as an alias for \\'squared_hinge\\', for backward compatibility reasons, but should not be used in new code. ",
          "name": "loss",
          "type": "\\'squared_hinge\\', \\'log\\'"
        },
        {
          "description": "Specifies if the intercept should be fitted by the model. It must match the fit() method parameter. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "when fit_intercept is True, instance vector x becomes [x, intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. It must match the fit() method parameter. ",
          "name": "intercept_scaling",
          "type": "float"
        }
      ],
      "returns": {
        "description": "minimum value for C '",
        "name": "l1_min_c: float"
      },
      "tags": [
        "svm",
        "bounds"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.dsolve.linsolve.splu",
      "description": "\"\nCompute the LU decomposition of a sparse, square matrix.\n",
      "id": "scipy.sparse.linalg.dsolve.linsolve.splu",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.dsolve.linsolve.splu",
      "parameters": [
        {
          "description": "Sparse matrix to factorize. Should be in CSR or CSC format.",
          "name": "A",
          "type": "sparse"
        },
        {
          "description": "How to permute the columns of the matrix for sparsity preservation. (default: 'COLAMD')  - ``NATURAL``: natural ordering. - ``MMD_ATA``: minimum degree ordering on the structure of A^T A. - ``MMD_AT_PLUS_A``: minimum degree ordering on the structure of A^T+A. - ``COLAMD``: approximate minimum degree column ordering ",
          "name": "permc_spec",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "Threshold used for a diagonal entry to be an acceptable pivot. See SuperLU user's guide for details [1]_",
          "name": "diag_pivot_thresh",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "(deprecated) No effect.",
          "name": "drop_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Expert option for customizing the degree of relaxing supernodes. See SuperLU user's guide for details [1]_",
          "name": "relax",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Expert option for customizing the panel size. See SuperLU user's guide for details [1]_",
          "name": "panel_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Dictionary containing additional expert options to SuperLU. See SuperLU user guide [1]_ (section 2.4 on the 'Options' argument) for more details. For example, you can specify ``options=dict(Equil=False, IterRefine='SINGLE'))`` to turn equilibration off and perform a single iterative refinement. ",
          "name": "options",
          "optional": "true",
          "type": "dict"
        }
      ],
      "returns": {
        "description": "Object, which has a ``solve`` method.  See also -------- spilu : incomplete LU decomposition  Notes ----- This function uses the SuperLU library.  References ---------- .. [1] SuperLU http://crd.lbl.gov/~xiaoye/SuperLU/  \"",
        "name": "invA",
        "type": "scipy"
      },
      "tags": [
        "sparse",
        "linalg",
        "dsolve",
        "linsolve"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.weighted_mode",
      "description": "\"Returns an array of the weighted modal (most common) value in a\n\nIf there is more than one such value, only the first is returned.\nThe bin-count for the modal bins is also returned.\n\nThis is an extension of the algorithm in scipy.stats.mode.\n",
      "id": "sklearn.utils.extmath.weighted_mode",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.weighted_mode",
      "parameters": [
        {
          "description": "n-dimensional array of which to find mode(s).",
          "name": "a",
          "type": "array"
        },
        {
          "description": "n-dimensional array of weights for each value",
          "name": "w",
          "type": "array"
        },
        {
          "description": "Axis along which to operate. Default is 0, i.e. the first axis. ",
          "name": "axis",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Array of modal values. score : ndarray Array of weighted counts for each mode.  Examples -------- >>> from sklearn.utils.extmath import weighted_mode >>> x = [4, 1, 4, 2, 4, 2] >>> weights = [1, 1, 1, 1, 1, 1] >>> weighted_mode(x, weights) (array([ 4.]), array([ 3.]))  The value 4 appears three times: with uniform weights, the result is simply the mode of the distribution.  >>> weights = [1, 3, 0.5, 1.5, 1, 2] # deweight the 4's >>> weighted_mode(x, weights) (array([ 2.]), array([ 3.5]))  The value 2 has the highest score: it appears twice with weights of 1.5 and 2: the sum of these is 3.  See Also -------- scipy.stats.mode \"",
        "name": "vals",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.jaccard_similarity_score",
      "description": "'Jaccard similarity coefficient score\n\nThe Jaccard index [1], or Jaccard similarity coefficient, defined as\nthe size of the intersection divided by the size of the union of two label\nsets, is used to compare set of predicted labels for a sample to the\ncorresponding set of labels in ``y_true``.\n\nRead more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "id": "sklearn.metrics.classification.jaccard_similarity_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.jaccard_similarity_score",
      "parameters": [
        {
          "description": "Ground truth (correct) labels. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Predicted labels, as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "default": "True",
          "description": "If ``False``, return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient. ",
          "name": "normalize",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "If ``normalize == True``, return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set.  The best performance is 1 with ``normalize == True`` and the number of samples with ``normalize == False``.  See also -------- accuracy_score, hamming_loss, zero_one_loss  Notes ----- In binary and multiclass classification, this function is equivalent to the ``accuracy_score``. It differs in the multilabel classification problem.  References ---------- .. [1] `Wikipedia entry for the Jaccard index <https://en.wikipedia.org/wiki/Jaccard_index>`_   Examples -------- >>> import numpy as np >>> from sklearn.metrics import jaccard_similarity_score >>> y_pred = [0, 2, 1, 3] >>> y_true = [0, 1, 2, 3] >>> jaccard_similarity_score(y_true, y_pred) 0.5 >>> jaccard_similarity_score(y_true, y_pred, normalize=False) 2  In the multilabel case with binary label indicators:  >>> jaccard_similarity_score(np.array([[0, 1], [1, 1]]),        np.ones((2, 2))) 0.75 '",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.matthews_corrcoef",
      "description": "'Compute the Matthews correlation coefficient (MCC) for binary classes\n\nThe Matthews correlation coefficient is used in machine learning as a\nmeasure of the quality of binary (two-class) classifications. It takes into\naccount true and false positives and negatives and is generally regarded as\na balanced measure which can be used even if the classes are of very\ndifferent sizes. The MCC is in essence a correlation coefficient value\nbetween -1 and +1. A coefficient of +1 represents a perfect prediction, 0\nan average random prediction and -1 an inverse prediction.  The statistic\nis also known as the phi coefficient. [source: Wikipedia]\n\nOnly in the binary case does this relate to information about true and\nfalse positives and negatives. See references below.\n\nRead more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "id": "sklearn.metrics.classification.matthews_corrcoef",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.matthews_corrcoef",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction).  References ---------- .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the accuracy of prediction algorithms for classification: an overview <http://dx.doi.org/10.1093/bioinformatics/16.5.412>`_  .. [2] `Wikipedia entry for the Matthews Correlation Coefficient <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_  Examples -------- >>> from sklearn.metrics import matthews_corrcoef >>> y_true = [+1, +1, +1, -1] >>> y_pred = [+1, -1, +1, +1] >>> matthews_corrcoef(y_true, y_pred)  # doctest: +ELLIPSIS -0.33...  '",
        "name": "mcc",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.minmax_scale",
      "description": "'Transforms features by scaling each feature to a given range.\n\nThis estimator scales and translates each feature individually such\nthat it is in the given range on the training set, i.e. between\nzero and one.\n\nThe transformation is given by::\n\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\nX_scaled = X_std * (max - min) + min\n\nwhere min, max = feature_range.\n\nThis transformation is often used as an alternative to zero mean,\nunit variance scaling.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n\n.. versionadded:: 0.17\n*minmax_scale* function interface\nto :class:`sklearn.preprocessing.MinMaxScaler`.\n",
      "id": "sklearn.preprocessing.data.minmax_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.minmax_scale",
      "parameters": [
        {
          "description": "Desired range of transformed data. ",
          "name": "feature_range",
          "type": "tuple"
        },
        {
          "description": "axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. ",
          "name": "axis",
          "type": "int"
        },
        {
          "description": "Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).  See also -------- MinMaxScaler: Performs scaling to a given range using the``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). '",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.svmlight_format.dump_svmlight_file",
      "description": "'Dump the dataset in svmlight / libsvm file format.\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n",
      "id": "sklearn.datasets.svmlight_format.dump_svmlight_file",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.svmlight_format.dump_svmlight_file",
      "parameters": [
        {
          "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Target values. Class labels must be an integer or float, or array-like objects of integer or float for multilabel classifications. ",
          "name": "y",
          "shape": "n_samples (, n_labels",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode. ",
          "name": "f",
          "type": "string"
        },
        {
          "description": "Whether column indices should be written zero-based (True) or one-based (False). ",
          "name": "zero_based",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Comment to insert at the top of the file. This should be either a Unicode string, which will be encoded as UTF-8, or an ASCII byte string. If a comment is given, then it will be preceded by one that identifies the file as having been dumped by scikit-learn. Note that not all tools grok comments in SVMlight files. ",
          "name": "comment",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Array containing pairwise preference constraints (qid in svmlight format). ",
          "name": "query_id",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)  .. versionadded:: 0.17 parameter *multilabel* to support multilabel datasets. '",
          "name": "multilabel",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "datasets",
        "svmlight_format"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_biclusters",
      "description": "'Generate an array with constant block diagonal structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_biclusters",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_biclusters",
      "parameters": [
        {
          "description": "The shape of the result. ",
          "name": "shape",
          "type": "iterable"
        },
        {
          "description": "The number of biclusters. ",
          "name": "n_clusters",
          "type": "integer"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "10",
          "description": "Minimum value of a bicluster. ",
          "name": "minval",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "100",
          "description": "Maximum value of a bicluster. ",
          "name": "maxval",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Shuffle the samples. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated array.  rows : array of shape (n_clusters, X.shape[0],) The indicators for cluster membership of each row.  cols : array of shape (n_clusters, X.shape[1],) The indicators for cluster membership of each column.  References ----------  .. [1] Dhillon, I. S. (2001, August). Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 269-274). ACM.  See also -------- make_checkerboard '",
        "name": "X",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_gaussian_quantiles",
      "description": "'Generate isotropic Gaussian and label samples by quantile\n\nThis classification dataset is constructed by taking a multi-dimensional\nstandard normal distribution and defining classes separated by nested\nconcentric multi-dimensional spheres such that roughly equal numbers of\nsamples are in each class (quantiles of the :math:`\\\\chi^2` distribution).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_gaussian_quantiles",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_gaussian_quantiles",
      "parameters": [
        {
          "default": "None",
          "description": "The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, ...). ",
          "name": "mean",
          "optional": "true",
          "shape": "n_features",
          "type": "array"
        },
        {
          "default": "1.",
          "description": "The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions. ",
          "name": "cov",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "100",
          "description": "The total number of points equally divided among classes. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of features for each sample. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "3",
          "description": "The number of classes ",
          "name": "n_classes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Shuffle the samples. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated samples.  y : array of shape [n_samples] The integer labels for quantile membership of each sample.  Notes ----- The dataset is from Zhu et al [1].  References ---------- .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.  '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_sparse_spd_matrix",
      "description": "'Generate a sparse symmetric definite positive matrix.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_sparse_spd_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_sparse_spd_matrix",
      "parameters": [
        {
          "default": "1",
          "description": "The size of the random matrix to generate. ",
          "name": "dim",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "0.95",
          "description": "The probability that a coefficient is zero (see notes). Larger values enforce more sparsity. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.9",
          "description": "The value of the largest coefficient. ",
          "name": "largest_coef",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "0.1",
          "description": "The value of the smallest coefficient. ",
          "name": "smallest_coef",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether to normalize the output matrix to make the leading diagonal elements all 1 ",
          "name": "norm_diag",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "The generated matrix.  Notes ----- The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.  See also -------- make_spd_matrix '",
        "name": "prec",
        "shape": "dim, dim",
        "type": "sparse"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.basic.solve_triangular",
      "description": "\"\nSolve the equation `a x = b` for `x`, assuming a is a triangular matrix.\n",
      "id": "scipy.linalg.basic.solve_triangular",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.basic.solve_triangular",
      "parameters": [
        {
          "description": "A triangular matrix",
          "name": "a",
          "type": ""
        },
        {
          "description": "Right-hand side matrix in `a x = b`",
          "name": "b",
          "type": ""
        },
        {
          "description": "Use only data contained in the lower triangle of `a`. Default is to use upper triangle.",
          "name": "lower",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Type of system to solve:  ========  ========= trans     system ========  ========= 0 or 'N'  a x  = b 1 or 'T'  a^T x = b 2 or 'C'  a^H x = b ========  =========",
          "name": "trans",
          "optional": "true",
          "type": "0, 1, 2, 'N', 'T', 'C'"
        },
        {
          "description": "If True, diagonal elements of `a` are assumed to be 1 and will not be referenced.",
          "name": "unit_diagonal",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Allow overwriting data in `b` (may enhance performance)",
          "name": "overwrite_b",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrices contain only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Solution to the system `a x = b`.  Shape of return matches `b`.  Raises ------ LinAlgError If `a` is singular  Notes ----- .. versionadded:: 0.9.0  \"",
        "name": "x",
        "type": ""
      },
      "tags": [
        "linalg",
        "basic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.image.extract_patches_2d",
      "description": "'Reshape a 2D image into a collection of patches\n\nThe resulting patches are allocated in a dedicated array.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.image.extract_patches_2d",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.image.extract_patches_2d",
      "parameters": [
        {
          "description": "(image_height, image_width, n_channels) The original image data. For color images, the last dimension specifies the channel: a RGB image would have `n_channels=3`. ",
          "name": "image",
          "shape": "image_height, image_width",
          "type": "array"
        },
        {
          "description": "the dimensions of one patch ",
          "name": "patch_size",
          "type": "tuple"
        },
        {
          "description": "The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches. ",
          "name": "max_patches",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Pseudo number generator state used for random sampling to use if `max_patches` is not None. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "returns": {
        "description": "(n_patches, patch_height, patch_width, n_channels) The collection of patches extracted from the image, where `n_patches` is either `max_patches` or the total number of patches that can be extracted.  Examples --------  >>> from sklearn.feature_extraction import image >>> one_image = np.arange(16).reshape((4, 4)) >>> one_image array([[ 0,  1,  2,  3], [ 4,  5,  6,  7], [ 8,  9, 10, 11], [12, 13, 14, 15]]) >>> patches = image.extract_patches_2d(one_image, (2, 2)) >>> print(patches.shape) (9, 2, 2) >>> patches[0] array([[0, 1], [4, 5]]) >>> patches[1] array([[1, 2], [5, 6]]) >>> patches[8] array([[10, 11], [14, 15]]) '",
        "name": "patches",
        "shape": "n_patches, patch_height, patch_width",
        "type": "array"
      },
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_digits",
      "description": "\"Load and return the digits dataset (classification).\n\nEach datapoint is a 8x8 image of a digit.\n\n=================   ==============\nClasses                         10\nSamples per class             ~180\nSamples total                 1797\nDimensionality                  64\nFeatures             integers 0-16\n=================   ==============\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.base.load_digits",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_digits",
      "parameters": [
        {
          "default": "10",
          "description": "The number of classes to return.  return_X_y : boolean, default=False. If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "n_class",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn, 'images', the images corresponding to each sample, 'target', the classification labels for each sample, 'target_names', the meaning of the labels, and 'DESCR', the full description of the dataset.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18  Examples -------- To load the data and visualize the images::  >>> from sklearn.datasets import load_digits >>> digits = load_digits() >>> print(digits.data.shape) (1797, 64) >>> import matplotlib.pyplot as plt #doctest: +SKIP >>> plt.gray() #doctest: +SKIP >>> plt.matshow(digits.images[0]) #doctest: +SKIP >>> plt.show() #doctest: +SKIP \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "The numpy array which backs the weight vector.",
          "name": "w",
          "type": "ndarray"
        },
        {
          "description": "The numpy array which backs the average_weight vector.",
          "name": "aw",
          "type": "ndarray"
        },
        {
          "description": "A pointer to the data of the numpy array.",
          "name": "w_data_ptr",
          "type": "double"
        },
        {
          "description": "The scale of the vector.",
          "name": "wscale",
          "type": "double"
        },
        {
          "description": "The number of features (= dimensionality of ``w``).",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "The squared norm of ``w``.",
          "name": "sq_norm",
          "type": "double"
        }
      ],
      "category": "utils.weight_vector",
      "common_name": "Weight Vector",
      "description": "\"Dense vector represented by a scalar and a numpy array.\n\nThe class provides methods to ``add`` a sparse vector\nand scale the vector.\nRepresenting a vector explicitly as a scalar times a\nvector allows for efficient scaling operations.\n\nAttributes\n----------\nw : ndarray, dtype=double, order='C'\nThe numpy array which backs the weight vector.\naw : ndarray, dtype=double, order='C'\nThe numpy array which backs the average_weight vector.\nw_data_ptr : double*\nA pointer to the data of the numpy array.\nwscale : double\nThe scale of the vector.\nn_features : int\nThe number of features (= dimensionality of ``w``).\nsq_norm : double\nThe squared norm of ``w``.\n\"",
      "id": "sklearn.utils.weight_vector.WeightVector",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.utils.weight_vector.WeightVector",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "utils",
        "weight_vector"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.externals.joblib.numpy_pickle.dump",
      "description": "\"Persist an arbitrary Python object into one file.\n",
      "id": "sklearn.externals.joblib.numpy_pickle.dump",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.externals.joblib.numpy_pickle.dump",
      "parameters": [
        {
          "description": "The object to store to disk. filename: str or pathlib.Path The path of the file in which it is to be stored. The compression method corresponding to one of the supported filename extensions ('.z', '.gz', '.bz2', '.xz' or '.lzma') will be used automatically. compress: int from 0 to 9 or bool or 2-tuple, optional Optional compression level for the data. 0 or False is no compression. Higher value means more compression, but also slower read and write times. Using a value of 3 is often a good compromise. See the notes for more details. If compress is True, the compression level used is 3. If compress is a 2-tuple, the first element must correspond to a string between supported compressors (e.g 'zlib', 'gzip', 'bz2', 'lzma' 'xz'), the second element must be an integer from 0 to 9, corresponding to the compression level. protocol: positive int Pickle protocol, see pickle.dump documentation for more details. cache_size: positive int, optional This option is deprecated in 0.10 and has no effect. ",
          "name": "value",
          "type": "any"
        }
      ],
      "returns": {
        "description": "The list of file names in which the data is stored. If compress is false, each array is stored in a different file.  See Also -------- joblib.load : corresponding loader  Notes ----- Memmapping on load cannot be used for compressed files. Thus using compression can significantly slow down loading. In addition, compressed files take extra extra memory during dump and load.  \"",
        "name": "filenames: list of strings"
      },
      "tags": [
        "externals",
        "joblib",
        "numpy_pickle"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_extraction.image.extract_patches",
      "description": "'Extracts patches of any n-dimensional array in place using strides.\n\nGiven an n-dimensional array it will return a 2n-dimensional array with\nthe first n dimensions indexing patch position and the last n indexing\nthe patch content. This operation is immediate (O(1)). A reshape\nperformed on the first n dimensions will cause numpy to copy data, leading\nto a list of extracted patches.\n\nRead more in the :ref:`User Guide <image_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.image.extract_patches",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_extraction.image.extract_patches",
      "parameters": [
        {
          "description": "n-dimensional array of which patches are to be extracted ",
          "name": "arr",
          "type": "ndarray"
        },
        {
          "description": "Indicates the shape of the patches to be extracted. If an integer is given, the shape will be a hypercube of sidelength given by its value. ",
          "name": "patch_shape",
          "type": "integer"
        },
        {
          "description": "Indicates step size at which extraction shall be performed. If integer is given, then the step is uniform in all dimensions.  ",
          "name": "extraction_step",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "2n-dimensional array indexing patches on first n dimensions and containing patches on the last n dimensions. These dimensions are fake, but this way no data is copied. A simple reshape invokes a copying operation to obtain a list of patches: result.reshape([-1] + list(patch_shape)) '",
        "name": "patches",
        "type": "strided"
      },
      "tags": [
        "feature_extraction",
        "image"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.t_sne.trustworthiness",
      "description": "'Expresses to what extent the local structure is retained.\n\nThe trustworthiness is within [0, 1]. It is defined as\n\n.. math::\n\nT(k) = 1 - \\x0crac{2}{nk (2n - 3k - 1)} \\\\sum^n_{i=1}\n\\\\sum_{j \\\\in U^{(k)}_i (r(i, j) - k)}\n\nwhere :math:`r(i, j)` is the rank of the embedded datapoint j\naccording to the pairwise distances between the embedded datapoints,\n:math:`U^{(k)}_i` is the set of points that are in the k nearest\nneighbors in the embedded space but not in the original space.\n\n* \"Neighborhood Preservation in Nonlinear Projection Methods: An\nExperimental Study\"\nJ. Venna, S. Kaski\n* \"Learning a Parametric Embedding by Preserving Local Structure\"\nL.J.P. van der Maaten\n",
      "id": "sklearn.manifold.t_sne.trustworthiness",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.t_sne.trustworthiness",
      "parameters": [
        {
          "description": "If the metric is \\'precomputed\\' X must be a square distance matrix. Otherwise it contains a sample per row.  X_embedded : array, shape (n_samples, n_components) Embedding of the training data in low-dimensional space. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Number of neighbors k that will be considered. ",
          "name": "n_neighbors",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Set this flag if X is a precomputed square distance matrix. ",
          "name": "precomputed",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Trustworthiness of the low-dimensional embedding. '",
        "name": "trustworthiness",
        "type": "float"
      },
      "tags": [
        "manifold",
        "t_sne"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.completeness_score",
      "description": "\"Completeness metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`homogeneity_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "id": "sklearn.metrics.cluster.supervised.completeness_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.completeness_score",
      "parameters": [
        {
          "description": "ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling  References ----------  .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_  See also -------- homogeneity_score v_measure_score  Examples --------  Perfect labelings are complete::  >>> from sklearn.metrics.cluster import completeness_score >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  Non-perfect labelings that assign all classes members to the same clusters are still complete::  >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0])) 1.0 >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1])) 1.0  If classes members are split across different clusters, the assignment cannot be complete::  >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1])) 0.0 >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3])) 0.0  \"",
        "name": "completeness: float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.additive_chi2_kernel",
      "description": "'Computes the additive chi-squared kernel between observations in X and Y\n\nThe chi-squared kernel is computed between each pair of rows in X and Y.  X\nand Y have to be non-negative. This kernel is most commonly applied to\nhistograms.\n\nThe chi-squared kernel is given by::\n\nk(x, y) = -Sum [(x - y)^2 / (x + y)]\n\nIt can be interpreted as a weighted difference per entry.\n\nRead more in the :ref:`User Guide <chi2_kernel>`.\n\nNotes\n-----\nAs the negative of a distance, this kernel is only conditionally positive\ndefinite.\n\n",
      "id": "sklearn.metrics.pairwise.additive_chi2_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.additive_chi2_kernel",
      "parameters": [
        {
          "description": " Y : array of shape (n_samples_Y, n_features) ",
          "name": "X",
          "shape": "n_samples_X, n_features",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf   See also -------- chi2_kernel : The exponentiated version of the kernel, which is usually preferable.  sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation to this kernel. '",
        "name": "kernel_matrix",
        "shape": "n_samples_X, n_samples_Y",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.check_pairwise_arrays",
      "description": "' Set X and Y appropriately and checks inputs\n\nIf Y is None, it is set as a pointer to X (i.e. not a copy).\nIf Y is given, this does not happen.\nAll distance metrics should use this function first to assert that the\ngiven parameters are correct and safe to use.\n\nSpecifically, this function first ensures that both X and Y are arrays,\nthen checks that they are at least two dimensional while ensuring that\ntheir elements are floats (or dtype if provided). Finally, the function\nchecks that the size of the second dimension of the two arrays is equal, or\nthe equivalent check for a precomputed distance matrix.\n",
      "id": "sklearn.metrics.pairwise.check_pairwise_arrays",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.check_pairwise_arrays",
      "parameters": [
        {
          "description": " Y : {array-like, sparse matrix}, shape (n_samples_b, n_features) ",
          "name": "X",
          "shape": "n_samples_a, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "True if X is to be treated as precomputed distances to the samples in Y. ",
          "name": "precomputed",
          "type": "bool"
        },
        {
          "description": "Data type required for X and Y. If None, the dtype will be an appropriate float type selected by _return_float_dtype.  .. versionadded:: 0.18 ",
          "name": "dtype",
          "type": "string"
        }
      ],
      "returns": {
        "description": "An array equal to X, guaranteed to be a numpy array.  safe_Y : {array-like, sparse matrix}, shape (n_samples_b, n_features) An array equal to Y if Y was not None, guaranteed to be a numpy array. If Y was None, safe_Y will be a pointer to X.  '",
        "name": "safe_X",
        "shape": "n_samples_a, n_features",
        "type": "array-like, sparse matrix"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.average_precision_score",
      "description": "'Compute average precision (AP) from prediction scores\n\nThis score corresponds to the area under the precision-recall curve.\n\nNote: this implementation is restricted to the binary classification task\nor multilabel classification task.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.ranking.average_precision_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.average_precision_score",
      "parameters": [
        {
          "description": "True binary labels in binary label indicators. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``\\'micro\\'``: Calculate metrics globally by considering each element of the label indicator matrix as a label. ``\\'macro\\'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``\\'weighted\\'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). ``\\'samples\\'``: Calculate metrics for each instance, and find their average. ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- .. [1] `Wikipedia entry for the Average precision <https://en.wikipedia.org/wiki/Average_precision>`_  See also -------- roc_auc_score : Area under the ROC curve  precision_recall_curve : Compute precision-recall pairs for different probability thresholds  Examples -------- >>> import numpy as np >>> from sklearn.metrics import average_precision_score >>> y_true = np.array([0, 0, 1, 1]) >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> average_precision_score(y_true, y_scores)  # doctest: +ELLIPSIS 0.79...  '",
        "name": "average_precision",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.scorer.make_scorer",
      "description": "\"Make a scorer from a performance metric or loss function.\n\nThis factory function wraps scoring functions for use in GridSearchCV\nand cross_val_score. It takes a score function, such as ``accuracy_score``,\n``mean_squared_error``, ``adjusted_rand_index`` or ``average_precision``\nand returns a callable that scores an estimator's output.\n\nRead more in the :ref:`User Guide <scoring>`.\n",
      "id": "sklearn.metrics.scorer.make_scorer",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.scorer.make_scorer",
      "parameters": [
        {
          "description": "Score function (or loss function) with signature ``score_func(y, y_pred, **kwargs)``. ",
          "name": "score_func",
          "type": "callable"
        },
        {
          "description": "Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func. ",
          "name": "greater_is_better",
          "type": "boolean"
        },
        {
          "description": "Whether score_func requires predict_proba to get probability estimates out of a classifier. ",
          "name": "needs_proba",
          "type": "boolean"
        },
        {
          "description": "Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method.  For example ``average_precision`` or the area under the roc curve can not be computed using discrete predictions alone.  **kwargs : additional arguments Additional parameters to be passed to score_func. ",
          "name": "needs_threshold",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Callable object that returns a scalar score; greater is better.  Examples -------- >>> from sklearn.metrics import fbeta_score, make_scorer >>> ftwo_scorer = make_scorer(fbeta_score, beta=2) >>> ftwo_scorer make_scorer(fbeta_score, beta=2) >>> from sklearn.model_selection import GridSearchCV >>> from sklearn.svm import LinearSVC >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]}, ...                     scoring=ftwo_scorer) \"",
        "name": "scorer",
        "type": "callable"
      },
      "tags": [
        "metrics",
        "scorer"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.roc_auc_score",
      "description": "'Compute Area Under the Curve (AUC) from prediction scores\n\nNote: this implementation is restricted to the binary classification task\nor multilabel classification task in label indicator format.\n\nRead more in the :ref:`User Guide <roc_metrics>`.\n",
      "id": "sklearn.metrics.ranking.roc_auc_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.roc_auc_score",
      "parameters": [
        {
          "description": "True binary labels in binary label indicators. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers). ",
          "name": "y_score",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``\\'micro\\'``: Calculate metrics globally by considering each element of the label indicator matrix as a label. ``\\'macro\\'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``\\'weighted\\'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). ``\\'samples\\'``: Calculate metrics for each instance, and find their average. ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " References ---------- .. [1] `Wikipedia entry for the Receiver operating characteristic <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_  See also -------- average_precision_score : Area under the precision-recall curve  roc_curve : Compute Receiver operating characteristic (ROC)  Examples -------- >>> import numpy as np >>> from sklearn.metrics import roc_auc_score >>> y_true = np.array([0, 0, 1, 1]) >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> roc_auc_score(y_true, y_scores) 0.75  '",
        "name": "auc",
        "type": "float"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.zero_one_loss",
      "description": "'Zero-one classification loss.\n\nIf normalize is ``True``, return the fraction of misclassifications\n(float), else it returns the number of misclassifications (int). The best\nperformance is 0.\n\nRead more in the :ref:`User Guide <zero_one_loss>`.\n",
      "id": "sklearn.metrics.classification.zero_one_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.zero_one_loss",
      "parameters": [
        {
          "description": "Ground truth (correct) labels. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Predicted labels, as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "default": "True",
          "description": "If ``False``, return the number of misclassifications. Otherwise, return the fraction of misclassifications. ",
          "name": "normalize",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "If ``normalize == True``, return the fraction of misclassifications (float), else it returns the number of misclassifications (int).  Notes ----- In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one.  See also -------- accuracy_score, hamming_loss, jaccard_similarity_score  Examples -------- >>> from sklearn.metrics import zero_one_loss >>> y_pred = [1, 2, 3, 4] >>> y_true = [2, 2, 3, 4] >>> zero_one_loss(y_true, y_pred) 0.25 >>> zero_one_loss(y_true, y_pred, normalize=False) 1  In the multilabel case with binary label indicators:  >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 0.5 '",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.randomized_l1.lasso_stability_path",
      "description": "\"Stability path based on randomized Lasso estimates\n\nRead more in the :ref:`User Guide <randomized_l1>`.\n",
      "id": "sklearn.linear_model.randomized_l1.lasso_stability_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.randomized_l1.lasso_stability_path",
      "parameters": [
        {
          "description": "training data. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "target values. ",
          "name": "y",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1. ",
          "name": "scaling",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The generator used to randomize the design. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of randomized models. ",
          "name": "n_resampling",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of grid points. The path is linearly reinterpolated on a grid between 0 and 1 before computing the scores. ",
          "name": "n_grid",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. ",
          "name": "sample_fraction",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Smallest value of alpha / alpha_max considered ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of CPUs to use during the resampling. If '-1', use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "The grid points between 0 and 1: alpha/alpha_max  scores_path : array, shape = [n_features, n_grid] The scores for each feature along the path.  Notes ----- See examples/linear_model/plot_sparse_recovery.py for an example. \"",
        "name": "alphas_grid",
        "shape": "n_grid",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "randomized_l1"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.affinity_propagation_.affinity_propagation",
      "description": "'Perform Affinity Propagation Clustering of data\n\nRead more in the :ref:`User Guide <affinity_propagation>`.\n",
      "id": "sklearn.cluster.affinity_propagation_.affinity_propagation",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.affinity_propagation_.affinity_propagation",
      "parameters": [
        {
          "description": "Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, i.e. of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities (resulting in a moderate number of clusters). For a smaller amount of clusters, this can be set to the minimum value of the similarities. ",
          "name": "preference",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Number of iterations with no change in the number of estimated clusters that stops the convergence. ",
          "name": "convergence_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Damping factor between 0.5 and 1. ",
          "name": "damping",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The verbosity level ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "cluster_centers_indices : array, shape (n_clusters,) index of clusters centers  labels : array, shape (n_samples,) cluster labels for each point  n_iter : int number of iterations run. Returned only if `return_n_iter` is set to True.  Notes ----- See examples/cluster/plot_affinity_propagation.py for an example.  References ---------- Brendan J. Frey and Delbert Dueck, \"Clustering by Passing Messages Between Data Points\", Science Feb. 2007 '",
        "name": ""
      },
      "tags": [
        "cluster",
        "affinity_propagation_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_checkerboard",
      "description": "'Generate an array with block checkerboard structure for\nbiclustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_checkerboard",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_checkerboard",
      "parameters": [
        {
          "description": "The shape of the result. ",
          "name": "shape",
          "type": "iterable"
        },
        {
          "description": "The number of row and column clusters. ",
          "name": "n_clusters",
          "type": "integer"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "10",
          "description": "Minimum value of a bicluster. ",
          "name": "minval",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "100",
          "description": "Maximum value of a bicluster. ",
          "name": "maxval",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Shuffle the samples. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated array.  rows : array of shape (n_clusters, X.shape[0],) The indicators for cluster membership of each row.  cols : array of shape (n_clusters, X.shape[1],) The indicators for cluster membership of each column.   References ----------  .. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003). Spectral biclustering of microarray data: coclustering genes and conditions. Genome research, 13(4), 703-716.  See also -------- make_biclusters '",
        "name": "X",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.graph_lasso_.graph_lasso_path",
      "description": "\"l1-penalized covariance estimator along a path of decreasing alphas\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n",
      "id": "sklearn.covariance.graph_lasso_.graph_lasso_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.graph_lasso_.graph_lasso_path",
      "parameters": [
        {
          "description": "Data from which to compute the covariance estimate. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": ""
        },
        {
          "description": "The list of regularization parameters, decreasing order.  X_test : 2D array, shape (n_test_samples, n_features), optional Optional test matrix to measure generalisation error. ",
          "name": "alphas",
          "type": "list"
        },
        {
          "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable. ",
          "name": "mode",
          "type": "'cd', 'lars'"
        },
        {
          "description": "The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. ",
          "name": "tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. ",
          "name": "enet_tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The maximum number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The higher the verbosity flag, the more information is printed during the fitting. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "The estimated covariance matrices.  precisions_ : List of 2D ndarray, shape (n_features, n_features) The estimated (sparse) precision matrices.  scores_ : List of float The generalisation error (log-likelihood) on the test data. Returned only if test data is passed. \"",
        "name": "covariances_",
        "shape": "n_features, n_features",
        "type": ""
      },
      "tags": [
        "covariance",
        "graph_lasso_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.multiclass.type_of_target",
      "description": "\"Determine the type of data indicated by target `y`\n",
      "id": "sklearn.utils.multiclass.type_of_target",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.multiclass.type_of_target",
      "parameters": [
        {
          "description": "",
          "name": "y",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "One of: * 'continuous': `y` is an array-like of floats that are not all integers, and is 1d or a column vector. * 'continuous-multioutput': `y` is a 2d array of floats that are not all integers, and both dimensions are of size > 1. * 'binary': `y` contains <= 2 discrete values and is 1d or a column vector. * 'multiclass': `y` contains more than two discrete values, is not a sequence of sequences, and is 1d or a column vector. * 'multiclass-multioutput': `y` is a 2d array that contains more than two discrete values, is not a sequence of sequences, and both dimensions are of size > 1. * 'multilabel-indicator': `y` is a label indicator matrix, an array of two dimensions with at least two columns, and at most 2 unique values. * 'unknown': `y` is array-like but none of the above, such as a 3d array, sequence of sequences, or an array of non-sequence objects.  Examples -------- >>> import numpy as np >>> type_of_target([0.1, 0.6]) 'continuous' >>> type_of_target([1, -1, -1, 1]) 'binary' >>> type_of_target(['a', 'b', 'a']) 'binary' >>> type_of_target([1.0, 2.0]) 'binary' >>> type_of_target([1, 0, 2]) 'multiclass' >>> type_of_target([1.0, 0.0, 3.0]) 'multiclass' >>> type_of_target(['a', 'b', 'c']) 'multiclass' >>> type_of_target(np.array([[1, 2], [3, 1]])) 'multiclass-multioutput' >>> type_of_target([[1, 2]]) 'multiclass-multioutput' >>> type_of_target(np.array([[1.5, 2.0], [3.0, 1.6]])) 'continuous-multioutput' >>> type_of_target(np.array([[0, 1], [1, 1]])) 'multilabel-indicator' \"",
        "name": "target_type",
        "type": "string"
      },
      "tags": [
        "utils",
        "multiclass"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.label.label_binarize",
      "description": "\"Binarize labels in a one-vs-all fashion\n\nSeveral regression and binary classification algorithms are\navailable in the scikit. A simple way to extend these algorithms\nto the multi-class classification case is to use the so-called\none-vs-all scheme.\n\nThis function makes it possible to compute this transformation for a\nfixed set of class labels known ahead of time.\n",
      "id": "sklearn.preprocessing.label.label_binarize",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.label.label_binarize",
      "parameters": [
        {
          "description": "Sequence of integer labels or multilabel data to encode. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "Uniquely holds the label for each class. ",
          "name": "classes",
          "shape": "n_classes",
          "type": "array-like"
        },
        {
          "description": "Value with which negative labels must be encoded. ",
          "name": "neg_label",
          "type": "int"
        },
        {
          "description": "Value with which positive labels must be encoded. ",
          "name": "pos_label",
          "type": "int"
        },
        {
          "description": "Set to true if output binary array is desired in CSR sparse format ",
          "name": "sparse_output",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Shape will be [n_samples, 1] for binary problems.  Examples -------- >>> from sklearn.preprocessing import label_binarize >>> label_binarize([1, 6], classes=[1, 2, 4, 6]) array([[1, 0, 0, 0], [0, 0, 0, 1]])  The class ordering is preserved:  >>> label_binarize([1, 6], classes=[1, 6, 4, 2]) array([[1, 0, 0, 0], [0, 1, 0, 0]])  Binary targets transform to a column vector  >>> label_binarize(['yes', 'no', 'no', 'yes'], classes=['no', 'yes']) array([[1], [0], [0], [1]])  See also -------- LabelBinarizer : class used to wrap the functionality of label_binarize and allow for fitting to classes independently of the transform operation \"",
        "name": "Y",
        "shape": "n_samples, n_classes",
        "type": "numpy"
      },
      "tags": [
        "preprocessing",
        "label"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.robust_scale",
      "description": "'Standardize a dataset along any axis\n\nCenter to the median and component wise scale\naccording to the interquartile range.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "id": "sklearn.preprocessing.data.robust_scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.robust_scale",
      "parameters": [
        {
          "description": "The data to center and scale. ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "axis used to compute the medians and IQR along. If 0, independently scale each feature, otherwise (if 1) scale each sample. ",
          "name": "axis",
          "type": "int"
        },
        {
          "description": "If True, center the data before scaling. ",
          "name": "with_centering",
          "type": "boolean"
        },
        {
          "description": "If True, scale the data to unit variance (or equivalently, unit standard deviation). ",
          "name": "with_scaling",
          "type": "boolean"
        },
        {
          "description": "Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate ``scale_``.  .. versionadded:: 0.18 ",
          "name": "quantile_range",
          "type": "tuple"
        },
        {
          "description": "set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1).  Notes ----- This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.  Instead the caller is expected to either set explicitly `with_centering=False` (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call `X.toarray()` if he/she expects the materialized dense array to fit in memory.  To avoid memory copy the caller should pass a CSR matrix.  See also -------- RobustScaler: Performs centering and scaling using the ``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). '",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.preprocessing.data.scale",
      "description": "'Standardize a dataset along any axis\n\nCenter to the mean and component wise scale to unit variance.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "id": "sklearn.preprocessing.data.scale",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.preprocessing.data.scale",
      "parameters": [
        {
          "description": "The data to center and scale. ",
          "name": "X",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample. ",
          "name": "axis",
          "type": "int"
        },
        {
          "description": "If True, center the data before scaling. ",
          "name": "with_mean",
          "type": "boolean"
        },
        {
          "description": "If True, scale the data to unit variance (or equivalently, unit standard deviation). ",
          "name": "with_std",
          "type": "boolean"
        },
        {
          "description": "set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSC matrix and if axis is 1).  Notes ----- This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems.  Instead the caller is expected to either set explicitly `with_mean=False` (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call `X.toarray()` if he/she expects the materialized dense array to fit in memory.  To avoid memory copy the caller should pass a CSC matrix.  See also -------- StandardScaler: Performs scaling to unit variance using the``Transformer`` API (e.g. as part of a preprocessing :class:`sklearn.pipeline.Pipeline`). '",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "tags": [
        "preprocessing",
        "data"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The training data ",
          "name": "data",
          "type": "np"
        }
      ],
      "category": "neighbors.kd_tree",
      "common_name": "KD Tree",
      "description": "\"KDTree for fast generalized N-point problems\n\nKDTree(X, leaf_size=40, metric='minkowski', \\\\**kwargs)\n",
      "id": "sklearn.neighbors.kd_tree.KDTree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.kd_tree.KDTree",
      "parameters": [
        {
          "description": "n_samples is the number of points in the data set, and n_features is the dimension of the parameter space. Note: if X is a C-contiguous array of doubles then data will not be copied. Otherwise, an internal copy will be made. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.  The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified ``leaf_size``, a leaf node is guaranteed to satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in the case that ``n_samples < leaf_size``. ",
          "name": "leaf_size",
          "type": "positive"
        },
        {
          "description": "the distance metric to use for the tree.  Default='minkowski' with p=2 (that is, a euclidean metric). See the documentation of the DistanceMetric class for a list of available metrics. kd_tree.valid_metrics gives a list of the metrics which are valid for KDTree.  Additional keywords are passed to the distance metric class. ",
          "name": "metric",
          "type": "string"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "kd_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.optimize.linesearch.line_search_wolfe2",
      "description": "\"Find alpha that satisfies strong Wolfe conditions.\n",
      "id": "scipy.optimize.linesearch.line_search_wolfe2",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.optimize.linesearch.line_search_wolfe2",
      "parameters": [
        {
          "description": "Objective function.",
          "name": "f",
          "type": "callable"
        },
        {
          "description": "Objective function gradient.",
          "name": "myfprime",
          "type": "callable"
        },
        {
          "description": "Starting point.",
          "name": "xk",
          "type": "ndarray"
        },
        {
          "description": "Search direction.",
          "name": "pk",
          "type": "ndarray"
        },
        {
          "description": "Gradient value for x=xk (xk being the current parameter estimate). Will be recomputed if omitted.",
          "name": "gfk",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "Function value for x=xk. Will be recomputed if omitted.",
          "name": "old_fval",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Function value for the point preceding x=xk",
          "name": "old_old_fval",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Additional arguments passed to objective function. c1 : float, optional Parameter for Armijo condition rule. c2 : float, optional Parameter for curvature condition rule.",
          "name": "args",
          "optional": "true",
          "type": "tuple"
        },
        {
          "description": "Maximum step size ",
          "name": "amax",
          "optional": "true",
          "type": "float"
        }
      ],
      "returns": {
        "description": "Alpha for which ``x_new = x0 + alpha * pk``, or None if the line search algorithm did not converge. fc : int Number of function evaluations made. gc : int Number of gradient evaluations made. new_fval : float or None New function value ``f(x_new)=f(x0+alpha*pk)``, or None if the line search algorithm did not converge. old_fval : float Old function value ``f(x0)``. new_slope : float or None The local slope along the search direction at the new value ``<myfprime(x_new), pk>``, or None if the line search algorithm did not converge.   Notes ----- Uses the line search algorithm to enforce strong Wolfe conditions.  See Wright and Nocedal, 'Numerical Optimization', 1999, pg. 59-60.  For the zoom phase it uses an algorithm by [...].  \"",
        "name": "alpha",
        "type": "float"
      },
      "tags": [
        "optimize",
        "linesearch"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.resample",
      "description": "\"Resample arrays or sparse matrices in a consistent way\n\nThe default strategy implements one step of the bootstrapping\nprocedure.\n",
      "id": "sklearn.utils.resample",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.resample",
      "parameters": [
        {
          "description": "Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension. ",
          "name": "*arrays",
          "type": "sequence"
        },
        {
          "description": "Implements resampling with replacement. If False, this will implement (sliced) random permutations. ",
          "name": "replace",
          "type": "boolean"
        },
        {
          "description": "Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. If replace is False it should not be larger than the length of arrays. ",
          "name": "n_samples",
          "type": "int"
        },
        {
          "description": "Control the shuffling for reproducible behavior. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Sequence of resampled views of the collections. The original arrays are not impacted.  Examples -------- It is possible to mix sparse and dense arrays in the same run::  >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]]) >>> y = np.array([0, 1, 2])  >>> from scipy.sparse import coo_matrix >>> X_sparse = coo_matrix(X)  >>> from sklearn.utils import resample >>> X, X_sparse, y = resample(X, X_sparse, y, random_state=0) >>> X array([[ 1.,  0.], [ 2.,  1.], [ 1.,  0.]])  >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE <3x2 sparse matrix of type '<... 'numpy.float64'>' with 4 stored elements in Compressed Sparse Row format>  >>> X_sparse.toarray() array([[ 1.,  0.], [ 2.,  1.], [ 1.,  0.]])  >>> y array([0, 1, 0])  >>> resample(y, n_samples=2, random_state=0) array([0, 1])   See also -------- :func:`sklearn.utils.shuffle` \"",
        "name": "resampled_arrays",
        "type": "sequence"
      },
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_friedman2",
      "description": "'Generate the \"Friedman \\\\#2\" regression problem\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n0 <= X[:, 0] <= 100,\n40 * pi <= X[:, 1] <= 560 * pi,\n0 <= X[:, 2] <= 1,\n1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\ny(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_friedman2",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_friedman2",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise applied to the output. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] The output values.  References ---------- .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals of Statistics 19 (1), pages 1-67, 1991.  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24, pages 123-140, 1996. '",
        "name": "X",
        "shape": "n_samples, 4",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_breast_cancer",
      "description": "\"Load and return the breast cancer wisconsin dataset (classification).\n\nThe breast cancer dataset is a classic and very easy binary classification\ndataset.\n\n=================   ==============\nClasses                          2\nSamples per class    212(M),357(B)\nSamples total                  569\nDimensionality                  30\nFeatures            real, positive\n=================   ==============\n",
      "id": "sklearn.datasets.base.load_breast_cancer",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_breast_cancer",
      "parameters": [
        {
          "description": "If True, returns ``(data, target)`` instead of a Bunch object. See below for more information about the `data` and `target` object.  .. versionadded:: 0.18 ",
          "name": "return_X_y",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn, 'target', the classification labels, 'target_names', the meaning of the labels, 'feature_names', the meaning of the features, and 'DESCR', the full description of the dataset.  (data, target) : tuple if ``return_X_y`` is True  .. versionadded:: 0.18  The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is downloaded from: https://goo.gl/U2Uwz2  Examples -------- Let's say you are interested in the samples 10, 50, and 85, and want to know their class name.  >>> from sklearn.datasets import load_breast_cancer >>> data = load_breast_cancer() >>> data.target[[10, 50, 85]] array([0, 1, 0]) >>> list(data.target_names) ['malignant', 'benign'] \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_blobs",
      "description": "'Generate isotropic Gaussian blobs for clustering.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_blobs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_blobs",
      "parameters": [
        {
          "default": "100",
          "description": "The total number of points equally divided among clusters. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of features for each sample. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "(default=3) The number of centers to generate, or the fixed center locations. ",
          "name": "centers",
          "optional": "true",
          "shape": "n_centers, n_features",
          "type": "int"
        },
        {
          "default": "1.0",
          "description": "The standard deviation of the clusters. ",
          "name": "cluster_std",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "(-10.0, 10.0",
          "description": "The bounding box for each cluster center when centers are generated at random. ",
          "name": "center_box",
          "optional": "true",
          "type": "pair"
        },
        {
          "default": "True",
          "description": "Shuffle the samples. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated samples.  y : array of shape [n_samples] The integer labels for cluster membership of each sample.  Examples -------- >>> from sklearn.datasets.samples_generator import make_blobs >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2, ...                   random_state=0) >>> print(X.shape) (10, 2) >>> y array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])  See also -------- make_classification: a more intricate variant '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.ensemble.partial_dependence.partial_dependence",
      "description": "'Partial dependence of ``target_variables``.\n\nPartial dependence plots show the dependence between the joint values\nof the ``target_variables`` and the function represented\nby the ``gbrt``.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n",
      "id": "sklearn.ensemble.partial_dependence.partial_dependence",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.ensemble.partial_dependence.partial_dependence",
      "parameters": [
        {
          "description": "A fitted gradient boosting model.",
          "name": "gbrt",
          "type": ""
        },
        {
          "description": "The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings).",
          "name": "target_variables",
          "type": "array-like"
        },
        {
          "description": "The grid of ``target_variables`` values for which the partial dependecy should be evaluated (either ``grid`` or ``X`` must be specified). X : array-like, shape=(n_samples, n_features) The data on which ``gbrt`` was trained. It is used to generate a ``grid`` for the ``target_variables``. The ``grid`` comprises ``grid_resolution`` equally spaced points between the two ``percentiles``.",
          "name": "grid",
          "shape": "n_points, len(target_variables",
          "type": "array-like"
        },
        {
          "description": "The lower and upper percentile used create the extreme values for the ``grid``. Only if ``X`` is not None.",
          "name": "percentiles",
          "type": ""
        },
        {
          "description": "The number of equally spaced points on the ``grid``. ",
          "name": "grid_resolution",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The partial dependence function evaluated on the ``grid``. For regression and binary classification ``n_classes==1``. axes : seq of ndarray or None The axes with which the grid has been created or None if the grid has been given.  Examples -------- >>> samples = [[0, 0, 2], [1, 0, 0]] >>> labels = [0, 1] >>> from sklearn.ensemble import GradientBoostingClassifier >>> gb = GradientBoostingClassifier(random_state=0).fit(samples, labels) >>> kwargs = dict(X=samples, percentiles=(0, 1), grid_resolution=2) >>> partial_dependence(gb, [0], **kwargs) # doctest: +SKIP (array([[-4.52...,  4.52...]]), [array([ 0.,  1.])]) '",
        "name": "pdp",
        "shape": "n_classes, n_points",
        "type": "array"
      },
      "tags": [
        "ensemble",
        "partial_dependence"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.chi2_kernel",
      "description": "'Computes the exponential chi-squared kernel X and Y.\n\nThe chi-squared kernel is computed between each pair of rows in X and Y.  X\nand Y have to be non-negative. This kernel is most commonly applied to\nhistograms.\n\nThe chi-squared kernel is given by::\n\nk(x, y) = exp(-gamma Sum [(x - y)^2 / (x + y)])\n\nIt can be interpreted as a weighted difference per entry.\n\nRead more in the :ref:`User Guide <chi2_kernel>`.\n",
      "id": "sklearn.metrics.pairwise.chi2_kernel",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.chi2_kernel",
      "parameters": [
        {
          "description": " Y : array of shape (n_samples_Y, n_features) ",
          "name": "X",
          "shape": "n_samples_X, n_features",
          "type": "array-like"
        },
        {
          "description": "Scaling parameter of the chi2 kernel. ",
          "name": "gamma",
          "type": "float"
        }
      ],
      "returns": {
        "description": " References ---------- * Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local features and kernels for classification of texture and object categories: A comprehensive study International Journal of Computer Vision 2007 http://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf  See also -------- additive_chi2_kernel : The additive version of this kernel  sklearn.kernel_approximation.AdditiveChi2Sampler : A Fourier approximation to the additive version of this kernel. '",
        "name": "kernel_matrix",
        "shape": "n_samples_X, n_samples_Y",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.spectral.discretize",
      "description": "'Search for a partition matrix (clustering) which is closest to the\neigenvector embedding.\n",
      "id": "sklearn.cluster.spectral.discretize",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.spectral.discretize",
      "parameters": [
        {
          "description": "The embedding space of the samples. ",
          "name": "vectors",
          "type": "array-like"
        },
        {
          "description": "Whether to copy vectors, or perform in-place normalization. ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Maximum number of attempts to restart SVD if convergence fails ",
          "name": "max_svd_restarts",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations to attempt in rotation and partition matrix search if machine precision convergence is not reached  random_state: int seed, RandomState instance, or None (default) A pseudo random number generator used for the initialization of the of the rotation matrix ",
          "name": "n_iter_max",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The labels of the clusters.  References ----------  - Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf  Notes -----  The eigenvector embedding is used to iteratively search for the closest discrete partition.  First, the eigenvector embedding is normalized to the space of partition matrices. An optimal discrete partition matrix closest to this normalized embedding multiplied by an initial rotation is calculated.  Fixing this discrete partition matrix, an optimal rotation matrix is calculated.  These two calculations are performed until convergence.  The discrete partition matrix is returned as the clustering solution.  Used in spectral clustering, this method tends to be faster and more robust to random initialization than k-means.  '",
        "name": "labels",
        "type": "array"
      },
      "tags": [
        "cluster",
        "spectral"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups_vectorized",
      "description": "\"Load the 20 newsgroups dataset and transform it into tf-idf vectors.\n\nThis is a convenience function; the tf-idf transformation is done using the\ndefault settings for `sklearn.feature_extraction.text.Vectorizer`. For more\nadvanced usage (stopword filtering, n-gram extraction, etc.), combine\nfetch_20newsgroups with a custom `Vectorizer` or `CountVectorizer`.\n\nRead more in the :ref:`User Guide <20newsgroups>`.\n",
      "id": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups_vectorized",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups_vectorized",
      "parameters": [
        {
          "description": "Select the dataset to load: 'train' for the training set, 'test' for the test set, 'all' for both, with shuffled ordering. ",
          "name": "subset",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Specify an download and cache folder for the datasets. If None, all scikit-learn data is stored in '~/scikit_learn_data' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "May contain any subset of ('headers', 'footers', 'quotes'). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.  'headers' removes newsgroup headers, 'footers' removes blocks at the ends of posts that look like signatures, and 'quotes' removes lines that appear to be quoting another post. ",
          "name": "remove",
          "type": "tuple"
        }
      ],
      "returns": {
        "description": "bunch : Bunch object bunch.data: sparse matrix, shape [n_samples, n_features] bunch.target: array, shape [n_samples] bunch.target_names: list, length [n_classes] \"",
        "name": ""
      },
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "model_selection._search",
      "common_name": "Grid Search CV",
      "description": "'Exhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"predict\", \"predict_proba\", \"decision_function\",\n\"transform\" and \"inverse_transform\" if they are implemented in the\nestimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated grid-search over a parameter grid.\n\nRead more in the :ref:`User Guide <grid_search>`.\n",
      "id": "sklearn.model_selection._search.GridSearchCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run fit with all sets of parameters.\n",
          "id": "sklearn.model_selection._search.GridSearchCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. '",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.model_selection._search.GridSearchCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n",
          "id": "sklearn.model_selection._search.GridSearchCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Input data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.model_selection._search.GridSearchCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.model_selection._search.GridSearchCV",
      "parameters": [
        {
          "description": "This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. ",
          "name": "param_grid",
          "type": "dict"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If ``None``, the ``score`` method of the estimator is used. ",
          "name": "scoring",
          "type": "string"
        },
        {
          "description": "Parameters to pass to the fit method. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Number of jobs to run in parallel. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in \\'2*n_jobs\\' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. ",
          "name": "iid",
          "type": "boolean"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Refit the best estimator with the entire dataset. If \"False\", it is impossible to make predictions using this GridSearchCV instance after fitting. ",
          "name": "refit",
          "type": "boolean"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "type": "integer"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to \\'raise\\', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. ",
          "name": "error_score",
          "type": ""
        },
        {
          "description": "If ``\\'False\\'``, the ``cv_results_`` attribute will not include training scores.   Examples -------- >>> from sklearn import svm, datasets >>> from sklearn.model_selection import GridSearchCV >>> iris = datasets.load_iris() >>> parameters = {\\'kernel\\':(\\'linear\\', \\'rbf\\'), \\'C\\':[1, 10]} >>> svr = svm.SVC() >>> clf = GridSearchCV(svr, parameters) >>> clf.fit(iris.data, iris.target) ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS GridSearchCV(cv=None, error_score=..., estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=..., decision_function_shape=None, degree=..., gamma=..., kernel=\\'rbf\\', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=..., verbose=False), fit_params={}, iid=..., n_jobs=1, param_grid=..., pre_dispatch=..., refit=..., return_train_score=..., scoring=..., verbose=...) >>> sorted(clf.cv_results_.keys()) ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS [\\'mean_fit_time\\', \\'mean_score_time\\', \\'mean_test_score\\',... \\'mean_train_score\\', \\'param_C\\', \\'param_kernel\\', \\'params\\',... \\'rank_test_score\\', \\'split0_test_score\\',... \\'split0_train_score\\', \\'split1_test_score\\', \\'split1_train_score\\',... \\'split2_test_score\\', \\'split2_train_score\\',... \\'std_fit_time\\', \\'std_score_time\\', \\'std_test_score\\', \\'std_train_score\\'...] ",
          "name": "return_train_score",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc:685",
      "tags": [
        "model_selection",
        "_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_friedman3",
      "description": "'Generate the \"Friedman \\\\#3\" regression problem\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are 4 independent features uniformly distributed on the\nintervals::\n\n0 <= X[:, 0] <= 100,\n40 * pi <= X[:, 1] <= 560 * pi,\n0 <= X[:, 2] <= 1,\n1 <= X[:, 3] <= 11.\n\nThe output `y` is created according to the formula::\n\ny(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_friedman3",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_friedman3",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise applied to the output. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] The output values.  References ---------- .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals of Statistics 19 (1), pages 1-67, 1991.  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24, pages 123-140, 1996. '",
        "name": "X",
        "shape": "n_samples, 4",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups",
      "description": "\"Load the filenames and data from the 20 newsgroups dataset.\n\nRead more in the :ref:`User Guide <20newsgroups>`.\n",
      "id": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.twenty_newsgroups.fetch_20newsgroups",
      "parameters": [
        {
          "description": "Select the dataset to load: 'train' for the training set, 'test' for the test set, 'all' for both, with shuffled ordering. ",
          "name": "subset",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Specify a download and cache folder for the datasets. If None, all scikit-learn data is stored in '~/scikit_learn_data' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If None (default), load all the categories. If not None, list of category names to load (other categories ignored). ",
          "name": "categories",
          "type": ""
        },
        {
          "description": "Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Used to shuffle the dataset. ",
          "name": "random_state",
          "type": "numpy"
        },
        {
          "description": "If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "May contain any subset of ('headers', 'footers', 'quotes'). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata.  'headers' removes newsgroup headers, 'footers' removes blocks at the ends of posts that look like signatures, and 'quotes' removes lines that appear to be quoting another post.  'headers' follows an exact standard; the other filters are not always correct. \"",
          "name": "remove",
          "type": "tuple"
        }
      ],
      "tags": [
        "datasets",
        "twenty_newsgroups"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.covtype.fetch_covtype",
      "description": "\"Load the covertype dataset, downloading it if necessary.\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.covtype.fetch_covtype",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.covtype.fetch_covtype",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in '~/scikit_learn_data' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to shuffle dataset. ",
          "name": "shuffle",
          "type": "bool"
        }
      ],
      "returns": {
        "description": " dataset.data : numpy array of shape (581012, 54) Each row corresponds to the 54 features in the dataset.  dataset.target : numpy array of shape (581012,) Each value corresponds to one of the 7 forest covertypes with values ranging between 1 to 7.  dataset.DESCR : string Description of the forest covertype dataset.  \"",
        "name": "dataset",
        "type": "dict-like"
      },
      "tags": [
        "datasets",
        "covtype"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.olivetti_faces.fetch_olivetti_faces",
      "description": "\"Loader for the Olivetti faces data-set from AT&T.\n\nRead more in the :ref:`User Guide <olivetti_faces>`.\n",
      "id": "sklearn.datasets.olivetti_faces.fetch_olivetti_faces",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.olivetti_faces.fetch_olivetti_faces",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in '~/scikit_learn_data' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If True the order of the dataset is shuffled to avoid having images of the same person grouped. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "The seed or the random number generator used to shuffle the data. ",
          "name": "random_state",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": " data : numpy array of shape (400, 4096) Each row corresponds to a ravelled face image of original size 64 x 64 pixels.  images : numpy array of shape (400, 64, 64) Each row is a face image corresponding to one of the 40 subjects of the dataset.  target : numpy array of shape (400, ) Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs.  DESCR : string Description of the modified Olivetti Faces Dataset.  Notes ------  This dataset consists of 10 pictures each of 40 individuals. The original database was available from (now defunct)  http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html  The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis:  http://www.cs.nyu.edu/~roweis/  \"",
        "name": "An object with the following attributes:"
      },
      "tags": [
        "datasets",
        "olivetti_faces"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.random_projection.sparse_random_matrix",
      "description": "'Generalized Achlioptas random sparse matrix for random projection\n\nSetting density to 1 / 3 will yield the original matrix by Dimitris\nAchlioptas while setting a lower value will yield the generalization\nby Ping Li et al.\n\nIf we note :math:`s = 1 / density`, the components of the random matrix are\ndrawn from:\n\n- -sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n-  0                              with probability 1 - 1 / s\n- +sqrt(s) / sqrt(n_components)   with probability 1 / 2s\n\nRead more in the :ref:`User Guide <sparse_random_matrix>`.\n",
      "id": "sklearn.random_projection.sparse_random_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.random_projection.sparse_random_matrix",
      "parameters": [
        {
          "description": "Dimensionality of the target projection space. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Dimensionality of the original source space. ",
          "name": "n_features",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Ratio of non-zero component in the random projection matrix.  If density = \\'auto\\', the value is set to the minimum density as recommended by Ping Li et al.: 1 / sqrt(n_features).  Use density = 1 / 3.0 if you want to reproduce the results from Achlioptas, 2001. ",
          "name": "density",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Control the pseudo random number generator used to generate the matrix at fit time. ",
          "name": "random_state",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "The generated Gaussian random matrix.  See Also -------- SparseRandomProjection gaussian_random_matrix  References ----------  .. [1] Ping Li, T. Hastie and K. W. Church, 2006, \"Very Sparse Random Projections\". http://web.stanford.edu/~hastie/Papers/Ping/KDD06_rp.pdf  .. [2] D. Achlioptas, 2001, \"Database-friendly random projections\", http://www.cs.ucsc.edu/~optas/papers/jl.pdf  '",
        "name": "components: numpy array or CSR matrix with shape [n_components, n_features]"
      },
      "tags": [
        "random_projection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.univariate_selection.chi2",
      "description": "'Compute chi-squared stats between each non-negative feature and class.\n\nThis score can be used to select the n_features features with the\nhighest values for the test chi-squared statistic from X, which must\ncontain only non-negative features such as booleans or frequencies\n(e.g., term counts in document classification), relative to the classes.\n\nRecall that the chi-square test measures dependence between stochastic\nvariables, so using this function \"weeds out\" the features that are the\nmost likely to be independent of class and therefore irrelevant for\nclassification.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.chi2",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.univariate_selection.chi2",
      "parameters": [
        {
          "description": "Sample vectors. ",
          "name": "X",
          "shape": "n_samples, n_features_in",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Target vector (class labels). ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "chi2 statistics of each feature. pval : array, shape = (n_features,) p-values of each feature.  Notes ----- Complexity of this algorithm is O(n_classes * n_features).  See also -------- f_classif: ANOVA F-value between label/feature for classification tasks. f_regression: F-value between label/feature for regression tasks. '",
        "name": "chi2",
        "shape": "n_features,",
        "type": "array"
      },
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.manhattan_distances",
      "description": "' Compute the L1 distances between the vectors in X and Y.\n\nWith sum_over_features equal to False it returns the componentwise\ndistances.\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.manhattan_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.manhattan_distances",
      "parameters": [
        {
          "description": "An array with shape (n_samples_X, n_features).  Y : array_like, optional An array with shape (n_samples_Y, n_features). ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs. ",
          "name": "sum_over_features",
          "type": "bool"
        },
        {
          "description": "Unused parameter. ",
          "name": "size_threshold",
          "type": "int"
        }
      ],
      "returns": {
        "description": "If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances.  Examples -------- >>> from sklearn.metrics.pairwise import manhattan_distances >>> manhattan_distances([[3]], [[3]])#doctest:+ELLIPSIS array([[ 0.]]) >>> manhattan_distances([[3]], [[2]])#doctest:+ELLIPSIS array([[ 1.]]) >>> manhattan_distances([[2]], [[3]])#doctest:+ELLIPSIS array([[ 1.]]) >>> manhattan_distances([[1, 2], [3, 4]],         [[1, 2], [0, 3]])#doctest:+ELLIPSIS array([[ 0.,  2.], [ 4.,  4.]]) >>> import numpy as np >>> X = np.ones((1, 2)) >>> y = 2 * np.ones((2, 2)) >>> manhattan_distances(X, y, sum_over_features=False)#doctest:+ELLIPSIS array([[ 1.,  1.], [ 1.,  1.]]...) '",
        "name": "D",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.regression.explained_variance_score",
      "description": "\"Explained variance regression score function\n\nBest possible score is 1.0, lower values are worse.\n\nRead more in the :ref:`User Guide <explained_variance_score>`.\n",
      "id": "sklearn.metrics.regression.explained_variance_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.regression.explained_variance_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Estimated target values. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Defines aggregating of multiple output scores. Array-like value defines weights used to average scores.  'raw_values' : Returns a full set of scores in case of multioutput input.  'uniform_average' : Scores of all outputs are averaged with uniform weight.  'variance_weighted' : Scores of all outputs are averaged, weighted by the variances of each individual output. ",
          "name": "multioutput",
          "shape": "n_outputs",
          "type": "string"
        }
      ],
      "returns": {
        "description": "The explained variance or ndarray if 'multioutput' is 'raw_values'.  Notes ----- This is not a symmetric function.  Examples -------- >>> from sklearn.metrics import explained_variance_score >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> explained_variance_score(y_true, y_pred)  # doctest: +ELLIPSIS 0.957... >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average') ... # doctest: +ELLIPSIS 0.983...  \"",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "regression"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.accuracy_score",
      "description": "'Accuracy classification score.\n\nIn multilabel classification, this function computes subset accuracy:\nthe set of labels predicted for a sample must *exactly* match the\ncorresponding set of labels in y_true.\n\nRead more in the :ref:`User Guide <accuracy_score>`.\n",
      "id": "sklearn.metrics.classification.accuracy_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.accuracy_score",
      "parameters": [
        {
          "description": "Ground truth (correct) labels. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Predicted labels, as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "default": "True",
          "description": "If ``False``, return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples. ",
          "name": "normalize",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "If ``normalize == True``, return the correctly classified samples (float), else it returns the number of correctly classified samples (int).  The best performance is 1 with ``normalize == True`` and the number of samples with ``normalize == False``.  See also -------- jaccard_similarity_score, hamming_loss, zero_one_loss  Notes ----- In binary and multiclass classification, this function is equal to the ``jaccard_similarity_score`` function.  Examples -------- >>> import numpy as np >>> from sklearn.metrics import accuracy_score >>> y_pred = [0, 2, 1, 3] >>> y_true = [0, 1, 2, 3] >>> accuracy_score(y_true, y_pred) 0.5 >>> accuracy_score(y_true, y_pred, normalize=False) 2  In the multilabel case with binary label indicators: >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))) 0.5 '",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.regression.mean_squared_error",
      "description": "\"Mean squared error regression loss\n\nRead more in the :ref:`User Guide <mean_squared_error>`.\n",
      "id": "sklearn.metrics.regression.mean_squared_error",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.regression.mean_squared_error",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Estimated target values. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "or array-like of shape (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors.  'raw_values' : Returns a full set of errors in case of multioutput input.  'uniform_average' : Errors of all outputs are averaged with uniform weight. ",
          "name": "multioutput",
          "type": "string"
        }
      ],
      "returns": {
        "description": "A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target.  Examples -------- >>> from sklearn.metrics import mean_squared_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> mean_squared_error(y_true, y_pred) 0.375 >>> y_true = [[0.5, 1],[-1, 1],[7, -6]] >>> y_pred = [[0, 2],[-1, 2],[8, -5]] >>> mean_squared_error(y_true, y_pred)  # doctest: +ELLIPSIS 0.708... >>> mean_squared_error(y_true, y_pred, multioutput='raw_values') ... # doctest: +ELLIPSIS array([ 0.416...,  1.        ]) >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7]) ... # doctest: +ELLIPSIS 0.824...  \"",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "regression"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.grid_search.fit_grid_point",
      "description": "\"Run fit on one set of parameters.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.fit_grid_point` instead.\n",
      "id": "sklearn.grid_search.fit_grid_point",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.grid_search.fit_grid_point",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "Targets for input data. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Parameters to be set on estimator for this grid point. ",
          "name": "parameters",
          "type": "dict"
        },
        {
          "description": "Boolean mask or indices for training set. ",
          "name": "train",
          "type": "ndarray"
        },
        {
          "description": "Boolean mask or indices for test set. ",
          "name": "test",
          "type": "ndarray"
        },
        {
          "description": "If provided must be a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scorer",
          "type": "callable"
        },
        {
          "description": "Verbosity level.  **fit_params : kwargs Additional parameter passed to the fit function of the estimator. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. ",
          "name": "error_score",
          "type": ""
        }
      ],
      "returns": {
        "description": "Score of this parameter setting on given training / test split.  parameters : dict The parameters that have been evaluated.  n_samples_test : int Number of test samples in this split. \"",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "grid_search"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.lapack.get_lapack_funcs",
      "description": "\"Return available LAPACK function objects from names.\n\nArrays are used to determine the optimal prefix of LAPACK routines.\n",
      "id": "scipy.linalg.lapack.get_lapack_funcs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.lapack.get_lapack_funcs",
      "parameters": [
        {
          "description": "Name(s) of LAPACK functions without type prefix. ",
          "name": "names",
          "type": "str"
        },
        {
          "description": "Arrays can be given to determine optimal prefix of LAPACK routines. If not given, double-precision routines will be used, otherwise the most generic type in arrays will be used. ",
          "name": "arrays",
          "optional": "true",
          "type": "sequence"
        },
        {
          "description": "Data-type specifier. Not used if `arrays` is non-empty.  ",
          "name": "dtype",
          "optional": "true",
          "type": "str"
        }
      ],
      "returns": {
        "description": "List containing the found function(s).   Notes ----- This routine automatically chooses between Fortran/C interfaces. Fortran code is used whenever possible for arrays with column major order. In all other cases, C code is preferred.  In LAPACK, the naming convention is that all functions start with a type prefix, which depends on the type of the principal matrix. These can be one of {'s', 'd', 'c', 'z'} for the numpy types {float32, float64, complex64, complex128} respectevely, and are stored in attribute `typecode` of the returned functions. \"",
        "name": "funcs",
        "type": "list"
      },
      "tags": [
        "linalg",
        "lapack"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "grid_search",
      "common_name": "Grid Search CV",
      "description": "'Exhaustive search over specified parameter values for an estimator.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.GridSearchCV` instead.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"predict\", \"predict_proba\", \"decision_function\",\n\"transform\" and \"inverse_transform\" if they are implemented in the\nestimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated grid-search over a parameter grid.\n\nRead more in the :ref:`User Guide <grid_search>`.\n",
      "id": "sklearn.grid_search.GridSearchCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run fit with all sets of parameters.\n",
          "id": "sklearn.grid_search.GridSearchCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning.  '",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.grid_search.GridSearchCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n",
          "id": "sklearn.grid_search.GridSearchCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Input data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- * The long-standing behavior of this method changed in version 0.16. * It no longer uses the metric provided by ``estimator.score`` if the ``scoring`` parameter was set when fitting.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.grid_search.GridSearchCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.grid_search.GridSearchCV",
      "parameters": [
        {
          "description": "A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. ",
          "name": "param_grid",
          "type": "dict"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If ``None``, the ``score`` method of the estimator is used. ",
          "name": "scoring",
          "type": "string"
        },
        {
          "description": "Parameters to pass to the fit method. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Number of jobs to run in parallel.  .. versionchanged:: 0.17 Upgraded to joblib 0.9.3. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in \\'2*n_jobs\\' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. ",
          "name": "iid",
          "type": "boolean"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Refit the best estimator with the entire dataset. If \"False\", it is impossible to make predictions using this GridSearchCV instance after fitting. ",
          "name": "refit",
          "type": "boolean"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "type": "integer"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to \\'raise\\', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.   Examples -------- >>> from sklearn import svm, grid_search, datasets >>> iris = datasets.load_iris() >>> parameters = {\\'kernel\\':(\\'linear\\', \\'rbf\\'), \\'C\\':[1, 10]} >>> svr = svm.SVC() >>> clf = grid_search.GridSearchCV(svr, parameters) >>> clf.fit(iris.data, iris.target) ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS GridSearchCV(cv=None, error_score=..., estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=..., decision_function_shape=None, degree=..., gamma=..., kernel=\\'rbf\\', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=..., verbose=False), fit_params={}, iid=..., n_jobs=1, param_grid=..., pre_dispatch=..., refit=..., scoring=..., verbose=...)  ",
          "name": "error_score",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc:626",
      "tags": [
        "grid_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The training data ",
          "name": "data",
          "type": "np"
        }
      ],
      "category": "neighbors.ball_tree",
      "common_name": "Ball Tree",
      "description": "\"BallTree for fast generalized N-point problems\n\nBallTree(X, leaf_size=40, metric='minkowski', \\\\**kwargs)\n",
      "id": "sklearn.neighbors.ball_tree.BallTree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.ball_tree.BallTree",
      "parameters": [
        {
          "description": "n_samples is the number of points in the data set, and n_features is the dimension of the parameter space. Note: if X is a C-contiguous array of doubles then data will not be copied. Otherwise, an internal copy will be made. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.  The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified ``leaf_size``, a leaf node is guaranteed to satisfy ``leaf_size <= n_points <= 2 * leaf_size``, except in the case that ``n_samples < leaf_size``. ",
          "name": "leaf_size",
          "type": "positive"
        },
        {
          "description": "the distance metric to use for the tree.  Default='minkowski' with p=2 (that is, a euclidean metric). See the documentation of the DistanceMetric class for a list of available metrics. ball_tree.valid_metrics gives a list of the metrics which are valid for BallTree.  Additional keywords are passed to the distance metric class. ",
          "name": "metric",
          "type": "string"
        }
      ],
      "source_code": ":",
      "tags": [
        "neighbors",
        "ball_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.basic.solve",
      "description": "'\nSolve the equation ``a x = b`` for ``x``.\n",
      "id": "scipy.linalg.basic.solve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.basic.solve",
      "parameters": [
        {
          "description": "A square matrix.",
          "name": "a",
          "type": ""
        },
        {
          "description": "Right-hand side matrix in ``a x = b``.",
          "name": "b",
          "type": ""
        },
        {
          "description": "Assume `a` is symmetric and positive definite.",
          "name": "sym_pos",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Use only data contained in the lower triangle of `a`, if `sym_pos` is true.  Default is to use upper triangle.",
          "name": "lower",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Allow overwriting data in `a` (may enhance performance). Default is False.",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Allow overwriting data in `b` (may enhance performance). Default is False.",
          "name": "overwrite_b",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrices contain only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Solution to the system ``a x = b``.  Shape of the return matches the shape of `b`.  Raises ------ LinAlgError If `a` is singular. ValueError If `a` is not square  Examples -------- Given `a` and `b`, solve for `x`:  >>> a = np.array([[3, 2, 0], [1, -1, 0], [0, 5, 1]]) >>> b = np.array([2, 4, -1]) >>> from scipy import linalg >>> x = linalg.solve(a, b) >>> x array([ 2., -2.,  9.]) >>> np.dot(a, x) == b array([ True,  True,  True], dtype=bool)  '",
        "name": "x",
        "type": ""
      },
      "tags": [
        "linalg",
        "basic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.homogeneity_completeness_v_measure",
      "description": "\"Compute the homogeneity and completeness and V-Measure scores at once.\n\nThose metrics are based on normalized conditional entropy measures of\nthe clustering labeling to evaluate given the knowledge of a Ground\nTruth class labels of the same samples.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nA clustering result satisfies completeness if all the data points\nthat are members of a given class are elements of the same cluster.\n\nBoth scores have positive values between 0.0 and 1.0, larger values\nbeing desirable.\n\nThose 3 metrics are independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore values in any way.\n\nV-Measure is furthermore symmetric: swapping ``labels_true`` and\n``label_pred`` will give the same score. This does not hold for\nhomogeneity and completeness.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "id": "sklearn.metrics.cluster.supervised.homogeneity_completeness_v_measure",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.homogeneity_completeness_v_measure",
      "parameters": [
        {
          "description": "ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling  completeness : float score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling  v_measure : float harmonic mean of the first two  See also -------- homogeneity_score completeness_score v_measure_score \"",
        "name": "homogeneity",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.regression.mean_absolute_error",
      "description": "\"Mean absolute error regression loss\n\nRead more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "id": "sklearn.metrics.regression.mean_absolute_error",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.regression.mean_absolute_error",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Estimated target values. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "or array-like of shape (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors.  'raw_values' : Returns a full set of errors in case of multioutput input.  'uniform_average' : Errors of all outputs are averaged with uniform weight.  ",
          "name": "multioutput",
          "type": "string"
        }
      ],
      "returns": {
        "description": "If multioutput is 'raw_values', then mean absolute error is returned for each output separately. If multioutput is 'uniform_average' or an ndarray of weights, then the weighted average of all output errors is returned.  MAE output is non-negative floating point. The best value is 0.0.  Examples -------- >>> from sklearn.metrics import mean_absolute_error >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> mean_absolute_error(y_true, y_pred) 0.5 >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> mean_absolute_error(y_true, y_pred) 0.75 >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values') array([ 0.5,  1. ]) >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7]) ... # doctest: +ELLIPSIS 0.849... \"",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "regression"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.confusion_matrix",
      "description": "'Compute confusion matrix to evaluate the accuracy of a classification\n\nBy definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\nis equal to the number of observations known to be in group :math:`i` but\npredicted to be in group :math:`j`.\n\nThus in binary classification, the count of true negatives is\n:math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n:math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n\nRead more in the :ref:`User Guide <confusion_matrix>`.\n",
      "id": "sklearn.metrics.classification.confusion_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.confusion_matrix",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in ``y_true`` or ``y_pred`` are used in sorted order. ",
          "name": "labels",
          "optional": "true",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Confusion matrix  References ---------- .. [1] `Wikipedia entry for the Confusion matrix <https://en.wikipedia.org/wiki/Confusion_matrix>`_  Examples -------- >>> from sklearn.metrics import confusion_matrix >>> y_true = [2, 0, 2, 2, 0, 1] >>> y_pred = [0, 0, 2, 2, 0, 2] >>> confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])  >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"] >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"] >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"]) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])  '",
        "name": "C",
        "shape": "n_classes, n_classes",
        "type": "array"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.robust_covariance.c_step",
      "description": "'C_step procedure described in [Rouseeuw1984]_ aiming at computing MCD.\n",
      "id": "sklearn.covariance.robust_covariance.c_step",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.robust_covariance.c_step",
      "parameters": [
        {
          "description": "Data set in which we look for the n_support observations whose scatter matrix has minimum determinant. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of observations to compute the robust estimates of location and covariance from. ",
          "name": "n_support",
          "type": "int"
        },
        {
          "description": "Number of iterations to perform. According to [Rouseeuw1999]_, two iterations are sufficient to get close to the minimum, and we never need more than 30 to reach convergence. ",
          "name": "remaining_iterations",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Initial estimates of location and shape from which to run the c_step procedure: - initial_estimates[0]: an initial location estimate - initial_estimates[1]: an initial covariance estimate ",
          "name": "initial_estimates",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Verbose mode. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The random generator used. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The function which will be used to compute the covariance. Must return shape (n_features, n_features) ",
          "name": "cov_computation_method",
          "type": "callable"
        }
      ],
      "returns": {
        "description": "Robust location estimates.  covariance : array-like, shape (n_features, n_features) Robust covariance estimates.  support : array-like, shape (n_samples,) A mask for the `n_support` observations whose scatter matrix has minimum determinant.  References ---------- .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS  '",
        "name": "location",
        "shape": "n_features,",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "robust_covariance"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_friedman1",
      "description": "'Generate the \"Friedman \\\\#1\" regression problem\n\nThis dataset is described in Friedman [1] and Breiman [2].\n\nInputs `X` are independent features uniformly distributed on the interval\n[0, 1]. The output `y` is created according to the formula::\n\ny(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n\nOut of the `n_features` features, only 5 are actually used to compute\n`y`. The remaining features are independent of `y`.\n\nThe number of features has to be >= 5.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_friedman1",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_friedman1",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "10",
          "description": "The number of features. Should be at least 5. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise applied to the output. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] The output values.  References ---------- .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals of Statistics 19 (1), pages 1-67, 1991.  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24, pages 123-140, 1996. '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_low_rank_matrix",
      "description": "\"Generate a mostly low rank matrix with bell-shaped singular values\n\nMost of the variance can be explained by a bell-shaped curve of width\neffective_rank: the low rank part of the singular values profile is::\n\n(1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n\nThe remaining singular values' tail is fat, decreasing as::\n\ntail_strength * exp(-0.1 * i / effective_rank).\n\nThe low rank part of the profile can be considered the structured\nsignal part of the data while the tail can be considered the noisy\npart of the data that cannot be summarized by a low number of linear\ncomponents (singular vectors).\n\nThis kind of singular profiles is often seen in practice, for instance:\n- gray level pictures of faces\n- TF-IDF vectors of text documents crawled from the web\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_low_rank_matrix",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_low_rank_matrix",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "100",
          "description": "The number of features. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "10",
          "description": "The approximate number of singular vectors required to explain most of the data by linear combinations. ",
          "name": "effective_rank",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.5",
          "description": "The relative importance of the fat noisy tail of the singular values profile. ",
          "name": "tail_strength",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The matrix. \"",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.class_weight.compute_sample_weight",
      "description": "'Estimate sample weights by class for unbalanced datasets.\n",
      "id": "sklearn.utils.class_weight.compute_sample_weight",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.class_weight.compute_sample_weight",
      "parameters": [
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: ``n_samples / (n_classes * np.bincount(y))``.  For multi-output, the weights of each column of y will be multiplied. ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Array of original class labels per sample. ",
          "name": "y",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only \"auto\" is supported for class_weight if this is provided. ",
          "name": "indices",
          "shape": "n_subsample,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Array with sample weights as applied to the original y '",
        "name": "sample_weight_vect",
        "shape": "n_samples,",
        "type": "ndarray"
      },
      "tags": [
        "utils",
        "class_weight"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.randomized_range_finder",
      "description": "\"Computes an orthonormal matrix whose range approximates the range of A.\n",
      "id": "sklearn.utils.extmath.randomized_range_finder",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.randomized_range_finder",
      "parameters": [
        {
          "description": "The input data matrix  size: integer Size of the return array  n_iter: integer Number of power iterations used to stabilize the result  power_iteration_normalizer: 'auto' (default), 'QR', 'LU', 'none' Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), 'none' (the fastest but numerically unstable when `n_iter` is large, e.g. typically 5 or larger), or 'LU' factorization (numerically stable but can lose slightly in accuracy). The 'auto' mode applies no normalization if `n_iter`<=2 and switches to LU otherwise.  .. versionadded:: 0.18  random_state: RandomState or an int seed (0 by default) A random number generator instance ",
          "name": "A",
          "type": ""
        }
      ],
      "returns": {
        "description": "A (size x size) projection matrix, the range of which approximates well the range of the input matrix A.  Notes -----  Follows Algorithm 4.3 of Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061  An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014 \"",
        "name": "Q: 2D array"
      },
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.shuffle",
      "description": "\"Shuffle arrays or sparse matrices in a consistent way\n\nThis is a convenience alias to ``resample(*arrays, replace=False)`` to do\nrandom permutations of the collections.\n",
      "id": "sklearn.utils.shuffle",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.shuffle",
      "parameters": [
        {
          "description": "Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension. ",
          "name": "*arrays",
          "type": "sequence"
        },
        {
          "description": "Control the shuffling for reproducible behavior. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. ",
          "name": "n_samples",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Sequence of shuffled views of the collections. The original arrays are not impacted.  Examples -------- It is possible to mix sparse and dense arrays in the same run::  >>> X = np.array([[1., 0.], [2., 1.], [0., 0.]]) >>> y = np.array([0, 1, 2])  >>> from scipy.sparse import coo_matrix >>> X_sparse = coo_matrix(X)  >>> from sklearn.utils import shuffle >>> X, X_sparse, y = shuffle(X, X_sparse, y, random_state=0) >>> X array([[ 0.,  0.], [ 2.,  1.], [ 1.,  0.]])  >>> X_sparse                   # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE <3x2 sparse matrix of type '<... 'numpy.float64'>' with 3 stored elements in Compressed Sparse Row format>  >>> X_sparse.toarray() array([[ 0.,  0.], [ 2.,  1.], [ 1.,  0.]])  >>> y array([2, 1, 0])  >>> shuffle(y, n_samples=2, random_state=0) array([0, 1])  See also -------- :func:`sklearn.utils.resample` \"",
        "name": "shuffled_arrays",
        "type": "sequence"
      },
      "tags": [
        "utils"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._search.fit_grid_point",
      "description": "\"Run fit on one set of parameters.\n",
      "id": "sklearn.model_selection._search.fit_grid_point",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._search.fit_grid_point",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "Targets for input data. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Parameters to be set on estimator for this grid point. ",
          "name": "parameters",
          "type": "dict"
        },
        {
          "description": "Boolean mask or indices for training set. ",
          "name": "train",
          "type": "ndarray"
        },
        {
          "description": "Boolean mask or indices for test set. ",
          "name": "test",
          "type": "ndarray"
        },
        {
          "description": "If provided must be a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scorer",
          "type": "callable"
        },
        {
          "description": "Verbosity level.  **fit_params : kwargs Additional parameter passed to the fit function of the estimator. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to 'raise', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. ",
          "name": "error_score",
          "type": ""
        }
      ],
      "returns": {
        "description": "Score of this parameter setting on given training / test split.  parameters : dict The parameters that have been evaluated.  n_samples_test : int Number of test samples in this split. \"",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "model_selection",
        "_search"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_cholesky.cholesky",
      "description": "'\nCompute the Cholesky decomposition of a matrix.\n\nReturns the Cholesky decomposition, :math:`A = L L^*` or\n:math:`A = U^* U` of a Hermitian positive-definite matrix A.\n",
      "id": "scipy.linalg.decomp_cholesky.cholesky",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_cholesky.cholesky",
      "parameters": [
        {
          "description": "Matrix to be decomposed",
          "name": "a",
          "type": ""
        },
        {
          "description": "Whether to compute the upper or lower triangular Cholesky factorization.  Default is upper-triangular.",
          "name": "lower",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to overwrite data in `a` (may improve performance).",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrix contains only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Upper- or lower-triangular Cholesky factor of `a`.  Raises ------ LinAlgError : if decomposition fails.  Examples -------- >>> from scipy import array, linalg, dot >>> a = array([[1,-2j],[2j,5]]) >>> L = linalg.cholesky(a, lower=True) >>> L array([[ 1.+0.j,  0.+0.j], [ 0.+2.j,  1.+0.j]]) >>> dot(L, L.T.conj()) array([[ 1.+0.j,  0.-2.j], [ 0.+2.j,  5.+0.j]])  '",
        "name": "c",
        "type": ""
      },
      "tags": [
        "linalg",
        "decomp_cholesky"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.univariate_selection.f_oneway",
      "description": "'Performs a 1-way ANOVA.\n\nThe one-way ANOVA tests the null hypothesis that 2 or more groups have\nthe same population mean. The test is applied to samples from two or\nmore groups, possibly with differing sizes.\n\nRead more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.univariate_selection.f_oneway",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.univariate_selection.f_oneway",
      "parameters": [
        {
          "description": "The sample measurements should be given as arguments. ",
          "name": "sample1, sample2, ...",
          "type": "array"
        }
      ],
      "returns": {
        "description": "The computed F-value of the test. p-value : float The associated p-value from the F-distribution.  Notes ----- The ANOVA test has important assumptions that must be satisfied in order for the associated p-value to be valid.  1. The samples are independent 2. Each sample is from a normally distributed population 3. The population standard deviations of the groups are all equal. This property is known as homoscedasticity.  If these assumptions are not true for a given set of data, it may still be possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although with some loss of power.  The algorithm is from Heiman[2], pp.394-7.  See ``scipy.stats.f_oneway`` that should give the same results while being less efficient.  References ----------  .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential Statistics\". Chapter 14. http://faculty.vassar.edu/lowry/ch14pt1.html  .. [2] Heiman, G.W.  Research Methods in Statistics. 2002.  '",
        "name": "F-value",
        "type": "float"
      },
      "tags": [
        "feature_selection",
        "univariate_selection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.sag.sag_solver",
      "description": "\"SAG solver for Ridge and LogisticRegression\n\nSAG stands for Stochastic Average Gradient: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na constant learning rate.\n\nIMPORTANT NOTE: 'sag' solver converges faster on columns that are on the\nsame scale. You can normalize the data by using\nsklearn.preprocessing.StandardScaler on your data before passing it to the\nfit method.\n\nThis implementation works with data represented as dense numpy arrays or\nsparse scipy arrays of floating point values for the features. It will\nfit the data according to squared loss or log loss.\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using the squared euclidean norm L2.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.linear_model.sag.sag_solver",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.sag.sag_solver",
      "parameters": [
        {
          "description": "Training data ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Target values. With loss='multinomial', y must be label encoded (see preprocessing.LabelEncoder). ",
          "name": "y",
          "shape": "n_samples,",
          "type": "numpy"
        },
        {
          "description": "Weights applied to individual samples (1. for unweighted). ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Loss function that will be optimized: -'log' is the binary logistic loss, as used in LogisticRegression. -'squared' is the squared loss, as used in Ridge. -'multinomial' is the multinomial logistic loss, as used in LogisticRegression.  .. versionadded:: 0.18 *loss='multinomial'* ",
          "name": "loss",
          "type": ""
        },
        {
          "description": "Constant that multiplies the regularization term. Defaults to 1.  max_iter: int, optional The max number of passes over the training data if the stopping criteria is not reached. Defaults to 1000.  tol: double, optional The stopping criteria for the weights. The iterations will stop when max(change in weights) / max(weights) < tol. Defaults to .001  verbose: integer, optional The verbosity level. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If False, the input arrays X and y will not be checked. ",
          "name": "check_input",
          "type": "bool"
        },
        {
          "description": "Maximum squared sum of X over samples. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation.  warm_start_mem: dict, optional The initialization parameters used for warm starting. Warm starting is currently used in LogisticRegression but not in Ridge. It contains: - 'coef': the weight vector, with the intercept in last line if the intercept is fitted. - 'gradient_memory': the scalar gradient for all seen samples. - 'sum_gradient': the sum of gradient over all seen samples, for each feature. - 'intercept_sum_gradient': the sum of gradient over all seen samples, for the intercept. - 'seen': array of boolean describing the seen samples. - 'num_seen': the number of seen samples. ",
          "name": "max_squared_sum",
          "type": "float"
        }
      ],
      "returns": {
        "description": "Weight vector.  n_iter_ : int The number of full pass on all samples.  warm_start_mem : dict Contains a 'coef' key with the fitted result, and possibly the fitted intercept at the end of the array. Contains also other keys used for warm starting.  Examples -------- >>> import numpy as np >>> from sklearn import linear_model >>> n_samples, n_features = 10, 5 >>> np.random.seed(0) >>> X = np.random.randn(n_samples, n_features) >>> y = np.random.randn(n_samples) >>> clf = linear_model.Ridge(solver='sag') >>> clf.fit(X, y) ... #doctest: +NORMALIZE_WHITESPACE Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='sag', tol=0.001)  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]]) >>> y = np.array([1, 1, 2, 2]) >>> clf = linear_model.LogisticRegression(solver='sag') >>> clf.fit(X, y) ... #doctest: +NORMALIZE_WHITESPACE LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1, penalty='l2', random_state=None, solver='sag', tol=0.0001, verbose=0, warm_start=False)  References ---------- Schmidt, M., Roux, N. L., & Bach, F. (2013). Minimizing finite sums with the stochastic average gradient https://hal.inria.fr/hal-00860051/document  See also -------- Ridge, SGDRegressor, ElasticNet, Lasso, SVR, and LogisticRegression, SGDClassifier, LinearSVC, Perceptron \"",
        "name": "coef_",
        "shape": "n_features",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "sag"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.homogeneity_score",
      "description": "'Homogeneity metric of a cluster labeling given a ground truth.\n\nA clustering result satisfies homogeneity if all of its clusters\ncontain only data points which are members of a single class.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won\\'t change the\nscore value in any way.\n\nThis metric is not symmetric: switching ``label_true`` with ``label_pred``\nwill return the :func:`completeness_score` which will be different in\ngeneral.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "id": "sklearn.metrics.cluster.supervised.homogeneity_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.homogeneity_score",
      "parameters": [
        {
          "description": "ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling  References ----------  .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_  See also -------- completeness_score v_measure_score  Examples --------  Perfect labelings are homogeneous::  >>> from sklearn.metrics.cluster import homogeneity_score >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  Non-perfect labelings that further split classes into more clusters can be perfectly homogeneous::  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2])) ...                                                  # doctest: +ELLIPSIS 1.0... >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3])) ...                                                  # doctest: +ELLIPSIS 1.0...  Clusters that include samples from different classes do not make for an homogeneous labeling::  >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1])) ...                                                  # doctest: +ELLIPSIS 0.0... >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0])) ...                                                  # doctest: +ELLIPSIS 0.0...  '",
        "name": "homogeneity",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.pairwise_distances_argmin",
      "description": "\"Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance).\n\nThis is mostly equivalent to calling:\n\npairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n\nbut uses much less memory, and is faster for large arrays.\n\nThis function works with dense 2D arrays only.\n",
      "id": "sklearn.metrics.pairwise.pairwise_distances_argmin",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.pairwise_distances_argmin",
      "parameters": [
        {
          "description": "Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features)  Y : array-like Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features) ",
          "name": "X",
          "type": "array-like"
        },
        {
          "description": "To reduce memory consumption over the naive solution, data are processed in batches, comprising batch_size rows of X and batch_size rows of Y. The default value is quite conservative, but can be changed for fine-tuning. The larger the number, the larger the memory usage. ",
          "name": "batch_size",
          "type": "integer"
        },
        {
          "description": "metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.  If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.  Distance matrices are not supported.  Valid values for metric are:  - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']  - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']  See the documentation for scipy.spatial.distance for details on these metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "keyword arguments to pass to specified metric function. ",
          "name": "metric_kwargs",
          "type": "dict"
        },
        {
          "description": "Axis along which the argmin and distances are to be computed. ",
          "name": "axis",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Y[argmin[i], :] is the row in Y that is closest to X[i, :].  See also -------- sklearn.metrics.pairwise_distances sklearn.metrics.pairwise_distances_argmin_min \"",
        "name": "argmin",
        "type": "numpy"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.euclidean_distances",
      "description": "'\nConsidering the rows of X (and Y=X) as vectors, compute the\ndistance matrix between each pair of vectors.\n\nFor efficiency reasons, the euclidean distance between a pair of row\nvector x and y is computed as::\n\ndist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n\nThis formulation has two advantages over other ways of computing distances.\nFirst, it is computationally efficient when dealing with sparse data.\nSecond, if one argument varies but the other remains unchanged, then\n`dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n\nHowever, this is not the most precise way of doing this computation, and\nthe distance matrix returned by this function may not be exactly\nsymmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.euclidean_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.euclidean_distances",
      "parameters": [
        {
          "description": " Y : {array-like, sparse matrix}, shape (n_samples_2, n_features)  Y_norm_squared : array-like, shape (n_samples_2, ), optional Pre-computed dot-products of vectors in Y (e.g., ``(Y**2).sum(axis=1)``) ",
          "name": "X",
          "shape": "n_samples_1, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Return squared Euclidean distances.  X_norm_squared : array-like, shape = [n_samples_1], optional Pre-computed dot-products of vectors in X (e.g., ``(X**2).sum(axis=1)``) ",
          "name": "squared",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": " Examples -------- >>> from sklearn.metrics.pairwise import euclidean_distances >>> X = [[0, 1], [1, 1]] >>> # distance between rows of X >>> euclidean_distances(X, X) array([[ 0.,  1.], [ 1.,  0.]]) >>> # get distance to origin >>> euclidean_distances(X, [[0, 0]]) array([[ 1.        ], [ 1.41421356]])  See also -------- paired_distances : distances betweens pairs of elements of X and Y. '",
        "name": "distances",
        "shape": "n_samples_1, n_samples_2",
        "type": "array, sparse matrix"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.dict_learning.dict_learning",
      "description": "\"Solves a dictionary learning matrix factorization problem.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n(U,V)\nwith || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n",
      "id": "sklearn.decomposition.dict_learning.dict_learning",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.dict_learning.dict_learning",
      "parameters": [
        {
          "description": "Data matrix. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Number of dictionary atoms to extract. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Sparsity controlling parameter. ",
          "name": "alpha",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations to perform. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "Tolerance for the stopping condition. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. ",
          "name": "method",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Number of parallel jobs to run, or -1 to autodetect. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Initial value for the dictionary for warm restart scenarios. ",
          "name": "dict_init",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "Initial value for the sparse code for warm restart scenarios.  callback : Callable that gets invoked every five iterations.  verbose : Degree of output the procedure will print. ",
          "name": "code_init",
          "shape": "n_samples, n_components",
          "type": "array"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The sparse code factor in the matrix factorization.  dictionary : array of shape (n_components, n_features), The dictionary factor in the matrix factorization.  errors : array Vector of errors at each iteration.  n_iter : int Number of iterations run. Returned only if `return_n_iter` is set to True.  See also -------- dict_learning_online DictionaryLearning MiniBatchDictionaryLearning SparsePCA MiniBatchSparsePCA \"",
        "name": "code",
        "shape": "n_samples, n_components",
        "type": "array"
      },
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.species_distributions.fetch_species_distributions",
      "description": "'Loader for species distribution dataset from Phillips et. al. (2006)\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.species_distributions.fetch_species_distributions",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.species_distributions.fetch_species_distributions",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in \\'~/scikit_learn_data\\' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": " coverages : array, shape = [14, 1592, 1212] These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999.  train : record array, shape = (1623,) The training points for the data.  Each point has three fields:  - train[\\'species\\'] is the species name - train[\\'dd long\\'] is the longitude, in degrees - train[\\'dd lat\\'] is the latitude, in degrees  test : record array, shape = (619,) The test points for the data.  Same format as the training data.  Nx, Ny : integers The number of longitudes (x) and latitudes (y) in the grid  x_left_lower_corner, y_left_lower_corner : floats The (x,y) position of the lower-left corner, in degrees  grid_size : float The spacing between points of the grid, in degrees  Notes ------  This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006).  The two species are:  - `\"Bradypus variegatus\" <http://www.iucnredlist.org/details/3038/0>`_ , the Brown-throated Sloth.  - `\"Microryzomys minutus\" <http://www.iucnredlist.org/details/13408/0>`_ , also known as the Forest Small Rice Rat, a rodent that lives in Peru, Colombia, Ecuador, Peru, and Venezuela.  References ----------  * `\"Maximum entropy modeling of species geographic distributions\" <http://www.cs.princeton.edu/~schapire/papers/ecolmod.pdf>`_ S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling, 190:231-259, 2006.  Notes -----  * See examples/applications/plot_species_distribution_modeling.py for an example of using this dataset with scikit-learn  '",
        "name": "The data is returned as a Bunch object with the following attributes:"
      },
      "tags": [
        "datasets",
        "species_distributions"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.brier_score_loss",
      "description": "'Compute the Brier score.\n\nThe smaller the Brier score, the better, hence the naming with \"loss\".\n\nAcross all items in a set N predictions, the Brier score measures the\nmean squared difference between (1) the predicted probability assigned\nto the possible outcomes for item i, and (2) the actual outcome.\nTherefore, the lower the Brier score is for a set of predictions, the\nbetter the predictions are calibrated. Note that the Brier score always\ntakes on a value between zero and one, since this is the largest\npossible difference between a predicted probability (which must be\nbetween zero and one) and the actual outcome (which can take on values\nof only 0 and 1).\n\nThe Brier score is appropriate for binary and categorical outcomes that\ncan be structured as true or false, but is inappropriate for ordinal\nvariables which can take on three or more values (this is because the\nBrier score assumes that all possible outcomes are equivalently\n\"distant\" from one another). Which label is considered to be the positive\nlabel is controlled via the parameter pos_label, which defaults to 1.\n\nRead more in the :ref:`User Guide <calibration>`.\n",
      "id": "sklearn.metrics.classification.brier_score_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.brier_score_loss",
      "parameters": [
        {
          "description": "True targets. ",
          "name": "y_true",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Probabilities of the positive class. ",
          "name": "y_prob",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Label of the positive class. If None, the maximum label is used as positive class ",
          "name": "pos_label",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Brier score  Examples -------- >>> import numpy as np >>> from sklearn.metrics import brier_score_loss >>> y_true = np.array([0, 1, 1, 0]) >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"]) >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3]) >>> brier_score_loss(y_true, y_prob)  # doctest: +ELLIPSIS 0.037... >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)  # doctest: +ELLIPSIS 0.037... >>> brier_score_loss(y_true_categorical, y_prob,                          pos_label=\"ham\")  # doctest: +ELLIPSIS 0.037... >>> brier_score_loss(y_true, np.array(y_prob) > 0.5) 0.0  References ---------- .. [1] `Wikipedia entry for the Brier score. <https://en.wikipedia.org/wiki/Brier_score>`_ '",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.hamming_loss",
      "description": "'Compute the average Hamming loss.\n\nThe Hamming loss is the fraction of labels that are incorrectly predicted.\n\nRead more in the :ref:`User Guide <hamming_loss>`.\n",
      "id": "sklearn.metrics.classification.hamming_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.hamming_loss",
      "parameters": [
        {
          "description": "Ground truth (correct) labels. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Predicted labels, as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "default": "None",
          "description": "Integer array of labels. If not provided, labels will be inferred from y_true and y_pred.  .. versionadded:: 0.18 ",
          "name": "labels",
          "optional": "true",
          "shape": "n_labels",
          "type": "array"
        },
        {
          "description": "Sample weights.  .. versionadded:: 0.18 ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "(deprecated) Integer array of labels. This parameter has been renamed to ``labels`` in version 0.18 and will be removed in 0.20. ",
          "name": "classes",
          "optional": "true",
          "shape": "n_labels",
          "type": "array"
        }
      ],
      "returns": {
        "description": "Return the average Hamming loss between element of ``y_true`` and ``y_pred``.  See Also -------- accuracy_score, jaccard_similarity_score, zero_one_loss  Notes ----- In multiclass classification, the Hamming loss correspond to the Hamming distance between ``y_true`` and ``y_pred`` which is equivalent to the subset ``zero_one_loss`` function.  In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels.  The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1.  References ---------- .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification: An Overview. International Journal of Data Warehousing & Mining, 3(3), 1-13, July-September 2007.  .. [2] `Wikipedia entry on the Hamming distance <https://en.wikipedia.org/wiki/Hamming_distance>`_  Examples -------- >>> from sklearn.metrics import hamming_loss >>> y_pred = [1, 2, 3, 4] >>> y_true = [2, 2, 3, 4] >>> hamming_loss(y_true, y_pred) 0.25  In the multilabel case with binary label indicators:  >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2))) 0.75 '",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.mutual_info_score",
      "description": "\"Mutual Information between two clusterings.\n\nThe Mutual Information is a measure of the similarity between two labels of\nthe same data. Where :math:`P(i)` is the probability of a random sample\noccurring in cluster :math:`U_i` and :math:`P'(j)` is the probability of a\nrandom sample occurring in cluster :math:`V_j`, the Mutual Information\nbetween clusterings :math:`U` and :math:`V` is given as:\n\n.. math::\n\nMI(U,V)=\\\\sum_{i=1}^R \\\\sum_{j=1}^C P(i,j)\\\\log\\\\frac{P(i,j)}{P(i)P'(j)}\n\nThis is equal to the Kullback-Leibler divergence of the joint distribution\nwith the product distribution of the marginals.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n",
      "id": "sklearn.metrics.cluster.supervised.mutual_info_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.mutual_info_score",
      "parameters": [
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "shape = [n_classes_true, n_classes_pred] A contingency matrix given by the :func:`contingency_matrix` function. If value is ``None``, it will be computed, otherwise the given value is used, with ``labels_true`` and ``labels_pred`` ignored. ",
          "name": "contingency",
          "type": "None, array, sparse matrix"
        }
      ],
      "returns": {
        "description": "Mutual information, a non-negative value  See also -------- adjusted_mutual_info_score: Adjusted against chance Mutual Information normalized_mutual_info_score: Normalized Mutual Information \"",
        "name": "mi",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.adjusted_rand_score",
      "description": "'Rand index adjusted for chance.\n\nThe Rand Index computes a similarity measure between two clusterings\nby considering all pairs of samples and counting pairs that are\nassigned in the same or different clusters in the predicted and\ntrue clusterings.\n\nThe raw RI score is then \"adjusted for chance\" into the ARI score\nusing the following scheme::\n\nARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n\nThe adjusted Rand index is thus ensured to have a value close to\n0.0 for random labeling independently of the number of clusters and\nsamples and exactly 1.0 when the clusterings are identical (up to\na permutation).\n\nARI is a symmetric measure::\n\nadjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n\nRead more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "id": "sklearn.metrics.cluster.supervised.adjusted_rand_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.adjusted_rand_score",
      "parameters": [
        {
          "description": "Ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "Cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "Similarity score between -1.0 and 1.0. Random labelings have an ARI close to 0.0. 1.0 stands for perfect match.  Examples --------  Perfectly maching labelings have a score of 1 even  >>> from sklearn.metrics.cluster import adjusted_rand_score >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1]) 1.0 >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  Labelings that assign all classes members to the same clusters are complete be not always pure, hence penalized::  >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])  # doctest: +ELLIPSIS 0.57...  ARI is symmetric, so labelings that have pure clusters with members coming from the same classes but unnecessary splits are penalized::  >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])  # doctest: +ELLIPSIS 0.57...  If classes members are completely split across different clusters, the assignment is totally incomplete, hence the ARI is very low::  >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3]) 0.0  References ----------  .. [Hubert1985] `L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985` http://link.springer.com/article/10.1007%2FBF01908075  .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index  See also -------- adjusted_mutual_info_score: Adjusted Mutual Information  '",
        "name": "ari",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.robust_covariance.fast_mcd",
      "description": "'Estimates the Minimum Covariance Determinant matrix.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n",
      "id": "sklearn.covariance.robust_covariance.fast_mcd",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.robust_covariance.fast_mcd",
      "parameters": [
        {
          "description": "The data matrix, with p features and n samples. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: `[n_sample + n_features + 1] / 2`. ",
          "name": "support_fraction",
          "type": "float"
        },
        {
          "description": "The generator used to randomly subsample. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The function which will be used to compute the covariance. Must return shape (n_features, n_features)  Notes ----- The FastMCD algorithm has been introduced by Rousseuw and Van Driessen in \"A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS\". The principle is to compute robust estimates and random subsets before pooling them into a larger subsets, and finally into the full data set. Depending on the size of the initial sample, we have one, two or three such computation levels.  Note that only raw estimates are returned. If one is interested in the correction and reweighting steps described in [Rouseeuw1999]_, see the MinCovDet object.  References ----------  .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS  .. [Butler1993] R. W. Butler, P. L. Davies and M. Jhun, Asymptotics For The Minimum Covariance Determinant Estimator, The Annals of Statistics, 1993, Vol. 21, No. 3, 1385-1400 ",
          "name": "cov_computation_method",
          "type": "callable"
        }
      ],
      "returns": {
        "description": "Robust location of the data.  covariance : array-like, shape (n_features, n_features) Robust covariance of the features.  support : array-like, type boolean, shape (n_samples,) A mask of the observations that have been used to compute the robust location and covariance estimates of the data set.  '",
        "name": "location",
        "shape": "n_features,",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "robust_covariance"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.core.fromnumeric.argpartition",
      "description": "\"\nPerform an indirect partition along the given axis using the algorithm\nspecified by the `kind` keyword. It returns an array of indices of the\nsame shape as `a` that index data along the given axis in partitioned\norder.\n\n.. versionadded:: 1.8.0\n",
      "id": "numpy.core.fromnumeric.argpartition",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.core.fromnumeric.argpartition",
      "parameters": [
        {
          "description": "Array to sort.",
          "name": "a",
          "type": "array"
        },
        {
          "description": "Element index to partition by. The kth element will be in its final sorted position and all smaller elements will be moved before it and all larger elements behind it. The order all elements in the partitions is undefined. If provided with a sequence of kth it will partition all of them into their sorted position at once.",
          "name": "kth",
          "type": "int"
        },
        {
          "description": "Axis along which to sort.  The default is -1 (the last axis). If None, the flattened array is used.",
          "name": "axis",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Selection algorithm. Default is 'introselect'",
          "name": "kind",
          "optional": "true",
          "type": "'introselect'"
        },
        {
          "description": "When `a` is an array with fields defined, this argument specifies which fields to compare first, second, etc.  A single field can be specified as a string, and not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties. ",
          "name": "order",
          "optional": "true",
          "type": "str"
        }
      ],
      "returns": {
        "description": "Array of indices that partition `a` along the specified axis. In other words, ``a[index_array]`` yields a sorted `a`.  See Also -------- partition : Describes partition algorithms used. ndarray.partition : Inplace partition. argsort : Full indirect sort  Notes ----- See `partition` for notes on the different selection algorithms.  Examples -------- One dimensional array:  >>> x = np.array([3, 4, 2, 1]) >>> x[np.argpartition(x, 3)] array([2, 1, 3, 4]) >>> x[np.argpartition(x, (1, 3))] array([1, 2, 3, 4])  >>> x = [3, 4, 2, 1] >>> np.array(x)[np.argpartition(x, 3)] array([2, 1, 3, 4])  \"",
        "name": "index_array",
        "type": "ndarray"
      },
      "tags": [
        "core",
        "fromnumeric"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.eigen.arpack.arpack.svds",
      "description": "'Compute the largest k singular values/vectors for a sparse matrix.\n",
      "id": "scipy.sparse.linalg.eigen.arpack.arpack.svds",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.eigen.arpack.arpack.svds",
      "parameters": [
        {
          "description": "Array to compute the SVD on, of shape (M, N)",
          "name": "A",
          "type": "sparse matrix, LinearOperator"
        },
        {
          "description": "Number of singular values and vectors to compute. Must be 1 <= k < min(A.shape).",
          "name": "k",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of Lanczos vectors generated ncv must be greater than k+1 and smaller than n; it is recommended that ncv > 2*k Default: ``min(n, 2*k + 1)``",
          "name": "ncv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Tolerance for singular values. Zero (default) means machine precision.",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Which `k` singular values to find:  - \\'LM\\' : largest singular values - \\'SM\\' : smallest singular values  .. versionadded:: 0.12.0 v0 : ndarray, optional Starting vector for iteration, of length min(A.shape). Should be an (approximate) left singular vector if N > M and a right singular vector otherwise. Default: random  .. versionadded:: 0.12.0",
          "name": "which",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "Maximum number of iterations.  .. versionadded:: 0.12.0",
          "name": "maxiter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "- True: return singular vectors (True) in addition to singular values.  .. versionadded:: 0.12.0  - \"u\": only return the u matrix, without computing vh (if N > M). - \"vh\": only return the vh matrix, without computing u (if N <= M).  .. versionadded:: 0.16.0 ",
          "name": "return_singular_vectors",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Unitary matrix having left singular vectors as columns. If `return_singular_vectors` is \"vh\", this variable is not computed, and None is returned instead. s : ndarray, shape=(k,) The singular values. vt : ndarray, shape=(k, N) Unitary matrix having right singular vectors as rows. If `return_singular_vectors` is \"u\", this variable is not computed, and None is returned instead.   Notes ----- This is a naive implementation using ARPACK as an eigensolver on A.H * A or A * A.H, depending on which one is more efficient.  '",
        "name": "u",
        "shape": "M, k",
        "type": "ndarray"
      },
      "tags": [
        "sparse",
        "linalg",
        "eigen",
        "arpack",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Feature scores between 0 and 1. ",
          "name": "scores_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max of         ``all_scores_``. ",
          "name": "all_scores_",
          "shape": "n_features, n_reg_parameter",
          "type": "array"
        }
      ],
      "category": "linear_model.randomized_l1",
      "common_name": "Randomized Lasso",
      "description": "\"Randomized Lasso.\n\nRandomized Lasso works by subsampling the training data and\ncomputing a Lasso estimate where the penalty of a random subset of\ncoefficients has been scaled. By performing this double\nrandomization several times, the method assigns high scores to\nfeatures that are repeatedly selected across randomizations. This\nis known as stability selection. In short, features selected more\noften are considered good features.\n\nRead more in the :ref:`User Guide <randomized_l1>`.\n",
      "id": "sklearn.linear_model.randomized_l1.RandomizedLasso",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Return a mask, or list, of the features/indices selected.'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.get_support",
          "name": "get_support",
          "parameters": []
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.inverse_transform",
          "name": "inverse_transform",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLasso.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.linear_model.randomized_l1.RandomizedLasso",
      "parameters": [
        {
          "description": "The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The s parameter used to randomly scale the penalty of different features (See :ref:`User Guide <randomized_l1>` for details ). Should be between 0 and 1. ",
          "name": "scaling",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. ",
          "name": "sample_fraction",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of randomized models.  selection_threshold: float, optional The score above which features should be selected. ",
          "name": "n_resampling",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "Maximum number of iterations to perform in the Lars algorithm. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the 'tol' parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of CPUs to use during the resampling. If '-1', use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory. ",
          "name": "memory",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/randomized_l1.pyc:187",
      "tags": [
        "linear_model",
        "randomized_l1"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.classification",
      "common_name": "K Neighbors Classifier",
      "description": "\"Classifier implementing the k-nearest neighbors vote.\n\nRead more in the :ref:`User Guide <classification>`.\n",
      "id": "sklearn.neighbors.classification.KNeighborsClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values of shape = [n_samples] or [n_samples, n_outputs]  \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Finds the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.kneighbors",
          "name": "kneighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors to get (default is the value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the lengths to points, only present if return_distance=True  ind : array Indices of the nearest points in the population matrix.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1,1,1]  >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=1) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS (array([[ 0.5]]), array([[2]]...))  As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:  >>> X = [[0., 1., 0.], [1., 0., 1.]] >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS array([[1], [2]]...)  \"",
            "name": "dist",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.kneighbors_graph",
          "name": "kneighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors for each sample. (default is value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=2) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.kneighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- NearestNeighbors.radius_neighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples_fit",
            "type": "sparse"
          }
        },
        {
          "description": "\"Predict the class labels for the provided data\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Class labels for each data sample. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "\"Return probability estimates for the test data X.\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "of such arrays if n_outputs > 1. The class probabilities of the input samples. Classes are ordered by lexicographic order. \"",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.classification.KNeighborsClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.classification.KNeighborsClassifier",
      "parameters": [
        {
          "description": "Number of neighbors to use by default for :meth:`k_neighbors` queries. ",
          "name": "n_neighbors",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. ",
          "name": "weights",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDTree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ",
          "name": "algorithm",
          "optional": "true",
          "type": "'auto', 'ball_tree', 'kd_tree', 'brute'"
        },
        {
          "description": "Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ",
          "name": "p",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Additional keyword arguments for the metric function. ",
          "name": "metric_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. Doesn't affect :meth:`fit` method. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc:23",
      "tags": [
        "neighbors",
        "classification"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "Non-negative components of the data. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Frobenius norm of the matrix difference between the training data and the reconstructed data from the fit produced by the model. ``|| X - WH ||_2`` ",
          "name": "reconstruction_err_",
          "type": "number"
        },
        {
          "description": "Actual number of iterations. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.nmf",
      "common_name": "Projected Gradient NMF",
      "description": "'Non-Negative Matrix Factorization (NMF)\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is::\n\n0.5 * ||X - WH||_Fro^2\n+ alpha * l1_ratio * ||vec(W)||_1\n+ alpha * l1_ratio * ||vec(H)||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\nWhere::\n\n||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)\n||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.\n",
      "id": "sklearn.decomposition.nmf.ProjectedGradientNMF",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Learn a NMF model for the data X.\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data matrix to be decomposed ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "\"Learn a NMF model for the data X and returns the transformed data.\n\nThis is more efficient than calling fit followed by transform.\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Data matrix to be decomposed  W : array-like, shape (n_samples, n_components) If init='custom', it is used as initial guess for the solution.  H : array-like, shape (n_components, n_features) If init='custom', it is used as initial guess for the solution. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Transformed data. \"",
            "name": "W: array, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform data back to its original space.\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Transformed data matrix ",
              "name": "W",
              "shape": "n_samples, n_components",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Data matrix of original shape  .. versionadded:: 0.18 '",
            "name": "X: {array-like, sparse matrix}, shape (n_samples, n_features)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform the data X according to the fitted NMF model\n",
          "id": "sklearn.decomposition.nmf.ProjectedGradientNMF.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Data matrix to be transformed by the model ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Transformed data '",
            "name": "W: array, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.decomposition.nmf.ProjectedGradientNMF",
      "parameters": [
        {
          "description": "Number of components, if n_components is not set all features are kept. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Method used to initialize the procedure. Default: \\'nndsvdar\\' if n_components < n_features, otherwise random. Valid options:  - \\'random\\': non-negative random matrices, scaled with: sqrt(X.mean() / n_components)  - \\'nndsvd\\': Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)  - \\'nndsvda\\': NNDSVD with zeros filled with the average of X (better when sparsity is not desired)  - \\'nndsvdar\\': NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)  - \\'custom\\': use custom matrices W and H ",
          "name": "init",
          "type": ""
        },
        {
          "description": "Numerical solver to use: \\'pg\\' is a Projected Gradient solver (deprecated). \\'cd\\' is a Coordinate Descent solver (recommended).  .. versionadded:: 0.17 Coordinate Descent solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. ",
          "name": "solver",
          "type": ""
        },
        {
          "description": "Tolerance value used in stopping conditions. ",
          "name": "tol",
          "type": "double"
        },
        {
          "description": "Number of iterations to compute. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "Random number generator seed control. ",
          "name": "random_state",
          "type": "integer"
        },
        {
          "description": "Constant that multiplies the regularization terms. Set it to zero to have no regularization.  .. versionadded:: 0.17 *alpha* used in the Coordinate Descent solver.  l1_ratio : double, default: 0. The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.  .. versionadded:: 0.17 Regularization parameter *l1_ratio* used in the Coordinate Descent solver. ",
          "name": "alpha",
          "type": "double"
        },
        {
          "description": "If true, randomize the order of coordinates in the CD solver.  .. versionadded:: 0.17 *shuffle* parameter used in the Coordinate Descent solver. ",
          "name": "shuffle",
          "type": "boolean"
        },
        {
          "description": "Number of iterations in NLS subproblem. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "nls_max_iter",
          "type": "integer"
        },
        {
          "description": "Where to enforce sparsity in the model. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "sparseness",
          "type": ""
        },
        {
          "description": "Degree of sparseness, if sparseness is not None. Larger values mean more sparseness. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "beta",
          "type": "double"
        },
        {
          "description": "Degree of correctness to maintain, if sparsity is not None. Smaller values mean larger error. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "eta",
          "type": "double"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc:1111",
      "tags": [
        "decomposition",
        "nmf"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "Weights for each estimator in the boosted ensemble. ",
          "name": "estimator_weights_",
          "type": "array"
        },
        {
          "description": "Regression error for each estimator in the boosted ensemble. ",
          "name": "estimator_errors_",
          "type": "array"
        },
        {
          "description": "The feature importances if supported by the ``base_estimator``.  See also -------- AdaBoostClassifier, GradientBoostingRegressor, DecisionTreeRegressor ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        }
      ],
      "category": "ensemble.weight_boosting",
      "common_name": "Ada Boost Regressor",
      "description": "'An AdaBoost regressor.\n\nAn AdaBoost [1] regressor is a meta-estimator that begins by fitting a\nregressor on the original dataset and then fits additional copies of the\nregressor on the same dataset but where the weights of instances are\nadjusted according to the error of the current prediction. As such,\nsubsequent regressors focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost.R2 [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Build a boosted regressor from the training set (X, y).\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (real numbers). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, the sample weights are initialized to 1 / n_samples. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression value for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted regression values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Return staged predictions for X.\n\nThe predicted regression value of an input sample is computed\nas the weighted median prediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.staged_predict",
          "name": "staged_predict",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted regression values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'Return staged scores for X, y.\n\nThis generator method yields the ensemble score after each iteration of\nboosting and therefore allows monitoring, such as to determine the\nscore on a test set after each boost.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostRegressor.staged_score",
          "name": "staged_score",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "z",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.ensemble.weight_boosting.AdaBoostRegressor",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "DecisionTreeRegressor",
          "description": "The base estimator from which the boosted ensemble is built. Support for sample weighting is required. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "default": "50",
          "description": "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "1.",
          "description": "Learning rate shrinks the contribution of each regressor by ``learning_rate``. There is a trade-off between ``learning_rate`` and ``n_estimators``. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "\\'linear\\'",
          "description": "The loss function to use when updating the weights after each boosting iteration. ",
          "name": "loss",
          "optional": "true",
          "type": "\\'linear\\', \\'square\\', \\'exponential\\'"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc:848",
      "tags": [
        "ensemble",
        "weight_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Coefficients of the regression model (mean of distribution) ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "estimated precision of the noise. ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "estimated precisions of the weights. ",
          "name": "lambda_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "estimated variance-covariance matrix of the weights ",
          "name": "sigma_",
          "shape": "n_features, n_features",
          "type": "array"
        },
        {
          "description": "if computed, value of the objective function (to be maximized) ",
          "name": "scores_",
          "type": "float"
        }
      ],
      "category": "linear_model.bayes",
      "common_name": "ARD Regression",
      "description": "'Bayesian ARD regression.\n\nFit the weights of a regression model, using an ARD prior. The weights of\nthe regression model are assumed to be in Gaussian distributions.\nAlso estimate the parameters lambda (precisions of the distributions of the\nweights) and alpha (precision of the distribution of the noise).\nThe estimation is done by an iterative procedures (Evidence Maximization)\n\nRead more in the :ref:`User Guide <bayesian_regression>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.linear_model.bayes.ARDRegression",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the ARDRegression model according to the given training data\nand parameters.\n\nIterative procedure to maximize the evidence\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features.",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers) ",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.bayes.ARDRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.bayes.ARDRegression",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "description": "Maximum number of iterations. Default is 300 ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stop the algorithm if w has converged. Default is 1.e-3.  alpha_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.  alpha_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6.  lambda_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6.  lambda_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If True, compute the objective function at each step of the model. Default is False. ",
          "name": "compute_score",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "threshold for removing (pruning) weights with high precision from the computation. Default is 1.e+4. ",
          "name": "threshold_lambda",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). Default is True. ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True. If True, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Verbose mode when fitting the model. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/bayes.pyc:228",
      "tags": [
        "linear_model",
        "bayes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the cost function formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "``sparse_coef_`` is a readonly property derived from ``coef_`` ",
          "name": "sparse_coef_",
          "shape": "n_features, 1",
          "type": "scipy"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance. ",
          "name": "n_iter_",
          "shape": "n_targets,",
          "type": "array-like"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Elastic Net",
      "description": "'Linear regression with combined L1 and L2 priors as regularizer.\n\nMinimizes the objective function::\n\n1 / (2 * n_samples) * ||y - Xw||^2_2\n+ alpha * l1_ratio * ||w||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nIf you are interested in controlling the L1 and L2 penalty\nseparately, keep in mind that this is equivalent to::\n\na * L1 + b * L2\n\nwhere::\n\nalpha = a + b and l1_ratio = a / (a + b)\n\nThe parameter l1_ratio corresponds to alpha in the glmnet R package while\nalpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n= 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\nunless you supply your own sequence of alpha.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.ElasticNet",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDecision function of the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "The predicted decision function '",
            "name": "T",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Fit model with coordinate descent.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data ",
              "name": "X",
              "type": "ndarray"
            },
            {
              "description": "Target ",
              "name": "y",
              "shape": "n_samples,",
              "type": "ndarray"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  Notes -----  Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary.  To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. \"",
              "name": "check_input",
              "type": "boolean"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNet.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.ElasticNet",
      "parameters": [
        {
          "description": "Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter.``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object.  l1_ratio : float The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Whether the intercept should be estimated or not. If ``False``, the data is assumed to be already centered. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The maximum number of iterations  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to ``True``, forces the coefficients to be positive. ",
          "name": "positive",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to \\'random\\', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to \\'random\\') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to \\'random\\'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:503",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The amount of penalization chosen by cross validation  l1_ratio_ : float The compromise between l1 and l2 penalization chosen by cross validation ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "Parameter vector (w in the cost function formula), ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Independent term in the decision function. ",
          "name": "intercept_",
          "shape": "n_targets, n_features",
          "type": "float"
        },
        {
          "description": "Mean square error for the test set on each fold, varying l1_ratio and alpha. ",
          "name": "mse_path_",
          "shape": "n_l1_ratio, n_alpha, n_folds",
          "type": "array"
        },
        {
          "description": "The grid of alphas used for fitting, for each l1_ratio. ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "numpy"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Elastic Net CV",
      "description": "\"Elastic Net model with iterative fitting along a regularization path\n\nThe best model is selected by cross-validation.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model with coordinate descent\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Pass directly as float64, Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.ElasticNetCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.ElasticNetCV",
      "parameters": [
        {
          "description": "float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For ``l1_ratio = 0`` the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]`` ",
          "name": "l1_ratio",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of alphas along the regularization path, used for each l1_ratio. ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "List of alphas where to compute the models. If None alphas are set automatically ",
          "name": "alphas",
          "optional": "true",
          "type": "numpy"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "When set to ``True``, forces the coefficients to be positive. ",
          "name": "positive",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1367",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "externals.joblib.parallel",
      "common_name": "Parallel",
      "description": "' Helper class for readable parallel mapping.\n",
      "id": "sklearn.externals.joblib.parallel.Parallel",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "None",
          "id": "sklearn.externals.joblib.parallel.Parallel.debug",
          "name": "debug",
          "parameters": []
        },
        {
          "description": "'Dispatch more data for parallel processing\n\nThis method is meant to be called concurrently by the multiprocessing\ncallback. We rely on the thread-safety of dispatch_one_batch to protect\nagainst concurrent consumption of the unprotected iterator.\n\n'",
          "id": "sklearn.externals.joblib.parallel.Parallel.dispatch_next",
          "name": "dispatch_next",
          "parameters": []
        },
        {
          "description": "'Prefetch the tasks for the next batch and dispatch them.\n\nThe effective size of the batch is computed here.\nIf there are no more jobs to dispatch, return False, else return True.\n\nThe iterator consumption and dispatching is protected by the same\nlock so calling this function should be thread safe.\n\n'",
          "id": "sklearn.externals.joblib.parallel.Parallel.dispatch_one_batch",
          "name": "dispatch_one_batch",
          "parameters": []
        },
        {
          "description": "' Return the formated representation of the object.\n'",
          "id": "sklearn.externals.joblib.parallel.Parallel.format",
          "name": "format",
          "parameters": []
        },
        {
          "description": "'Display the process of the parallel execution only a fraction\nof time, controlled by self.verbose.\n'",
          "id": "sklearn.externals.joblib.parallel.Parallel.print_progress",
          "name": "print_progress",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.parallel.Parallel.retrieve",
          "name": "retrieve",
          "parameters": []
        },
        {
          "description": "None",
          "id": "sklearn.externals.joblib.parallel.Parallel.warn",
          "name": "warn",
          "parameters": []
        }
      ],
      "name": "sklearn.externals.joblib.parallel.Parallel",
      "parameters": [
        {
          "description": "The maximum number of concurrently running jobs, such as the number of Python worker processes when backend=\"multiprocessing\" or the size of the thread-pool when backend=\"threading\". If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. backend: str or None, default: \\'multiprocessing\\' Specify the parallelization backend implementation. Supported backends are: - \"multiprocessing\" used by default, can induce some communication and memory overhead when exchanging input and output data with the worker Python processes. - \"threading\" is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the called function relies a lot on Python objects. \"threading\" is mostly useful when the execution bottleneck is a compiled extension that explicitly releases the GIL (for instance a Cython loop wrapped in a \"with nogil\" block or an expensive call to a library such as NumPy). - finally, you can register backends by calling register_parallel_backend. This will allow you to implement a backend of your liking. verbose: int, optional The verbosity level: if non zero, progress messages are printed. Above 50, the output is sent to stdout. The frequency of the messages increases with the verbosity level. If it more than 10, all iterations are reported. timeout: float, optional Timeout limit for each task to complete.  If any task takes longer a TimeOutError will be raised. Only applied when n_jobs != 1 pre_dispatch: {\\'all\\', integer, or expression, as in \\'3*n_jobs\\'} The number of batches (of tasks) to be pre-dispatched. Default is \\'2*n_jobs\\'. When batch_size=\"auto\" this is reasonable default and the multiprocessing workers should never starve. batch_size: int or \\'auto\\', default: \\'auto\\' The number of atomic tasks to dispatch at once to each worker. When individual evaluations are very fast, multiprocessing can be slower than sequential computation because of the overhead. Batching fast computations together can mitigate this. The ``\\'auto\\'`` strategy keeps track of the time it takes for a batch to complete, and dynamically adjusts the batch size to keep the time on the order of half a second, using a heuristic. The initial batch size is 1. ``batch_size=\"auto\"`` with ``backend=\"threading\"`` will dispatch batches of a single task at a time as the threading backend has very little overhead and using larger batch size has not proved to bring any gain in that case. temp_folder: str, optional Folder to be used by the pool for memmaping large arrays for sharing memory with worker processes. If None, this will try in order: - a folder pointed by the JOBLIB_TEMP_FOLDER environment variable, - /dev/shm if the folder exists and is writable: this is a RAMdisk filesystem available by default on modern Linux distributions, - the default system temporary folder that can be overridden with TMP, TMPDIR or TEMP environment variables, typically /tmp under Unix operating systems. Only active when backend=\"multiprocessing\". max_nbytes int, str, or None, optional, 1M by default Threshold on the size of arrays passed to the workers that triggers automated memory mapping in temp_folder. Can be an int in Bytes, or a human-readable string, e.g., \\'1M\\' for 1 megabyte. Use None to disable memmaping of large arrays. Only active when backend=\"multiprocessing\". mmap_mode: {None, \\'r+\\', \\'r\\', \\'w+\\', \\'c\\'} Memmapping mode for numpy arrays passed to workers. See \\'max_nbytes\\' parameter documentation for more details.  Notes -----  This object uses the multiprocessing module to compute in parallel the application of a function to many different arguments. The main functionality it brings in addition to using the raw multiprocessing API are (see examples for details):  * More readable code, in particular since it avoids constructing list of arguments.  * Easier debugging: - informative tracebacks even when the error happens on the client side - using \\'n_jobs=1\\' enables to turn off parallel computing for debugging without changing the codepath - early capture of pickling errors  * An optional progress meter.  * Interruption of multiprocesses jobs with \\'Ctrl-C\\'  * Flexible pickling control for the communication to and from the worker processes.  * Ability to use shared memory efficiently with worker processes for large numpy-based datastructures. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc:272",
      "tags": [
        "externals",
        "joblib",
        "parallel"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Smoothed empirical log probability for each class. ",
          "name": "class_log_prior_",
          "shape": "n_classes, ",
          "type": "array"
        },
        {
          "description": "Mirrors ``class_log_prior_`` for interpreting MultinomialNB as a linear model. ",
          "name": "intercept_",
          "type": "property"
        },
        {
          "description": "Empirical log probability of features given a class, ``P(x_i|y)``. ",
          "name": "feature_log_prob_",
          "shape": "n_classes, n_features",
          "type": "array"
        },
        {
          "description": "Mirrors ``feature_log_prob_`` for interpreting MultinomialNB as a linear model. ",
          "name": "coef_",
          "type": "property"
        },
        {
          "description": "Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided. ",
          "name": "class_count_",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided. ",
          "name": "feature_count_",
          "shape": "n_classes, n_features",
          "type": "array"
        }
      ],
      "category": "naive_bayes",
      "common_name": "Multinomial NB",
      "description": "'\nNaive Bayes classifier for multinomial models\n\nThe multinomial Naive Bayes classifier is suitable for classification with\ndiscrete features (e.g., word counts for text classification). The\nmultinomial distribution normally requires integer feature counts. However,\nin practice, fractional counts such as tf-idf may also work.\n\nRead more in the :ref:`User Guide <multinomial_naive_bayes>`.\n",
      "id": "sklearn.naive_bayes.MultinomialNB",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Naive Bayes classifier according to X, y\n",
          "id": "sklearn.naive_bayes.MultinomialNB.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls. ",
              "name": "classes",
              "optional": "true",
              "shape": "n_classes",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'\nPerform classification on an array of test vectors X.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log-probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.naive_bayes.MultinomialNB.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.naive_bayes.MultinomialNB",
      "parameters": [
        {
          "default": "1.0",
          "description": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to learn class prior probabilities or not. If false, a uniform prior will be used. ",
          "name": "fit_prior",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "Prior probabilities of the classes. If specified the priors are not adjusted according to the data. ",
          "name": "class_prior",
          "optional": "true",
          "size": "n_classes,",
          "type": "array-like"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc:606",
      "tags": [
        "naive_bayes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The amount of penalization chosen by cross validation ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "parameter vector (w in the cost function formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "mean square error for the test set on each fold, varying alpha ",
          "name": "mse_path_",
          "shape": "n_alphas, n_folds",
          "type": "array"
        },
        {
          "description": "The grid of alphas used for fitting ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "numpy"
        },
        {
          "description": "The dual gap at the end of the optimization for the optimal alpha (``alpha_``). ",
          "name": "dual_gap_",
          "shape": "",
          "type": "ndarray"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Lasso CV",
      "description": "\"Lasso linear model with iterative fitting along a regularization path\n\nThe best model is selected by cross-validation.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.LassoCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model with coordinate descent\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Pass directly as float64, Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.LassoCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.LassoCV",
      "parameters": [
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of alphas along the regularization path ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "List of alphas where to compute the models. If ``None`` alphas are set automatically ",
          "name": "alphas",
          "optional": "true",
          "type": "numpy"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "If positive, restrict regression coefficients to be positive ",
          "name": "positive",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1221",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the formulation formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "type": "float"
        },
        {
          "description": "the varying values of the coefficients along the path ",
          "name": "coef_path_",
          "shape": "n_features, n_alphas",
          "type": "array"
        },
        {
          "description": "the estimated regularization parameter alpha ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "the different values of alpha along the path ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "array"
        },
        {
          "description": "all the values of alpha along the path for the different folds ",
          "name": "cv_alphas_",
          "shape": "n_cv_alphas,",
          "type": "array"
        },
        {
          "description": "the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``) ",
          "name": "cv_mse_path_",
          "shape": "n_folds, n_cv_alphas",
          "type": "array"
        },
        {
          "description": "the number of iterations run by Lars with the optimal alpha. ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "linear_model.least_angle",
      "common_name": "Lasso Lars CV",
      "description": "\"Cross-validated Lasso, using the LARS algorithm\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.LassoLarsCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.least_angle.LassoLarsCV",
      "parameters": [
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "Maximum number of iterations to perform. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation ",
          "name": "max_n_alphas",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.pyc:1169",
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "linear_model.perceptron",
      "common_name": "Perceptron",
      "description": "'Perceptron\n\nRead more in the :ref:`User Guide <perceptron>`.\n",
      "id": "sklearn.linear_model.perceptron.Perceptron",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_classes, n_features",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of the training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of the target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained by via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to contain all labels in `classes`. ",
              "name": "classes",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.perceptron.Perceptron.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.linear_model.perceptron.Perceptron.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.linear_model.perceptron.Perceptron",
      "parameters": [
        {
          "description": "The penalty (aka regularization term) to be used. Defaults to None. ",
          "name": "penalty",
          "type": ""
        },
        {
          "description": "Constant that multiplies the regularization term if regularization is used. Defaults to 0.0001 ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The number of passes over the training data (aka epochs). Defaults to 5. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether or not the training data should be shuffled after each epoch. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The verbosity level ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means \\'all CPUs\\'. Defaults to 1.  eta0 : double Constant by which the updates are multiplied. Defaults to 1. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "type": "dict"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/perceptron.pyc:8",
      "tags": [
        "linear_model",
        "perceptron"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Per feature relative scaling of the data.  .. versionadded:: 0.17 *scale_* is recommended instead of deprecated *std_*. ",
          "name": "scale_",
          "shape": "n_features,",
          "type": "ndarray"
        },
        {
          "description": "The mean value for each feature in the training set. ",
          "name": "mean_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The variance for each feature in the training set. Used to compute `scale_` ",
          "name": "var_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls.  See also -------- scale: Equivalent function without the object oriented API.  :class:`sklearn.decomposition.PCA` Further removes the linear correlation across features with 'whiten=True'.",
          "name": "n_samples_seen_",
          "type": "int"
        }
      ],
      "category": "preprocessing.data",
      "common_name": "Standard Scaler",
      "description": "\"Standardize features by removing the mean and scaling to unit variance\n\nCentering and scaling happen independently on each feature by computing\nthe relevant statistics on the samples in the training set. Mean and\nstandard deviation are then stored to be used on later data using the\n`transform` method.\n\nStandardization of a dataset is a common requirement for many\nmachine learning estimators: they might behave badly if the\nindividual feature do not more or less look like standard normally\ndistributed data (e.g. Gaussian with 0 mean and unit variance).\n\nFor instance many elements used in the objective function of\na learning algorithm (such as the RBF kernel of Support Vector\nMachines or the L1 and L2 regularizers of linear models) assume that\nall features are centered around 0 and have variance in the same\norder. If a feature has a variance that is orders of magnitude larger\nthat others, it might dominate the objective function and make the\nestimator unable to learn from other features correctly as expected.\n\nThis scaler can also be applied to sparse CSR or CSC matrices by passing\n`with_mean=False` to avoid breaking the sparsity structure of the data.\n\nRead more in the :ref:`User Guide <preprocessing_scaler>`.\n",
      "id": "sklearn.preprocessing.data.StandardScaler",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the mean and std to be used for later scaling.\n",
          "id": "sklearn.preprocessing.data.StandardScaler.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The data used to compute the mean and standard deviation used for later scaling along the features axis.  y: Passthrough for ``Pipeline`` compatibility. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.preprocessing.data.StandardScaler.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.preprocessing.data.StandardScaler.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Scale back the data to the original representation\n",
          "id": "sklearn.preprocessing.data.StandardScaler.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The data used to scale along the features axis. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Online computation of mean and std on X for later scaling.\nAll of X is processed as a single batch. This is intended for cases\nwhen `fit` is not feasible due to very large number of `n_samples`\nor because X is read from a continuous stream.\n\nThe algorithm for incremental mean and std is given in Equation 1.5a,b\nin Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. \"Algorithms\nfor computing the sample variance: Analysis and recommendations.\"\nThe American Statistician 37.3 (1983): 242-247:\n",
          "id": "sklearn.preprocessing.data.StandardScaler.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "The data used to compute the mean and standard deviation used for later scaling along the features axis.  y: Passthrough for ``Pipeline`` compatibility. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.preprocessing.data.StandardScaler.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Perform standardization by centering and scaling\n",
          "id": "sklearn.preprocessing.data.StandardScaler.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The data used to scale along the features axis. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        }
      ],
      "name": "sklearn.preprocessing.data.StandardScaler",
      "parameters": [
        {
          "description": "If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. ",
          "name": "with_mean",
          "type": "boolean"
        },
        {
          "description": "If True, scale the data to unit variance (or equivalently, unit standard deviation). ",
          "name": "with_std",
          "type": "boolean"
        },
        {
          "description": "If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. ",
          "name": "copy",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/preprocessing/data.pyc:449",
      "tags": [
        "preprocessing",
        "data"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Log probability of each class (smoothed). ",
          "name": "class_log_prior_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Empirical log probability of features given a class, P(x_i|y). ",
          "name": "feature_log_prob_",
          "shape": "n_classes, n_features",
          "type": "array"
        },
        {
          "description": "Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided. ",
          "name": "class_count_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided. ",
          "name": "feature_count_",
          "shape": "n_classes, n_features",
          "type": "array"
        }
      ],
      "category": "naive_bayes",
      "common_name": "Bernoulli NB",
      "description": "'Naive Bayes classifier for multivariate Bernoulli models.\n\nLike MultinomialNB, this classifier is suitable for discrete data. The\ndifference is that while MultinomialNB works with occurrence counts,\nBernoulliNB is designed for binary/boolean features.\n\nRead more in the :ref:`User Guide <bernoulli_naive_bayes>`.\n",
      "id": "sklearn.naive_bayes.BernoulliNB",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Naive Bayes classifier according to X, y\n",
          "id": "sklearn.naive_bayes.BernoulliNB.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance overhead hence it is better to call\npartial_fit on chunks of data that are as large as possible\n(as long as fitting in the memory budget) to hide the overhead.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls. ",
              "name": "classes",
              "optional": "true",
              "shape": "n_classes",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'\nPerform classification on an array of test vectors X.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log-probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.naive_bayes.BernoulliNB.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.naive_bayes.BernoulliNB",
      "parameters": [
        {
          "default": "1.0",
          "description": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "0.0",
          "description": "Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. ",
          "name": "binarize",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to learn class prior probabilities or not. If false, a uniform prior will be used. ",
          "name": "fit_prior",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "Prior probabilities of the classes. If specified the priors are not adjusted according to the data. ",
          "name": "class_prior",
          "optional": "true",
          "size": "n_classes,",
          "type": "array-like"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc:711",
      "tags": [
        "naive_bayes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "X block weights vectors. ",
          "name": "x_weights_",
          "type": "array"
        },
        {
          "description": "Y block weights vectors. ",
          "name": "y_weights_",
          "type": "array"
        },
        {
          "description": "X block loadings vectors. ",
          "name": "x_loadings_",
          "type": "array"
        },
        {
          "description": "Y block loadings vectors. ",
          "name": "y_loadings_",
          "type": "array"
        },
        {
          "description": "X scores. ",
          "name": "x_scores_",
          "type": "array"
        },
        {
          "description": "Y scores. ",
          "name": "y_scores_",
          "type": "array"
        },
        {
          "description": "X block to latents rotations. ",
          "name": "x_rotations_",
          "type": "array"
        },
        {
          "description": "Y block to latents rotations. ",
          "name": "y_rotations_",
          "type": "array"
        },
        {
          "description": "Number of iterations of the NIPALS inner loop for each component.  Notes ----- For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that ``|u| = |v| = 1``  Note that it maximizes only the correlations between the scores.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "cross_decomposition.cca_",
      "common_name": "CCA",
      "description": "'CCA Canonical Correlation Analysis.\n\nCCA inherits from PLS with mode=\"B\" and deflation_mode=\"canonical\".\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n",
      "id": "sklearn.cross_decomposition.cca_.CCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit model to data.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and n_features is the number of predictors.  Y : array-like of response, shape = [n_samples, n_targets] Target vectors, where n_samples in the number of samples and n_targets is the number of response variables. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Learn and apply the dimension reduction on the train data.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization.  Notes ----- This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. '",
              "name": "copy",
              "type": "boolean"
            }
          ]
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.cca_.CCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        }
      ],
      "name": "sklearn.cross_decomposition.cca_.CCA",
      "parameters": [
        {
          "description": "number of components to keep. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "whether to scale the data? ",
          "name": "scale",
          "type": "boolean"
        },
        {
          "description": "the maximum number of iterations of the NIPALS inner loop ",
          "name": "max_iter",
          "type": "an"
        },
        {
          "description": "the tolerance used in the iterative algorithm ",
          "name": "tol",
          "type": "non-negative"
        },
        {
          "description": "Whether the deflation be done on a copy. Let the default value to True unless you don\\'t care about side effects ",
          "name": "copy",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_decomposition/cca_.pyc:6",
      "tags": [
        "cross_decomposition",
        "cca_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "X block weights vectors. ",
          "name": "x_weights_",
          "type": "array"
        },
        {
          "description": "Y block weights vectors. ",
          "name": "y_weights_",
          "type": "array"
        },
        {
          "description": "X block loadings vectors. ",
          "name": "x_loadings_",
          "type": "array"
        },
        {
          "description": "Y block loadings vectors. ",
          "name": "y_loadings_",
          "type": "array"
        },
        {
          "description": "X scores. ",
          "name": "x_scores_",
          "type": "array"
        },
        {
          "description": "Y scores. ",
          "name": "y_scores_",
          "type": "array"
        },
        {
          "description": "X block to latents rotations. ",
          "name": "x_rotations_",
          "type": "array"
        },
        {
          "description": "Y block to latents rotations.  coef_: array, [p, q] The coefficients of the linear model: ``Y = X coef_ + Err`` ",
          "name": "y_rotations_",
          "type": "array"
        },
        {
          "description": "Number of iterations of the NIPALS inner loop for each component.  Notes ----- Matrices::  T: x_scores_ U: y_scores_ W: x_weights_ C: y_weights_ P: x_loadings_ Q: y_loadings__  Are computed such that::  X = T P.T + Err and Y = U Q.T + Err T[:, k] = Xk W[:, k] for k in range(n_components) U[:, k] = Yk C[:, k] for k in range(n_components) x_rotations_ = W (P.T W)^(-1) y_rotations_ = C (Q.T C)^(-1)  where Xk and Yk are residual matrices at iteration k.  `Slides explaining PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`  For each component k, find weights u, v that optimizes: ``max corr(Xk u, Yk v) * std(Xk u) std(Yk u)``, such that ``|u| = 1``  Note that it maximizes both the correlations between the scores and the intra-block variances.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented.  This implementation provides the same results that 3 PLS packages provided in the R language (R-project):  - \"mixOmics\" with function pls(X, Y, mode = \"regression\") - \"plspm \" with function plsreg2(X, Y) - \"pls\" with function oscorespls.fit(X, Y) ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "cross_decomposition.pls_",
      "common_name": "PLS Regression",
      "description": "'PLS regression\n\nPLSRegression implements the PLS 2 blocks regression known as PLS2 or PLS1\nin case of one dimensional response.\nThis class inherits from _PLS with mode=\"A\", deflation_mode=\"regression\",\nnorm_y_weights=False and algorithm=\"nipals\".\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n",
      "id": "sklearn.cross_decomposition.pls_.PLSRegression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit model to data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and n_features is the number of predictors.  Y : array-like of response, shape = [n_samples, n_targets] Target vectors, where n_samples in the number of samples and n_targets is the number of response variables. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Learn and apply the dimension reduction on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization.  Notes ----- This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. '",
              "name": "copy",
              "type": "boolean"
            }
          ]
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSRegression.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        }
      ],
      "name": "sklearn.cross_decomposition.pls_.PLSRegression",
      "parameters": [
        {
          "description": "Number of components to keep. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "whether to scale the data ",
          "name": "scale",
          "type": "boolean"
        },
        {
          "description": "the maximum number of iterations of the NIPALS inner loop (used only if algorithm=\"nipals\") ",
          "name": "max_iter",
          "type": "an"
        },
        {
          "description": "Tolerance used in the iterative algorithm default 1e-06. ",
          "name": "tol",
          "type": "non-negative"
        },
        {
          "description": "Whether the deflation should be done on a copy. Let the default value to True unless you don\\'t care about side effect ",
          "name": "copy",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.pyc:465",
      "tags": [
        "cross_decomposition",
        "pls_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix. ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. (stored only if store_precision is True) ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "coefficient in the convex combination used for the computation of the shrunk estimate. ",
          "name": "shrinkage_",
          "type": "float"
        }
      ],
      "category": "covariance.shrunk_covariance_",
      "common_name": "OAS",
      "description": "'Oracle Approximating Shrinkage Estimator\n\nRead more in the :ref:`User Guide <shrunk_covariance>`.\n\nOAS is a particular form of shrinkage described in\n\"Shrinkage Algorithms for MMSE Covariance Estimation\"\nChen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.\n\nThe formula used here does not correspond to the one given in the\narticle. It has been taken from the Matlab program available from the\nauthors\\' webpage (http://tbayes.eecs.umich.edu/yilun/covestimation).\nIn the original article, formula (23) states that 2/p is multiplied by\nTrace(cov*cov) in both the numerator and denominator, this operation is omitted\nin the author\\'s MATLAB program because for a large p, the value of 2/p is so\nsmall that it doesn\\'t affect the value of the estimator.\n",
      "id": "sklearn.covariance.shrunk_covariance_.OAS",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "' Fits the Oracle Approximating Shrinkage covariance model\naccording to the given training data and parameters.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features.",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self: object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.shrunk_covariance_.OAS.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.shrunk_covariance_.OAS",
      "parameters": [
        {
          "description": "Specify if the estimated precision is stored.  assume_centered: bool, default=False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation. ",
          "name": "store_precision",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/shrunk_covariance_.pyc:477",
      "tags": [
        "covariance",
        "shrunk_covariance_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "Topic word distribution. ``components_[i, j]`` represents word j in topic `i`. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Number of iterations of the EM step. ",
          "name": "n_batch_iter_",
          "type": "int"
        },
        {
          "description": "Number of passes over the dataset. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.online_lda",
      "common_name": "Latent Dirichlet Allocation",
      "description": "'Latent Dirichlet Allocation with online variational Bayes algorithm\n\n.. versionadded:: 0.17\n\nRead more in the :ref:`User Guide <LatentDirichletAllocation>`.\n",
      "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Learn model for the data X with variational Bayes method.\n\nWhen `learning_method` is 'online', use mini-batch update.\nOtherwise, use batch update.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Document word matrix. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Online VB with Mini-Batch update.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Document word matrix. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Calculate approximate perplexity for data X.\n\nPerplexity is defined as exp(-1. * log-likelihood per word)\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.perplexity",
          "name": "perplexity",
          "parameters": [
            {
              "description": "Document word matrix. ",
              "name": "X",
              "type": "array-like"
            },
            {
              "description": "Document topic distribution. If it is None, it will be generated by applying transform on X. ",
              "name": "doc_topic_distr",
              "shape": "n_samples, n_topics",
              "type": ""
            }
          ],
          "returns": {
            "description": "Perplexity score. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "'Calculate approximate log-likelihood as score.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.score",
          "name": "score",
          "parameters": [
            {
              "description": "Document word matrix. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Use approximate bound as score. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform data X according to the fitted model.\n",
          "id": "sklearn.decomposition.online_lda.LatentDirichletAllocation.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Document word matrix. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Document topic distribution for X. '",
            "name": "doc_topic_distr",
            "shape": "n_samples, n_topics",
            "type": "shape"
          }
        }
      ],
      "name": "sklearn.decomposition.online_lda.LatentDirichletAllocation",
      "parameters": [
        {
          "default": "10",
          "description": "Number of topics. ",
          "name": "n_topics",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Prior of document topic distribution `theta`. If the value is None, defaults to `1 / n_topics`. In the literature, this is called `alpha`. ",
          "name": "doc_topic_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Prior of topic word distribution `beta`. If the value is None, defaults to `1 / n_topics`. In the literature, this is called `eta`. ",
          "name": "topic_word_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Method used to update `_component`. Only used in `fit` method. In general, if the data size is large, the online update will be much faster than the batch update. The default learning method is going to be changed to \\'batch\\' in the 0.20 release. Valid options::  \\'batch\\': Batch variational Bayes method. Use all training data in each EM update. Old `components_` will be overwritten in each iteration. \\'online\\': Online variational Bayes method. In each EM update, use mini-batch of training data to update the ``components_`` variable incrementally. The learning rate is controlled by the ``learning_decay`` and the ``learning_offset`` parameters. ",
          "name": "learning_method",
          "type": ""
        },
        {
          "default": "0.7",
          "description": "It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is ``n_samples``, the update method is same as batch learning. In the literature, this is called kappa. ",
          "name": "learning_decay",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "10.",
          "description": "A (positive) parameter that downweights early iterations in online learning.  It should be greater than 1.0. In the literature, this is called tau_0. ",
          "name": "learning_offset",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "10",
          "description": "The maximum number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "1e6",
          "description": "Total number of documents. Only used in the `partial_fit` method. ",
          "name": "total_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "128",
          "description": "Number of documents to use in each EM iteration. Only used in online learning. ",
          "name": "batch_size",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "How often to evaluate perplexity. Only used in `fit` method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold. ",
          "name": "evaluate_every",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-1",
          "description": "Perplexity tolerance in batch learning. Only used when ``evaluate_every`` is greater than 0. ",
          "name": "perp_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "1e-3",
          "description": "Stopping tolerance for updating document topic distribution in E-step. ",
          "name": "mean_change_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Max number of iterations for updating document topic distribution in the E-step. ",
          "name": "max_doc_update_iter",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The number of jobs to use in the E-step. If -1, all CPUs are used. For ``n_jobs`` below -1, (n_cpus + 1 + n_jobs) are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Pseudo-random number generator seed control. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/online_lda.pyc:137",
      "tags": [
        "decomposition",
        "online_lda"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Weight vector(s). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Intercept term. ",
          "name": "intercept_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Covariance matrix (shared by all classes). ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Percentage of variance explained by each of the selected components. If ``n_components`` is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used. ",
          "name": "explained_variance_ratio_",
          "shape": "n_components,",
          "type": "array"
        },
        {
          "description": "Class means. ",
          "name": "means_",
          "shape": "n_classes, n_features",
          "type": "array-like"
        },
        {
          "description": "Class priors (sum to 1). ",
          "name": "priors_",
          "shape": "n_classes,",
          "type": "array-like"
        },
        {
          "description": "Scaling of the features in the space spanned by the class centroids. ",
          "name": "scalings_",
          "shape": "rank, n_classes - 1",
          "type": "array-like"
        },
        {
          "description": "Overall mean. ",
          "name": "xbar_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "Unique class labels.  See also -------- sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis: Quadratic Discriminant Analysis  Notes ----- The default solver is 'svd'. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the 'svd' solver cannot be used with shrinkage.  The 'lsqr' solver is an efficient algorithm that only works for classification. It supports shrinkage.  The 'eigen' solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the 'eigen' solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features. ",
          "name": "classes_",
          "shape": "n_classes,",
          "type": "array-like"
        }
      ],
      "category": "discriminant_analysis",
      "common_name": "Linear Discriminant Analysis",
      "description": "\"Linear Discriminant Analysis\n\nA classifier with a linear decision boundary, generated by fitting class\nconditional densities to the data and using Bayes' rule.\n\nThe model fits a Gaussian density to each class, assuming that all classes\nshare the same covariance matrix.\n\nThe fitted model can also be used to reduce the dimensionality of the input\nby projecting it to the most discriminative directions.\n\n.. versionadded:: 0.17\n*LinearDiscriminantAnalysis*.\n\nRead more in the :ref:`User Guide <lda_qda>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Fit LinearDiscriminantAnalysis model according to the given\ntraining data and parameters.\n\n.. versionchanged:: 0.17\nDeprecated *store_covariance* have been moved to main constructor.\n\n.. versionchanged:: 0.17\nDeprecated *tol* have been moved to main constructor.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Estimate log probability.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Estimated log probabilities. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Estimate probability.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Estimated probabilities. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Project data to maximize class separation.\n",
          "id": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed data. '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "description": "Solver to use, possible values: - 'svd': Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features. - 'lsqr': Least squares solution, can be combined with shrinkage. - 'eigen': Eigenvalue decomposition, can be combined with shrinkage. ",
          "name": "solver",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Shrinkage parameter, possible values: - None: no shrinkage (default). - 'auto': automatic shrinkage using the Ledoit-Wolf lemma. - float between 0 and 1: fixed shrinkage parameter.  Note that shrinkage works only with 'lsqr' and 'eigen' solvers. ",
          "name": "shrinkage",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Class priors. ",
          "name": "priors",
          "optional": "true",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "Number of components (< n_classes - 1) for dimensionality reduction. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Additionally compute class covariance matrix (default False).  .. versionadded:: 0.17 ",
          "name": "store_covariance",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Threshold used for rank estimation in SVD solver.  .. versionadded:: 0.17 ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.pyc:130",
      "tags": [
        "discriminant_analysis"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Components with maximum variance. ",
          "name": "components_",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "Variance explained by each of the selected components. ",
          "name": "explained_variance_",
          "shape": "n_components,",
          "type": "array"
        },
        {
          "description": "Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0 ",
          "name": "explained_variance_ratio_",
          "shape": "n_components,",
          "type": "array"
        },
        {
          "description": "Per-feature empirical mean, aggregate over calls to ``partial_fit``. ",
          "name": "mean_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Per-feature empirical variance, aggregate over calls to ``partial_fit``. ",
          "name": "var_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See \"Pattern Recognition and Machine Learning\" by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. ",
          "name": "noise_variance_",
          "type": "float"
        },
        {
          "description": "The estimated number of components. Relevant when ``n_components=None``. ",
          "name": "n_components_",
          "type": "int"
        },
        {
          "description": "The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across ``partial_fit`` calls. ",
          "name": "n_samples_seen_",
          "type": "int"
        }
      ],
      "category": "decomposition.incremental_pca",
      "common_name": "Incremental PCA",
      "description": "'Incremental principal components analysis (IPCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of\ncentered data, keeping only the most significant singular vectors to\nproject the data to a lower dimensional space.\n\nDepending on the size of the input data, this algorithm can be much more\nmemory efficient than a PCA.\n\nThis algorithm has constant memory complexity, on the order\nof ``batch_size``, enabling use of np.memmap files without loading the\nentire file into memory.\n\nThe computational overhead of each SVD is\n``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples\nremain in memory at a time. There will be ``n_samples / batch_size`` SVD\ncomputations to get the principal components, versus 1 large SVD of\ncomplexity ``O(n_samples * n_features ** 2)`` for PCA.\n\nRead more in the :ref:`User Guide <IncrementalPCA>`.\n",
      "id": "sklearn.decomposition.incremental_pca.IncrementalPCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model with X, using minibatches of size batch_size.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features.  y: Passthrough for ``Pipeline`` compatibility. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self: object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Compute data covariance with the generative model.\n\n``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\nwhere  S**2 contains the explained variances, and sigma2 contains the\nnoise variances.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.get_covariance",
          "name": "get_covariance",
          "parameters": [],
          "returns": {
            "description": "Estimated covariance of data. '",
            "name": "cov",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Compute data precision matrix with the generative model.\n\nEquals the inverse of the covariance but computed with\nthe matrix inversion lemma for efficiency.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "Estimated precision of data. '",
            "name": "precision",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Transform data back to its original space.\n\nIn other words, return an input X_original whose transform would be X.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "New data, where n_samples is the number of samples and n_components is the number of components. ",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening. '",
            "name": "X_original array-like, shape (n_samples, n_features)"
          }
        },
        {
          "description": "'Incremental fit with X. All of X is processed as a single batch.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self: object"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply dimensionality reduction to X.\n\nX is projected on the first principal components previously extracted\nfrom a training set.\n",
          "id": "sklearn.decomposition.incremental_pca.IncrementalPCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Examples --------  >>> import numpy as np >>> from sklearn.decomposition import IncrementalPCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> ipca = IncrementalPCA(n_components=2, batch_size=3) >>> ipca.fit(X) IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False) >>> ipca.transform(X) # doctest: +SKIP '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.decomposition.incremental_pca.IncrementalPCA",
      "parameters": [
        {
          "description": "Number of components to keep. If ``n_components `` is ``None``, then ``n_components`` is set to ``min(n_samples, n_features)``. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "The number of samples to use for each batch. Only used when calling ``fit``. If ``batch_size`` is ``None``, then ``batch_size`` is inferred from the data and set to ``5 * n_features``, to provide a balance between approximation accuracy and memory consumption. ",
          "name": "batch_size",
          "type": "int"
        },
        {
          "description": "If False, X will be overwritten. ``copy=False`` can be used to save memory but is unsafe for general use. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "When True (False by default) the ``components_`` vectors are divided by ``n_samples`` times ``components_`` to ensure uncorrelated outputs with unit component-wise variances.  Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions. ",
          "name": "whiten",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/incremental_pca.pyc:15",
      "tags": [
        "decomposition",
        "incremental_pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``n_nonzero_coefs`` or ``n_features``,         whichever is smaller. ",
          "name": "alphas_",
          "shape": "n_alphas + 1,",
          "type": "array"
        },
        {
          "description": "Indices of active variables at the end of the path. ",
          "name": "active_",
          "type": "list"
        },
        {
          "description": "The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``. ",
          "name": "coef_path_",
          "shape": "n_features, n_alphas + 1",
          "type": "array"
        },
        {
          "description": "Parameter vector (w in the formulation formula). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "The number of iterations taken by lars_path to find the grid of alphas for each target. ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "linear_model.least_angle",
      "common_name": "Lars",
      "description": "\"Least Angle Regression model a.k.a. LAR\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.Lars",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.least_angle.Lars.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n\nparameters\n----------\nX : array-like, shape (n_samples, n_features)\nTraining data.\n\ny : array-like, shape (n_samples,) or (n_samples, n_targets)\nTarget values.\n\nXy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\nXy = np.dot(X.T, y) that can be precomputed. It is useful\nonly when the Gram matrix is precomputed.\n\nreturns\n-------\nself : object\nreturns an instance of self.\n'",
          "id": "sklearn.linear_model.least_angle.Lars.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.least_angle.Lars.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.least_angle.Lars.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.least_angle.Lars.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.least_angle.Lars.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.least_angle.Lars",
      "parameters": [
        {
          "description": "Target number of non-zero coefficients. Use ``np.inf`` for no limit. ",
          "name": "n_nonzero_coefs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If True the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha. ",
          "name": "fit_path",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.pyc:492",
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Representation of weight vector(s) in kernel space  X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features] Training data, which is also required for prediction  References ---------- * Kevin P. Murphy \"Machine Learning: A Probabilistic Perspective\", The MIT Press chapter 14.4.3, pp. 492-493  See also -------- Ridge Linear ridge regression. SVR Support Vector Regression implemented using libsvm. ",
          "name": "dual_coef_",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "category": "kernel_ridge",
      "common_name": "Kernel Ridge",
      "description": "'Kernel ridge regression.\n\nKernel ridge regression (KRR) combines ridge regression (linear least\nsquares with l2-norm regularization) with the kernel trick. It thus\nlearns a linear function in the space induced by the respective kernel and\nthe data. For non-linear kernels, this corresponds to a non-linear\nfunction in the original space.\n\nThe form of the model learned by KRR is identical to support vector\nregression (SVR). However, different loss functions are used: KRR uses\nsquared error loss while support vector regression uses epsilon-insensitive\nloss, both combined with l2 regularization. In contrast to SVR, fitting a\nKRR model can be done in closed-form and is typically faster for\nmedium-sized datasets. On the other  hand, the learned model is non-sparse\nand thus slower than SVR, which learns a sparse model for epsilon > 0, at\nprediction-time.\n\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n\nRead more in the :ref:`User Guide <kernel_ridge>`.\n",
      "id": "sklearn.kernel_ridge.KernelRidge",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Kernel Ridge regression model\n",
          "id": "sklearn.kernel_ridge.KernelRidge.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Individual weights for each sample, ignored if None is passed. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "float"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.kernel_ridge.KernelRidge.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the kernel ridge model\n",
          "id": "sklearn.kernel_ridge.KernelRidge.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.kernel_ridge.KernelRidge.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.kernel_ridge.KernelRidge.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.kernel_ridge.KernelRidge",
      "parameters": [
        {
          "description": "Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates.  Alpha corresponds to ``(2*C)^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. ",
          "name": "alpha",
          "shape": "n_targets",
          "type": "float, array-like"
        },
        {
          "description": "Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. ",
          "name": "kernel",
          "type": "string"
        },
        {
          "description": "Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels. ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. ",
          "name": "degree",
          "type": "float"
        },
        {
          "description": "Additional parameters (keyword arguments) for kernel function passed as callable object. ",
          "name": "kernel_params",
          "optional": "true",
          "type": "mapping"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/kernel_ridge.pyc:16",
      "tags": [
        "kernel_ridge"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the formulation formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "type": "float"
        },
        {
          "description": "the alpha parameter chosen by the information criterion ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "number of iterations run by lars_path to find the grid of alphas. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "The value of the information criteria (\\'aic\\', \\'bic\\') across all alphas. The alpha which has the smallest information criteria is chosen. ",
          "name": "criterion_",
          "shape": "n_alphas,",
          "type": "array"
        }
      ],
      "category": "linear_model.least_angle",
      "common_name": "Lasso Lars IC",
      "description": "'Lasso model fit with Lars using BIC or AIC for model selection\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nAIC is the Akaike information criterion and BIC is the Bayes\nInformation criterion. Such criteria are useful to select the value\nof the regularization parameter by making a trade-off between the\ngoodness of fit and the complexity of the model. A good model should\nexplain well the data while being simple.\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.LassoLarsIC",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "target values.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.least_angle.LassoLarsIC.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.least_angle.LassoLarsIC",
      "parameters": [
        {
          "description": "The type of criterion to use. ",
          "name": "criterion",
          "type": ""
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``\\'auto\\'`` let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "Maximum number of iterations to perform. Can be used for early stopping. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization.  ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.pyc:1295",
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (W in the cost function formula) ",
          "name": "coef_",
          "shape": "n_tasks, n_features",
          "type": "array"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_tasks,",
          "type": "array"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Multi Task Lasso",
      "description": "\"Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDecision function of the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "The predicted decision function '",
            "name": "T",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit MultiTaskLasso model with coordinate descent\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            },
            {
              "description": "Target  Notes -----  Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary.  To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. '",
              "name": "y",
              "shape": "n_samples, n_tasks",
              "type": "ndarray"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLasso.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.MultiTaskLasso",
      "parameters": [
        {
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0 ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4 ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1746",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Weight vector(s). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.  .. versionadded:: 0.17  See also -------- RidgeClassifier, RidgeCV, :class:`sklearn.kernel_ridge.KernelRidge` ",
          "name": "n_iter_",
          "shape": "n_targets,",
          "type": "array"
        }
      ],
      "category": "linear_model.ridge",
      "common_name": "Ridge",
      "description": "\"Linear least squares with l2 regularization.\n\nThis model solves a regression model where the loss function is\nthe linear least squares function and regularization is given by\nthe l2-norm. Also known as Ridge Regression or Tikhonov regularization.\nThis estimator has built-in support for multi-variate regression\n(i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n",
      "id": "sklearn.linear_model.ridge.Ridge",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.ridge.Ridge.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit Ridge regression model\n",
          "id": "sklearn.linear_model.ridge.Ridge.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Individual weights for each sample ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "float"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.ridge.Ridge.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.ridge.Ridge.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.ridge.Ridge.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.ridge.Ridge.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.ridge.Ridge",
      "parameters": [
        {
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "alpha",
          "shape": "n_targets",
          "type": "float, array-like"
        },
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than 'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to obtain a closed-form solution.  - 'sparse_cg' uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than 'cholesky' for large-scale data (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure.  - 'sag' uses a Stochastic Average Gradient descent. It also uses an iterative procedure, and is often faster than other solvers when both n_samples and n_features are large. Note that 'sag' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  All last four solvers support both dense and sparse data. However, only 'sag' supports sparse input when `fit_intercept` is True.  .. versionadded:: 0.17 Stochastic Average Gradient descent solver. ",
          "name": "solver",
          "type": "'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag'"
        },
        {
          "description": "Precision of the solution. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. Used only in 'sag' solver.  .. versionadded:: 0.17 *random_state* to support Stochastic Average Gradient. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc:494",
      "tags": [
        "linear_model",
        "ridge"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Cross-validation values for each alpha (if `store_cv_values=True` and         `cv=None`). After `fit()` has been called, this attribute will         contain the mean squared errors (by default) or the values of the         `{loss,score}_func` function (if provided in the constructor). ",
          "name": "cv_values_",
          "optional": "true",
          "shape": "n_samples, n_targets, n_alphas",
          "type": "array"
        },
        {
          "description": "Weight vector(s). ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Estimated regularization parameter.  See also -------- Ridge: Ridge regression RidgeClassifier: Ridge classifier RidgeClassifierCV: Ridge classifier with built-in cross validation",
          "name": "alpha_",
          "type": "float"
        }
      ],
      "category": "linear_model.ridge",
      "common_name": "Ridge CV",
      "description": "\"Ridge regression with built-in cross-validation.\n\nBy default, it performs Generalized Cross-Validation, which is a form of\nefficient Leave-One-Out cross-validation.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n",
      "id": "sklearn.linear_model.ridge.RidgeCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit Ridge regression model\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weight ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "float"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.ridge.RidgeCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.ridge.RidgeCV",
      "parameters": [
        {
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. ",
          "name": "alphas",
          "shape": "n_alphas",
          "type": "numpy"
        },
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used, else, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are::  'auto' : use svd if n_samples > n_features or when X is a sparse matrix, otherwise use eigen 'svd' : force computation via singular value decomposition of X (does not work for sparse matrices) 'eigen' : force computation via eigendecomposition of X^T X  The 'auto' mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data. ",
          "name": "gcv_mode",
          "optional": "true",
          "type": "None, 'auto', 'svd', eigen'"
        },
        {
          "description": "Flag indicating if the cross-validation values corresponding to each alpha should be stored in the `cv_values_` attribute (see below). This flag is only compatible with `cv=None` (i.e. using Generalized Cross-Validation). ",
          "name": "store_cv_values",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc:1105",
      "tags": [
        "linear_model",
        "ridge"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Coordinates of cluster centers  labels_ : Labels of each point (if compute_labels is set to True). ",
          "name": "cluster_centers_",
          "type": "array"
        },
        {
          "description": "The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor.  See also --------  KMeans The classic implementation of the clustering method based on the Lloyd's algorithm. It consumes the whole set of input data at each iteration. ",
          "name": "inertia_",
          "type": "float"
        }
      ],
      "category": "cluster.k_means_",
      "common_name": "Mini Batch K Means",
      "description": "\"Mini-Batch K-Means clustering\n\nRead more in the :ref:`User Guide <mini_batch_kmeans>`.\n",
      "id": "sklearn.cluster.k_means_.MiniBatchKMeans",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the centroids on X by chunking it into mini-batches.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training instances to cluster. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n'",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.fit_predict",
          "name": "fit_predict",
          "parameters": []
        },
        {
          "description": "'Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n'",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.fit_transform",
          "name": "fit_transform",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Update k means estimate on a single mini-batch X.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Coordinates of the data points to cluster. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "New data to predict. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Index of the cluster each sample belongs to. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Opposite of the value of X on the K-means objective.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.score",
          "name": "score",
          "parameters": [
            {
              "description": "New data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Opposite of the value of X on the K-means objective. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters.  Note that even if X is sparse, the array returned by\n`transform` will typically be dense.\n",
          "id": "sklearn.cluster.k_means_.MiniBatchKMeans.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data to transform. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "X transformed in the new space. '",
            "name": "X_new",
            "shape": "n_samples, k",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.cluster.k_means_.MiniBatchKMeans",
      "parameters": [
        {
          "description": "The number of clusters to form as well as the number of centroids to generate. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia.  To disable convergence detection based on inertia, set max_no_improvement to None. ",
          "name": "max_no_improvement",
          "type": "int"
        },
        {
          "description": "Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic.  To disable convergence detection based on normalized center change, set tol to 0.0 (default). ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Size of the mini batches. ",
          "name": "batch_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n_clusters. ",
          "name": "init_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Method for initialization, defaults to 'k-means++':  'k-means++' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.  'random': choose k observations (rows) at random from data for the initial centroids.  If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. ",
          "name": "init",
          "type": "'k-means++', 'random' or an ndarray"
        },
        {
          "description": "Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the ``n_init`` initializations as measured by inertia. ",
          "name": "n_init",
          "type": "int"
        },
        {
          "description": "Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit. ",
          "name": "compute_labels",
          "type": "boolean"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering. ",
          "name": "reassignment_ratio",
          "type": "float"
        },
        {
          "description": "Verbosity mode. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc:1183",
      "tags": [
        "cluster",
        "k_means_"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.spatial.distance.squareform",
      "description": "\"\nConverts a vector-form distance vector to a square-form distance\nmatrix, and vice-versa.\n",
      "id": "scipy.spatial.distance.squareform",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.spatial.distance.squareform",
      "parameters": [
        {
          "description": "Either a condensed or redundant distance matrix.",
          "name": "X",
          "type": "ndarray"
        },
        {
          "description": "As with MATLAB(TM), if force is equal to 'tovector' or 'tomatrix', the input will be treated as a distance matrix or distance vector respectively.",
          "name": "force",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "If `checks` is set to False, no checks will be made for matrix symmetry nor zero diagonals. This is useful if it is known that ``X - X.T1`` is small and ``diag(X)`` is close to zero. These values are ignored any way so they do not disrupt the squareform transformation. ",
          "name": "checks",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "If a condensed distance matrix is passed, a redundant one is returned, or if a redundant one is passed, a condensed distance matrix is returned.  Notes -----  1. v = squareform(X)  Given a square d-by-d symmetric distance matrix X, ``v=squareform(X)`` returns a ``d * (d-1) / 2`` (or `${n \\\\choose 2}$`) sized vector v.  v[{n \\\\choose 2}-{n-i \\\\choose 2} + (j-i-1)] is the distance between points i and j. If X is non-square or asymmetric, an error is returned.  2. X = squareform(v)  Given a d*d(-1)/2 sized v for some integer d>=2 encoding distances as described, X=squareform(v) returns a d by d distance matrix X. The X[i, j] and X[j, i] values are set to v[{n \\\\choose 2}-{n-i \\\\choose 2} + (j-u-1)] and all diagonal elements are zero.  \"",
        "name": "Y",
        "type": "ndarray"
      },
      "tags": [
        "spatial",
        "distance"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.cohen_kappa_score",
      "description": "'Cohen\\'s kappa: a statistic that measures inter-annotator agreement.\n\nThis function computes Cohen\\'s kappa [1]_, a score that expresses the level\nof agreement between two annotators on a classification problem. It is\ndefined as\n\n.. math::\n\\\\kappa = (p_o - p_e) / (1 - p_e)\n\nwhere :math:`p_o` is the empirical probability of agreement on the label\nassigned to any sample (the observed agreement ratio), and :math:`p_e` is\nthe expected agreement when both annotators assign labels randomly.\n:math:`p_e` is estimated using a per-annotator empirical prior over the\nclass labels [2]_.\n\nRead more in the :ref:`User Guide <cohen_kappa>`.\n",
      "id": "sklearn.metrics.classification.cohen_kappa_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.cohen_kappa_score",
      "parameters": [
        {
          "description": "Labels assigned by the first annotator.  y2 : array, shape = [n_samples] Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping ``y1`` and ``y2`` doesn\\'t change the value. ",
          "name": "y1",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in ``y1`` or ``y2`` are used. ",
          "name": "labels",
          "optional": "true",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "List of weighting type to calculate the score. None means no weighted; \"linear\" means linear weighted; \"quadratic\" means quadratic weighted. ",
          "name": "weights",
          "optional": "true",
          "type": "str"
        }
      ],
      "returns": {
        "description": "The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement.  References ---------- .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\". Educational and Psychological Measurement 20(1):37-46. doi:10.1177/001316446002000104. .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for computational linguistics\". Computational Linguistics 34(4):555-596. <http://www.mitpressjournals.org/doi/abs/10.1162/coli.07-034-R2#.V0J1MJMrIWo>`_ .. [3] `Wikipedia entry for the Cohen\\'s kappa. <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_ '",
        "name": "kappa",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.fowlkes_mallows_score",
      "description": "'Measure the similarity of two clusterings of a set of points.\n\nThe Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\nthe precision and recall::\n\nFMI = TP / sqrt((TP + FP) * (TP + FN))\n\nWhere ``TP`` is the number of **True Positive** (i.e. the number of pair of\npoints that belongs in the same clusters in both ``labels_true`` and\n``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\nnumber of pair of points that belongs in the same clusters in\n``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n**False Negative** (i.e the number of pair of points that belongs in the\nsame clusters in ``labels_pred`` and not in ``labels_True``).\n\nThe score ranges from 0 to 1. A high value indicates a good similarity\nbetween two clusters.\n\nRead more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "id": "sklearn.metrics.cluster.supervised.fowlkes_mallows_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.fowlkes_mallows_score",
      "parameters": [
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_true",
          "shape": "``n_samples``,",
          "type": "int"
        },
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_pred",
          "shape": "``n_samples``, ",
          "type": "array"
        }
      ],
      "returns": {
        "description": "The resulting Fowlkes-Mallows score.  Examples --------  Perfect labelings are both homogeneous and complete, hence have score 1.0::  >>> from sklearn.metrics.cluster import fowlkes_mallows_score >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1]) 1.0 >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  If classes members are completely split across different clusters, the assignment is totally random, hence the FMI is null::  >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3]) 0.0  References ---------- .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two hierarchical clusterings\". Journal of the American Statistical Association <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_  .. [2] `Wikipedia entry for the Fowlkes-Mallows Index <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_ '",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.ranking.precision_recall_curve",
      "description": "'Compute precision-recall pairs for different probability thresholds\n\nNote: this implementation is restricted to the binary classification task.\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe last precision and recall values are 1. and 0. respectively and do not\nhave a corresponding threshold.  This ensures that the graph starts on the\nx axis.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.ranking.precision_recall_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.ranking.precision_recall_curve",
      "parameters": [
        {
          "description": "True targets of binary classification in range {-1, 1} or {0, 1}. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "Estimated probabilities or decision function. ",
          "name": "probas_pred",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "The label of the positive class ",
          "name": "pos_label",
          "type": "int"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Precision values such that element i is the precision of predictions with score >= thresholds[i] and the last element is 1.  recall : array, shape = [n_thresholds + 1] Decreasing recall values such that element i is the recall of predictions with score >= thresholds[i] and the last element is 0.  thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))] Increasing thresholds on the decision function used to compute precision and recall.  Examples -------- >>> import numpy as np >>> from sklearn.metrics import precision_recall_curve >>> y_true = np.array([0, 0, 1, 1]) >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8]) >>> precision, recall, thresholds = precision_recall_curve( ...     y_true, y_scores) >>> precision  # doctest: +ELLIPSIS array([ 0.66...,  0.5       ,  1.        ,  1.        ]) >>> recall array([ 1. ,  0.5,  0.5,  0. ]) >>> thresholds array([ 0.35,  0.4 ,  0.8 ])  '",
        "name": "precision",
        "shape": "n_thresholds + 1",
        "type": "array"
      },
      "tags": [
        "metrics",
        "ranking"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.regression.r2_score",
      "description": "\"R^2 (coefficient of determination) regression score function.\n\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n\nRead more in the :ref:`User Guide <r2_score>`.\n",
      "id": "sklearn.metrics.regression.r2_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.regression.r2_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Estimated target values. ",
          "name": "y_pred",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": " Defines aggregating of multiple output scores. Array-like value defines weights used to average scores. Default value corresponds to 'variance_weighted', this behaviour is deprecated since version 0.17 and will be changed to 'uniform_average' starting from 0.19.  'raw_values' : Returns a full set of scores in case of multioutput input.  'uniform_average' : Scores of all outputs are averaged with uniform weight.  'variance_weighted' : Scores of all outputs are averaged, weighted by the variances of each individual output. ",
          "name": "multioutput",
          "shape": "n_outputs",
          "type": "string"
        }
      ],
      "returns": {
        "description": "The R^2 score or ndarray of scores if 'multioutput' is 'raw_values'.  Notes ----- This is not a symmetric function.  Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).  References ---------- .. [1] `Wikipedia entry on the Coefficient of determination <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_  Examples -------- >>> from sklearn.metrics import r2_score >>> y_true = [3, -0.5, 2, 7] >>> y_pred = [2.5, 0.0, 2, 8] >>> r2_score(y_true, y_pred)  # doctest: +ELLIPSIS 0.948... >>> y_true = [[0.5, 1], [-1, 1], [7, -6]] >>> y_pred = [[0, 2], [-1, 2], [8, -5]] >>> r2_score(y_true, y_pred, multioutput='variance_weighted')  # doctest: +ELLIPSIS 0.938... >>> y_true = [1,2,3] >>> y_pred = [1,2,3] >>> r2_score(y_true, y_pred) 1.0 >>> y_true = [1,2,3] >>> y_pred = [2,2,2] >>> r2_score(y_true, y_pred) 0.0 >>> y_true = [1,2,3] >>> y_pred = [3,2,1] >>> r2_score(y_true, y_pred) -3.0 \"",
        "name": "z",
        "type": "float"
      },
      "tags": [
        "metrics",
        "regression"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.unsupervised.silhouette_samples",
      "description": "'Compute the Silhouette Coefficient for each sample.\n\nThe Silhouette Coefficient is a measure of how well samples are clustered\nwith samples that are similar to themselves. Clustering models with a high\nSilhouette Coefficient are said to be dense, where samples in the same\ncluster are similar to each other, and well separated, where samples in\ndifferent clusters are not very similar to each other.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.\nNote that Silhouette Coefficent is only defined if number of labels\nis 2 <= n_labels <= n_samples - 1.\n\nThis function returns the Silhouette Coefficient for each sample.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "id": "sklearn.metrics.cluster.unsupervised.silhouette_samples",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.unsupervised.silhouette_samples",
      "parameters": [
        {
          "description": "Array of pairwise distances between samples, or a feature array. ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "label values for each sample ",
          "name": "labels",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`. If X is the distance array itself, use \"precomputed\" as the metric.  `**kwds` : optional keyword parameters Any further parameters are passed directly to the distance function. If using a ``scipy.spatial.distance`` metric, the parameters are still metric dependent. See the scipy docs for usage examples. ",
          "name": "metric",
          "type": "string"
        }
      ],
      "returns": {
        "description": "Silhouette Coefficient for each samples.  References ----------  .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis\". Computational and Applied Mathematics 20: 53-65. <http://www.sciencedirect.com/science/article/pii/0377042787901257>`_  .. [2] `Wikipedia entry on the Silhouette Coefficient <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_  '",
        "name": "silhouette",
        "shape": "n_samples",
        "type": "array"
      },
      "tags": [
        "metrics",
        "cluster",
        "unsupervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_svd.svd",
      "description": "'\nSingular Value Decomposition.\n\nFactorizes the matrix a into two unitary matrices U and Vh, and\na 1-D array s of singular values (real, non-negative) such that\n``a == U*S*Vh``, where S is a suitably shaped matrix of zeros with\nmain diagonal s.\n",
      "id": "scipy.linalg.decomp_svd.svd",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_svd.svd",
      "parameters": [
        {
          "description": "Matrix to decompose.",
          "name": "a",
          "type": ""
        },
        {
          "description": "If True, `U` and `Vh` are of shape ``(M,M)``, ``(N,N)``. If False, the shapes are ``(M,K)`` and ``(K,N)``, where ``K = min(M,N)``.",
          "name": "full_matrices",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to compute also `U` and `Vh` in addition to `s`. Default is True.",
          "name": "compute_uv",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to overwrite `a`; may improve performance. Default is False.",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrix contains only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Unitary matrix having left singular vectors as columns. Of shape ``(M,M)`` or ``(M,K)``, depending on `full_matrices`. s : ndarray The singular values, sorted in non-increasing order. Of shape (K,), with ``K = min(M, N)``. Vh : ndarray Unitary matrix having right singular vectors as rows. Of shape ``(N,N)`` or ``(K,N)`` depending on `full_matrices`.  For ``compute_uv = False``, only `s` is returned.  Raises ------ LinAlgError If SVD computation does not converge.  See also -------- svdvals : Compute singular values of a matrix. diagsvd : Construct the Sigma matrix, given the vector s.  Examples -------- >>> from scipy import linalg >>> a = np.random.randn(9, 6) + 1.j*np.random.randn(9, 6) >>> U, s, Vh = linalg.svd(a) >>> U.shape, Vh.shape, s.shape ((9, 9), (6, 6), (6,))  >>> U, s, Vh = linalg.svd(a, full_matrices=False) >>> U.shape, Vh.shape, s.shape ((9, 6), (6, 6), (6,)) >>> S = linalg.diagsvd(s, 6, 6) >>> np.allclose(a, np.dot(U, np.dot(S, Vh))) True  >>> s2 = linalg.svd(a, compute_uv=False) >>> np.allclose(s, s2) True  '",
        "name": "U",
        "type": "ndarray"
      },
      "tags": [
        "linalg",
        "decomp_svd"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The raw robust estimated location before correction and re-weighting. ",
          "name": "raw_location_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "The raw robust estimated covariance before correction and re-weighting. ",
          "name": "raw_covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "A mask of the observations that have been used to compute the raw robust estimates of location and shape, before correction and re-weighting. ",
          "name": "raw_support_",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Estimated robust location ",
          "name": "location_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "Estimated robust covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. (stored only if store_precision is True) ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "A mask of the observations that have been used to compute the robust estimates of location and shape. ",
          "name": "support_",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Mahalanobis distances of the training set (on which `fit` is called) observations. ",
          "name": "dist_",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "category": "covariance.robust_covariance",
      "common_name": "Min Cov Det",
      "description": "'Minimum Covariance Determinant (MCD): robust estimator of covariance.\n\nThe Minimum Covariance Determinant covariance estimator is to be applied\non Gaussian-distributed data, but could still be relevant on data\ndrawn from a unimodal, symmetric distribution. It is not meant to be used\nwith multi-modal data (the algorithm used to fit a MinCovDet object is\nlikely to fail in such a case).\nOne should consider projection pursuit methods to deal with multi-modal\ndatasets.\n\nRead more in the :ref:`User Guide <robust_covariance>`.\n",
      "id": "sklearn.covariance.robust_covariance.MinCovDet",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [Rouseeuw1984]_.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.correct_covariance",
          "name": "correct_covariance",
          "parameters": [
            {
              "description": "The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates. ",
              "name": "data",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Corrected robust covariance estimate.  '",
            "name": "covariance_corrected",
            "shape": "n_features, n_features",
            "type": "array-like"
          }
        },
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "'Fits a Minimum Covariance Determinant with the FastMCD algorithm.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "\"Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates). [Rouseeuw1984]_\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.reweight_covariance",
          "name": "reweight_covariance",
          "parameters": [
            {
              "description": "The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates. ",
              "name": "data",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Re-weighted robust location estimate.  covariance_reweighted : array-like, shape (n_features, n_features) Re-weighted robust covariance estimate.  support_reweighted : array-like, type boolean, shape (n_samples,) A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.  \"",
            "name": "location_reweighted",
            "shape": "n_features, ",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.robust_covariance.MinCovDet.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.robust_covariance.MinCovDet",
      "parameters": [
        {
          "description": "Specify if the estimated precision is stored. ",
          "name": "store_precision",
          "type": "bool"
        },
        {
          "description": "If True, the support of the robust location and the covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment. ",
          "name": "assume_centered",
          "type": ""
        },
        {
          "description": "The proportion of points to be included in the support of the raw MCD estimate. Default is None, which implies that the minimum value of support_fraction will be used within the algorithm: [n_sample + n_features + 1] / 2 ",
          "name": "support_fraction",
          "type": "float"
        },
        {
          "description": "The random generator used. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/robust_covariance.pyc:501",
      "tags": [
        "covariance",
        "robust_covariance"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated covariance matrix. ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "numpy"
        },
        {
          "description": "Estimated precision matrix (inverse covariance). ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "numpy"
        },
        {
          "description": "Penalization parameter selected. ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "All penalization parameters explored.  `grid_scores`: 2D numpy.ndarray (n_alphas, n_folds) Log-likelihood score on left-out data across folds. ",
          "name": "cv_alphas_",
          "type": "list"
        },
        {
          "description": "Number of iterations run for the optimal alpha.  See Also -------- graph_lasso, GraphLasso ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "covariance.graph_lasso_",
      "common_name": "Graph Lasso CV",
      "description": "\"Sparse inverse covariance w/ cross-validated choice of the l1 penalty\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n",
      "id": "sklearn.covariance.graph_lasso_.GraphLassoCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "'Fits the GraphLasso covariance model to X.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data from which to compute the covariance estimate '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "'Computes the log-likelihood of a Gaussian data set with\n`self.covariance_` as an estimator of its covariance matrix.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering). ",
              "name": "X_test",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "",
              "name": "y",
              "type": "not"
            }
          ],
          "returns": {
            "description": "The likelihood of the data set with `self.covariance_` as an estimator of its covariance matrix.  '",
            "name": "res",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.graph_lasso_.GraphLassoCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.graph_lasso_.GraphLassoCV",
      "parameters": [
        {
          "description": "If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details.  n_refinements: strictly positive integer The number of times the grid is refined. Not used if explicit values of alphas are passed. ",
          "name": "alphas",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. ",
          "name": "tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. ",
          "name": "enet_tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "Maximum number of iterations.  mode: {'cd', 'lars'} The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "number of jobs to run in parallel (default 1). ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If verbose is True, the objective function and duality gap are printed at each iteration. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation. ",
          "name": "assume_centered",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/graph_lasso_.pyc:451",
      "tags": [
        "covariance",
        "graph_lasso_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_tasks,",
          "type": "array"
        },
        {
          "description": "Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), ``coef_`` is then a 1D array ",
          "name": "coef_",
          "shape": "n_tasks, n_features",
          "type": "array"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Multi Task Elastic Net",
      "description": "\"Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer\n\nThe optimization objective for MultiTaskElasticNet is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n+ alpha * l1_ratio * ||W||_21\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_elastic_net>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDecision function of the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "The predicted decision function '",
            "name": "T",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit MultiTaskLasso model with coordinate descent\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            },
            {
              "description": "Target  Notes -----  Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary.  To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. '",
              "name": "y",
              "shape": "n_samples, n_tasks",
              "type": "ndarray"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNet",
      "parameters": [
        {
          "description": "Constant that multiplies the L1/L2 term. Defaults to 1.0  l1_ratio : float The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "When set to ``True``, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1552",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function.  See also --------  SGDClassifier Perceptron ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "linear_model.passive_aggressive",
      "common_name": "Passive Aggressive Classifier",
      "description": "'Passive Aggressive Classifier\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Passive Aggressive algorithm.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_classes,n_features",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "n_classes",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Fit linear model with Passive Aggressive algorithm.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of the training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of the target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained by via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to contain all labels in `classes`. ",
              "name": "classes",
              "shape": "n_classes",
              "type": "array"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.passive_aggressive.PassiveAggressiveClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The number of passes over the training data (aka epochs). Defaults to 5. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether or not the training data should be shuffled after each epoch. ",
          "name": "shuffle",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The verbosity level ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means \\'all CPUs\\'. Defaults to 1. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper. ",
          "name": "loss",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  .. versionadded:: 0.17 parameter *class_weight* to automatically weight samples. ",
          "name": "class_weight",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/passive_aggressive.pyc:9",
      "tags": [
        "linear_model",
        "passive_aggressive"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_tasks,",
          "type": "array"
        },
        {
          "description": "Parameter vector (W in the cost function formula). ",
          "name": "coef_",
          "shape": "n_tasks, n_features",
          "type": "array"
        },
        {
          "description": "The amount of penalization chosen by cross validation ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "mean square error for the test set on each fold, varying alpha ",
          "name": "mse_path_",
          "shape": "n_alphas, n_folds",
          "type": "array"
        },
        {
          "description": "The grid of alphas used for fitting. ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "numpy"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha.  See also -------- MultiTaskElasticNet ElasticNetCV MultiTaskElasticNetCV ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Multi Task Lasso CV",
      "description": "\"Multi-task L1/L2 Lasso with built-in cross-validation.\n\nThe optimization objective for MultiTaskLasso is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model with coordinate descent\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Pass directly as float64, Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.MultiTaskLassoCV",
      "parameters": [
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "List of alphas where to compute the models. If not provided, set automatically. ",
          "name": "alphas",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Number of alphas along the regularization path ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The maximum number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs. Note that this is used only if multiple values for l1_ratio are given. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:2040",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "ensemble.gradient_boosting",
      "common_name": "Base Gradient Boosting",
      "description": "'Abstract base class for Gradient Boosting. '",
      "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nCompute the decision function of ``X``.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification produce an array of shape [n_samples]. '",
            "name": "score",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Fit the gradient boosting model.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers in classification, real numbers in regression) For classification, labels must correspond to classes. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of ``_fit_stages`` as keyword arguments ``callable(i, self, locals())``. If the callable returns ``True`` the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting. ",
              "name": "monitor",
              "optional": "true",
              "type": "callable"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nCompute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.staged_decision_function",
          "name": "staged_decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification are special cases with ``k == 1``, otherwise ``k==n_classes``. '",
            "name": "score",
            "shape": "n_samples, k",
            "type": "generator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.BaseGradientBoosting",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:718",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Cross-validation values for each alpha (if `store_cv_values=True` and `cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor). ",
          "name": "cv_values_",
          "optional": "true",
          "shape": "n_samples, n_responses, n_alphas",
          "type": "array"
        },
        {
          "description": "Weight vector(s). ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Estimated regularization parameter  See also -------- Ridge: Ridge regression RidgeClassifier: Ridge classifier RidgeCV: Ridge regression with built-in cross validation ",
          "name": "alpha_",
          "type": "float"
        }
      ],
      "category": "linear_model.ridge",
      "common_name": "Ridge Classifier CV",
      "description": "'Ridge classifier with built-in cross-validation.\n\nBy default, it performs Generalized Cross-Validation, which is a form of\nefficient Leave-One-Out cross-validation. Currently, only the n_features >\nn_samples case is handled efficiently.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n",
      "id": "sklearn.linear_model.ridge.RidgeClassifierCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Fit the ridge classifier.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Sample weight. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "float"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifierCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.ridge.RidgeClassifierCV",
      "parameters": [
        {
          "description": "Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. ",
          "name": "alphas",
          "shape": "n_alphas",
          "type": "numpy"
        },
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the efficient Leave-One-Out cross-validation - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc:1207",
      "tags": [
        "linear_model",
        "ridge"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function.  See also --------  SGDRegressor ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "linear_model.passive_aggressive",
      "common_name": "Passive Aggressive Regressor",
      "description": "'Passive Aggressive Regressor\n\nRead more in the :ref:`User Guide <passive_aggressive>`.\n",
      "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nPredict using the linear model\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Passive Aggressive algorithm.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_features",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "1",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Fit linear model with Passive Aggressive algorithm.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        }
      ],
      "name": "sklearn.linear_model.passive_aggressive.PassiveAggressiveRegressor",
      "parameters": [
        {
          "description": "If the difference between the current prediction and the correct label is below this threshold, the model is not updated. ",
          "name": "epsilon",
          "type": "float"
        },
        {
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The number of passes over the training data (aka epochs). Defaults to 5. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether or not the training data should be shuffled after each epoch. ",
          "name": "shuffle",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The verbosity level ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper. ",
          "name": "loss",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/passive_aggressive.pyc:170",
      "tags": [
        "linear_model",
        "passive_aggressive"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Feature scores between 0 and 1. ",
          "name": "scores_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests ``scores_`` is the max         of ``all_scores_``. ",
          "name": "all_scores_",
          "shape": "n_features, n_reg_parameter",
          "type": "array"
        }
      ],
      "category": "linear_model.randomized_l1",
      "common_name": "Randomized Logistic Regression",
      "description": "\"Randomized Logistic Regression\n\nRandomized Logistic Regression works by subsampling the training\ndata and fitting a L1-penalized LogisticRegression model where the\npenalty of a random subset of coefficients has been scaled. By\nperforming this double randomization several times, the method\nassigns high scores to features that are repeatedly selected across\nrandomizations. This is known as stability selection. In short,\nfeatures selected more often are considered good features.\n\nRead more in the :ref:`User Guide <randomized_l1>`.\n",
      "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Return a mask, or list, of the features/indices selected.'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.get_support",
          "name": "get_support",
          "parameters": []
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.inverse_transform",
          "name": "inverse_transform",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform a new matrix using the selected features'",
          "id": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression.transform",
          "name": "transform",
          "parameters": []
        }
      ],
      "name": "sklearn.linear_model.randomized_l1.RandomizedLogisticRegression",
      "parameters": [
        {
          "description": "The regularization parameter C in the LogisticRegression. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The s parameter used to randomly scale the penalty of different features (See :ref:`User Guide <randomized_l1>` for details ). Should be between 0 and 1. ",
          "name": "scaling",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. ",
          "name": "sample_fraction",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of randomized models. ",
          "name": "n_resampling",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The score above which features should be selected. ",
          "name": "selection_threshold",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "tolerance for stopping criteria of LogisticRegression ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of CPUs to use during the resampling. If '-1', use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in '2*n_jobs' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory. ",
          "name": "memory",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/randomized_l1.pyc:383",
      "tags": [
        "linear_model",
        "randomized_l1"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Coefficients of the regression model (median of distribution). ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Estimated intercept of regression model. ",
          "name": "intercept_",
          "type": "float"
        },
        {
          "description": "Approximated breakdown point. ",
          "name": "breakdown_",
          "type": "float"
        },
        {
          "description": "Number of iterations needed for the spatial median. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "Number of combinations taken into account from \\'n choose k\\', where n is the number of samples and k is the number of subsamples. ",
          "name": "n_subpopulation_",
          "type": "int"
        }
      ],
      "category": "linear_model.theil_sen",
      "common_name": "Theil Sen Regressor",
      "description": "'Theil-Sen Estimator: robust multivariate regression model.\n\nThe algorithm calculates least square solutions on subsets with size\nn_subsamples of the samples in X. Any value of n_subsamples between the\nnumber of features and samples leads to an estimator with a compromise\nbetween robustness and efficiency. Since the number of least square\nsolutions is \"n_samples choose n_subsamples\", it can be extremely large\nand can therefore be limited with max_subpopulation. If this limit is\nreached, the subsets are chosen randomly. In a final step, the spatial\nmedian (or L1 median) is calculated of all least square solutions.\n\nRead more in the :ref:`User Guide <theil_sen_regression>`.\n",
      "id": "sklearn.linear_model.theil_sen.TheilSenRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model.\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.theil_sen.TheilSenRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.theil_sen.TheilSenRegressor",
      "parameters": [
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Instead of computing with a set of cardinality \\'n choose k\\', where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if \\'n choose k\\' is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed. ",
          "name": "max_subpopulation",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares. ",
          "name": "n_subsamples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations for the calculation of spatial median. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Tolerance when calculating spatial median. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "A random number generator instance to define the state of the random permutations generator. ",
          "name": "random_state",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Verbose mode when fitting the model. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/theil_sen.pyc:199",
      "tags": [
        "linear_model",
        "theil_sen"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is a readonly property derived from `raw_coef_` that follows the internal memory layout of liblinear. ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function.   See also -------- LinearSVC Implementation of Support Vector Machine classifier using the same library as this class (liblinear).  SVR Implementation of Support Vector Machine regression using libsvm: the kernel can be non-linear but its SMO algorithm does not scale to large number of samples as LinearSVC does.  sklearn.linear_model.SGDRegressor SGDRegressor can optimize the same cost function as LinearSVR by adjusting the penalty and loss parameters. In addition it requires less memory, allows incremental (online) learning, and implements various loss functions and regularization regimes.",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "Linear SVR",
      "description": "'Linear Support Vector Regression.\n\nSimilar to SVR with parameter kernel=\\'linear\\', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThis class supports both dense and sparse input.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n",
      "id": "sklearn.svm.classes.LinearSVR",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.svm.classes.LinearSVR.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model according to the given training data.\n",
          "id": "sklearn.svm.classes.LinearSVR.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target vector relative to X ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.LinearSVR.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.svm.classes.LinearSVR.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.svm.classes.LinearSVR.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.LinearSVR.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.LinearSVR",
      "parameters": [
        {
          "default": "1.0",
          "description": "Penalty parameter C of the error term. The penalty is a squared l2 penalty. The bigger this parameter, the less regularization is used. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specifies the loss function. \\'l1\\' is the epsilon-insensitive loss (standard SVR) while \\'l2\\' is the squared epsilon-insensitive loss. ",
          "name": "loss",
          "type": "string"
        },
        {
          "default": "0.1",
          "description": "Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set ``epsilon=0``. ",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. ",
          "name": "dual",
          "type": "bool"
        },
        {
          "default": "1e-4",
          "description": "Tolerance for stopping criteria. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1",
          "description": "When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. ",
          "name": "intercept_scaling",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The maximum number of iterations to be run. ",
          "name": "max_iter",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:226",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Indices of support vectors. ",
          "name": "support_",
          "shape": "n_SV",
          "type": "array-like"
        },
        {
          "description": "Support vectors. ",
          "name": "support_vectors_",
          "shape": "n_SV, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of support vectors for each class. ",
          "name": "n_support_",
          "shape": "n_class",
          "type": "array-like"
        },
        {
          "description": "Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the section about multi-class classification in the SVM section of the User Guide for details. ",
          "name": "dual_coef_",
          "shape": "n_class-1, n_SV",
          "type": "array"
        },
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`. ",
          "name": "coef_",
          "shape": "n_class-1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "n_class * (n_class-1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "Nu SVC",
      "description": "'Nu-Support Vector Classification.\n\nSimilar to SVC but uses a parameter to control the number of support\nvectors.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n",
      "id": "sklearn.svm.classes.NuSVC",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Distance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.classes.NuSVC.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. If decision_function_shape='ovr', the shape is (n_samples, n_classes) \"",
            "name": "X",
            "shape": "n_samples, n_classes * (n_classes-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.classes.NuSVC.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.NuSVC.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.classes.NuSVC.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train] ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Class labels for samples in X. '",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.svm.classes.NuSVC.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.NuSVC.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.NuSVC",
      "parameters": [
        {
          "default": "0.5",
          "description": "An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. ",
          "name": "nu",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "\\'rbf\\'",
          "description": "Specifies the kernel type to be used in the algorithm. It must be one of \\'linear\\', \\'poly\\', \\'rbf\\', \\'sigmoid\\', \\'precomputed\\' or a callable. If none is given, \\'rbf\\' will be used. If a callable is given it is used to precompute the kernel matrix. ",
          "name": "kernel",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "3",
          "description": "Degree of the polynomial kernel function (\\'poly\\'). Ignored by all other kernels. ",
          "name": "degree",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Kernel coefficient for \\'rbf\\', \\'poly\\' and \\'sigmoid\\'. If gamma is \\'auto\\' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in \\'poly\\' and \\'sigmoid\\'. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether to enable probability estimates. This must be enabled prior to calling `fit`, and will slow down that method. ",
          "name": "probability",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "True",
          "description": "Whether to use the shrinking heuristic. ",
          "name": "shrinking",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1e-3",
          "description": "Tolerance for stopping criterion. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specify the size of the kernel cache (in MB). ",
          "name": "cache_size",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict, \\'balanced\\'"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "default": "-1",
          "description": "Hard limit on iterations within solver, or -1 for no limit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to return a one-vs-rest (\\'ovr\\') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (\\'ovo\\') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). The default of None will currently behave as \\'ovo\\' for backward compatibility and raise a deprecation warning, but will change \\'ovr\\' in 0.19.  .. versionadded:: 0.17 *decision_function_shape=\\'ovr\\'* is recommended.  .. versionchanged:: 0.17 Deprecated *decision_function_shape=\\'ovo\\' and None*. ",
          "name": "decision_function_shape",
          "type": ""
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data for probability estimation. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:546",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Results of the clustering. `rows[i, r]` is True if cluster `i` contains row `r`. Available only after calling ``fit``. ",
          "name": "rows_",
          "shape": "n_row_clusters, n_rows",
          "type": "array-like"
        },
        {
          "description": "Results of the clustering, like `rows`. ",
          "name": "columns_",
          "shape": "n_column_clusters, n_columns",
          "type": "array-like"
        },
        {
          "description": "Row partition labels. ",
          "name": "row_labels_",
          "shape": "n_rows,",
          "type": "array-like"
        },
        {
          "description": "Column partition labels. ",
          "name": "column_labels_",
          "shape": "n_cols,",
          "type": "array-like"
        }
      ],
      "category": "cluster.bicluster",
      "common_name": "Spectral Biclustering",
      "description": "\"Spectral biclustering (Kluger, 2003).\n\nPartitions rows and columns under the assumption that the data has\nan underlying checkerboard structure. For instance, if there are\ntwo row partitions and three column partitions, each row will\nbelong to three biclusters, and each column will belong to two\nbiclusters. The outer product of the corresponding row and column\nlabel vectors gives this checkerboard structure.\n\nRead more in the :ref:`User Guide <spectral_biclustering>`.\n",
      "id": "sklearn.cluster.bicluster.SpectralBiclustering",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Creates a biclustering for X.\n",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.fit",
          "name": "fit",
          "parameters": [
            {
              "description": " '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "\"Row and column indices of the i'th bicluster.\n\nOnly works if ``rows_`` and ``columns_`` attributes exist.\n",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.get_indices",
          "name": "get_indices",
          "parameters": [],
          "returns": {
            "description": "Indices of rows in the dataset that belong to the bicluster. col_ind : np.array, dtype=np.intp Indices of columns in the dataset that belong to the bicluster.  \"",
            "name": "row_ind",
            "type": "np"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Shape of the i'th bicluster.\n",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.get_shape",
          "name": "get_shape",
          "parameters": [],
          "returns": {
            "description": "Number of rows and columns (resp.) in the bicluster. \"",
            "name": "shape",
            "type": ""
          }
        },
        {
          "description": "'Returns the submatrix corresponding to bicluster `i`.\n\nWorks with sparse matrices. Only works if ``rows_`` and\n``columns_`` attributes exist.\n\n'",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.get_submatrix",
          "name": "get_submatrix",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.bicluster.SpectralBiclustering.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.bicluster.SpectralBiclustering",
      "parameters": [
        {
          "description": "The number of row and column clusters in the checkerboard structure. ",
          "name": "n_clusters",
          "type": "integer"
        },
        {
          "description": "Method of normalizing and converting singular vectors into biclusters. May be one of 'scale', 'bistochastic', or 'log'. The authors recommend using 'log'. If the data is sparse, however, log normalization will not work, which is why the default is 'bistochastic'. CAUTION: if `method='log'`, the data must not be sparse. ",
          "name": "method",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of singular vectors to check. ",
          "name": "n_components",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of best singular vectors to which to project the data for clustering. ",
          "name": "n_best",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Selects the algorithm for finding singular vectors. May be 'randomized' or 'arpack'. If 'randomized', uses `sklearn.utils.extmath.randomized_svd`, which may be faster for large matrices. If 'arpack', uses `sklearn.utils.arpack.svds`, which is more accurate, but possibly slower in some cases. ",
          "name": "svd_method",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of vectors to use in calculating the SVD. Corresponds to `ncv` when `svd_method=arpack` and `n_oversamples` when `svd_method` is 'randomized`. ",
          "name": "n_svd_vecs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to use mini-batch k-means, which is faster but may get different results. ",
          "name": "mini_batch",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Method for initialization of k-means algorithm; defaults to 'k-means++'. ",
          "name": "init",
          "type": "'k-means++', 'random' or an ndarray"
        },
        {
          "description": "Number of random initializations that are tried with the k-means algorithm.  If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A pseudo random number generator used by the K-Means initialization. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/bicluster.pyc:297",
      "tags": [
        "cluster",
        "bicluster"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "neighbors.regression",
      "common_name": "K Neighbors Regressor",
      "description": "\"Regression based on k-nearest neighbors.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n",
      "id": "sklearn.neighbors.regression.KNeighborsRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values, array of float values, shape = [n_samples] or [n_samples, n_outputs] \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Finds the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.kneighbors",
          "name": "kneighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors to get (default is the value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the lengths to points, only present if return_distance=True  ind : array Indices of the nearest points in the population matrix.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1,1,1]  >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=1) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS (array([[ 0.5]]), array([[2]]...))  As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:  >>> X = [[0., 1., 0.], [1., 0., 1.]] >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS array([[1], [2]]...)  \"",
            "name": "dist",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.kneighbors_graph",
          "name": "kneighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors for each sample. (default is value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=2) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.kneighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- NearestNeighbors.radius_neighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples_fit",
            "type": "sparse"
          }
        },
        {
          "description": "\"Predict the target for the provided data\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Target values \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.regression.KNeighborsRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.regression.KNeighborsRegressor",
      "parameters": [
        {
          "description": "Number of neighbors to use by default for :meth:`k_neighbors` queries. ",
          "name": "n_neighbors",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  Uniform weights are used by default. ",
          "name": "weights",
          "type": "str"
        },
        {
          "description": "Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDtree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ",
          "name": "algorithm",
          "optional": "true",
          "type": "'auto', 'ball_tree', 'kd_tree', 'brute'"
        },
        {
          "description": "Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ",
          "name": "p",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Additional keyword arguments for the metric function. ",
          "name": "metric_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. Doesn't affect :meth:`fit` method. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/regression.pyc:19",
      "tags": [
        "neighbors",
        "regression"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering"
      ],
      "attributes": [
        {
          "description": "Affinity matrix used for clustering. Available only if after calling ``fit``.  labels_ : Labels of each point ",
          "name": "affinity_matrix_",
          "shape": "n_samples, n_samples",
          "type": "array-like"
        }
      ],
      "category": "cluster.spectral",
      "common_name": "Spectral Clustering",
      "description": "\"Apply clustering to a projection to the normalized laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance when clusters are\nnested circles on the 2D plan.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nWhen calling ``fit``, an affinity matrix is constructed using either\nkernel function such the Gaussian (aka RBF) kernel of the euclidean\ndistanced ``d(X, X)``::\n\nnp.exp(-gamma * d(X,X) ** 2)\n\nor a k-nearest neighbors connectivity matrix.\n\nAlternatively, using ``precomputed``, a user-provided affinity\nmatrix can be used.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n",
      "id": "sklearn.cluster.spectral.SpectralClustering",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Creates an affinity matrix for X using the selected affinity,\nthen applies spectral clustering to this affinity matrix.\n",
          "id": "sklearn.cluster.spectral.SpectralClustering.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "OR, if affinity==`precomputed`, a precomputed affinity matrix of shape (n_samples, n_samples) '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Performs clustering on X and returns cluster labels.\n",
          "id": "sklearn.cluster.spectral.SpectralClustering.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "Input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "cluster labels '",
            "name": "y",
            "shape": "n_samples,",
            "type": "ndarray"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.spectral.SpectralClustering.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.spectral.SpectralClustering.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.cluster.spectral.SpectralClustering",
      "parameters": [
        {
          "description": "The dimension of the projection subspace. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "If a string, this may be one of 'nearest_neighbors', 'precomputed', 'rbf' or one of the kernels supported by `sklearn.metrics.pairwise_kernels`.  Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm. ",
          "name": "affinity",
          "type": "string"
        },
        {
          "description": "Scaling factor of RBF, polynomial, exponential chi^2 and sigmoid affinity kernel. Ignored for ``affinity='nearest_neighbors'``. ",
          "name": "gamma",
          "type": "float"
        },
        {
          "description": "Degree of the polynomial kernel. Ignored by other kernels.  coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. ",
          "name": "degree",
          "type": "float"
        },
        {
          "description": "Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for ``affinity='rbf'``. ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities ",
          "name": "eigen_solver",
          "type": "None, 'arpack', 'lobpcg', or 'amg'"
        },
        {
          "description": "A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by the K-Means initialization. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. ",
          "name": "eigen_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. ",
          "name": "assign_labels",
          "type": "'kmeans', 'discretize'"
        },
        {
          "description": "Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. ",
          "name": "kernel_params",
          "optional": "true",
          "type": "dictionary"
        },
        {
          "description": "The number of parallel jobs to run. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/spectral.pyc:269",
      "tags": [
        "cluster",
        "spectral"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Indices of support vectors. ",
          "name": "support_",
          "shape": "n_SV",
          "type": "array-like"
        },
        {
          "description": "Support vectors. ",
          "name": "support_vectors_",
          "shape": "nSV, n_features",
          "type": "array-like"
        },
        {
          "description": "Coefficients of the support vector in the decision function. ",
          "name": "dual_coef_",
          "shape": "1, n_SV",
          "type": "array"
        },
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "Nu SVR",
      "description": "\"Nu Support Vector Regression.\n\nSimilar to NuSVC, for regression, uses a parameter nu to control\nthe number of support vectors. However, unlike NuSVC, where nu\nreplaces C, here nu replaces the parameter epsilon of epsilon-SVR.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n",
      "id": "sklearn.svm.classes.NuSVR",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDistance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.classes.NuSVR.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train]. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. '",
            "name": "X",
            "shape": "n_samples, n_class * (n_class-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.classes.NuSVR.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.NuSVR.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform regression on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.classes.NuSVR.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is (n_samples_test, n_samples_train). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.svm.classes.NuSVR.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.NuSVR.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.NuSVR",
      "parameters": [
        {
          "default": "1.0",
          "description": "Penalty parameter C of the error term. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].  By default 0.5 will be taken. ",
          "name": "nu",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "'rbf'",
          "description": "Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. ",
          "name": "kernel",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "3",
          "description": "Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. ",
          "name": "degree",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "'auto'",
          "description": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to use the shrinking heuristic. ",
          "name": "shrinking",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1e-3",
          "description": "Tolerance for stopping criterion. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specify the size of the kernel cache (in MB). ",
          "name": "cache_size",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "default": "-1",
          "description": "Hard limit on iterations within solver, or -1 for no limit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:810",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.rcv1.fetch_rcv1",
      "description": "\"Load the RCV1 multilabel dataset, downloading it if necessary.\n\nVersion: RCV1-v2, vectors, full sets, topics multilabels.\n\n==============     =====================\nClasses                              103\nSamples total                     804414\nDimensionality                     47236\nFeatures           real, between 0 and 1\n==============     =====================\n\nRead more in the :ref:`User Guide <datasets>`.\n\n.. versionadded:: 0.17\n",
      "id": "sklearn.datasets.rcv1.fetch_rcv1",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.rcv1.fetch_rcv1",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in '~/scikit_learn_data' subfolders.  subset: string, 'train', 'test', or 'all', default='all' Select the dataset to load: 'train' for the training set (23149 samples), 'test' for the test set (781265 samples), 'all' for both, with the training samples first if shuffle is False. This follows the official LYRL2004 chronological split. ",
          "name": "data_home",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to shuffle dataset. ",
          "name": "shuffle",
          "type": "bool"
        }
      ],
      "returns": {
        "description": " dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236) The array has 0.16% of non zero values.  dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103) Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values.  dataset.sample_id : numpy array, dtype np.uint32, shape (804414,) Identification number of each sample, as ordered in dataset.data.  dataset.target_names : numpy array, dtype object, length (103) Names of each target (RCV1 topics), as ordered in dataset.target.  dataset.DESCR : string Description of the RCV1 dataset.  References ---------- Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004). RCV1: A new benchmark collection for text categorization research. The Journal of Machine Learning Research, 5, 361-397.  \"",
        "name": "dataset",
        "type": "dict-like"
      },
      "tags": [
        "datasets",
        "rcv1"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.optimize.optimize.fmin_bfgs",
      "description": "\"\nMinimize a function using the BFGS algorithm.\n",
      "id": "scipy.optimize.optimize.fmin_bfgs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.optimize.optimize.fmin_bfgs",
      "parameters": [
        {
          "description": "Objective function to be minimized. x0 : ndarray Initial guess.",
          "name": "f",
          "type": "callable"
        },
        {
          "description": "Gradient of f.",
          "name": "fprime",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Extra arguments passed to f and fprime.",
          "name": "args",
          "optional": "true",
          "type": "tuple"
        },
        {
          "description": "Gradient norm must be less than gtol before successful termination.",
          "name": "gtol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Order of norm (Inf is max, -Inf is min)",
          "name": "norm",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If fprime is approximated, use this value for the step size.",
          "name": "epsilon",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "An optional user-supplied function to call after each iteration.  Called as callback(xk), where xk is the current parameter vector.",
          "name": "callback",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Maximum number of iterations to perform.",
          "name": "maxiter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True,return fopt, func_calls, grad_calls, and warnflag in addition to xopt.",
          "name": "full_output",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Print convergence message if True.",
          "name": "disp",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Return a list of results at each iteration if True. ",
          "name": "retall",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Parameters which minimize f, i.e. f(xopt) == fopt. fopt : float Minimum value. gopt : ndarray Value of gradient at minimum, f'(xopt), which should be near 0. Bopt : ndarray Value of 1/f''(xopt), i.e. the inverse hessian matrix. func_calls : int Number of function_calls made. grad_calls : int Number of gradient calls made. warnflag : integer 1 : Maximum number of iterations exceeded. 2 : Gradient and/or function calls not changing. allvecs  :  list `OptimizeResult` at each iteration.  Only returned if retall is True.  See also -------- minimize: Interface to minimization algorithms for multivariate functions. See the 'BFGS' `method` in particular.  Notes ----- Optimize the function, f, whose gradient is given by fprime using the quasi-Newton method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS)  References ---------- Wright, and Nocedal 'Numerical Optimization', 1999, pg. 198.  \"",
        "name": "xopt",
        "type": "ndarray"
      },
      "tags": [
        "optimize",
        "optimize"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.stats.stats.rankdata",
      "description": "'\nrankdata(a, method=\\'average\\')\n\nAssign ranks to data, dealing with ties appropriately.\n\nRanks begin at 1.  The `method` argument controls how ranks are assigned\nto equal values.  See [1]_ for further discussion of ranking methods.\n",
      "id": "scipy.stats.stats.rankdata",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.stats.stats.rankdata",
      "parameters": [
        {
          "description": "The array of values to be ranked.  The array is first flattened.",
          "name": "a",
          "type": "array"
        },
        {
          "description": "The method used to assign ranks to tied elements. The options are \\'average\\', \\'min\\', \\'max\\', \\'dense\\' and \\'ordinal\\'.  \\'average\\': The average of the ranks that would have been assigned to all the tied values is assigned to each value. \\'min\\': The minimum of the ranks that would have been assigned to all the tied values is assigned to each value.  (This is also referred to as \"competition\" ranking.) \\'max\\': The maximum of the ranks that would have been assigned to all the tied values is assigned to each value. \\'dense\\': Like \\'min\\', but the rank of the next highest element is assigned the rank immediately after those assigned to the tied elements. \\'ordinal\\': All values are given a distinct rank, corresponding to the order that the values occur in `a`.  The default is \\'average\\'. ",
          "name": "method",
          "optional": "true",
          "type": "str"
        }
      ],
      "returns": {
        "description": "An array of length equal to the size of `a`, containing rank scores.  References ---------- .. [1] \"Ranking\", http://en.wikipedia.org/wiki/Ranking  Examples -------- >>> from scipy.stats import rankdata >>> rankdata([0, 2, 3, 2]) array([ 1. ,  2.5,  4. ,  2.5]) >>> rankdata([0, 2, 3, 2], method=\\'min\\') array([ 1,  2,  4,  2]) >>> rankdata([0, 2, 3, 2], method=\\'max\\') array([ 1,  3,  4,  3]) >>> rankdata([0, 2, 3, 2], method=\\'dense\\') array([ 1,  2,  3,  2]) >>> rankdata([0, 2, 3, 2], method=\\'ordinal\\') array([ 1,  2,  4,  3]) '",
        "name": "ranks",
        "type": "ndarray"
      },
      "tags": [
        "stats",
        "stats"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.neighbors.graph.radius_neighbors_graph",
      "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "id": "sklearn.neighbors.graph.radius_neighbors_graph",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.neighbors.graph.radius_neighbors_graph",
      "parameters": [
        {
          "description": "Sample data, in the form of a numpy array or a precomputed :class:`BallTree`. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Radius of neighborhoods. ",
          "name": "radius",
          "type": "float"
        },
        {
          "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, and 'distance' will return the distances between neighbors according to the given metric. ",
          "name": "mode",
          "optional": "true",
          "type": "'connectivity', 'distance'"
        },
        {
          "description": "The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is 'euclidean' ('minkowski' metric with the param equal to 2.)  include_self: bool, default=False Whether or not to mark each sample as the first nearest neighbor to itself. If `None`, then True is used for mode='connectivity' and False for mode='distance' as this will preserve backwards compatibilty. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric_params: dict, optional additional keyword arguments for the metric function. ",
          "name": "p",
          "type": "int"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import radius_neighbors_graph >>> A = radius_neighbors_graph(X, 1.5, mode='connectivity', include_self=True) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
        "name": "A",
        "shape": "n_samples, n_samples",
        "type": "sparse"
      },
      "tags": [
        "neighbors",
        "graph"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._split.train_test_split",
      "description": "'Split arrays or matrices into random train and test subsets\n\nQuick utility that wraps input validation and\n``next(ShuffleSplit().split(X, y))`` and application to input data\ninto a single call for splitting (and optionally subsampling) data in a\noneliner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._split.train_test_split",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._split.train_test_split",
      "parameters": [
        {
          "description": "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. ",
          "name": "*arrays",
          "shape": "0",
          "type": "sequence"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. If train size is also None, test size is set to 0.25. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If not None, data is split in a stratified fashion, using this as the class labels. ",
          "name": "stratify",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "List containing train-test split of inputs.  .. versionadded:: 0.16 If the input is sparse, the output will be a ``scipy.sparse.csr_matrix``. Else, output type is the same as the input type.  Examples -------- >>> import numpy as np >>> from sklearn.model_selection import train_test_split >>> X, y = np.arange(10).reshape((5, 2)), range(5) >>> X array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) >>> list(y) [0, 1, 2, 3, 4]  >>> X_train, X_test, y_train, y_test = train_test_split( ...     X, y, test_size=0.33, random_state=42) ... >>> X_train array([[4, 5], [0, 1], [6, 7]]) >>> y_train [2, 0, 3] >>> X_test array([[2, 3], [8, 9]]) >>> y_test [1, 4]  '",
        "name": "splitting",
        "type": "list"
      },
      "tags": [
        "model_selection",
        "_split"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.random_projection.johnson_lindenstrauss_min_dim",
      "description": "'Find a \\'safe\\' number of components to randomly project to\n\nThe distortion introduced by a random projection `p` only changes the\ndistance between two points by a factor (1 +- eps) in an euclidean space\nwith good probability. The projection `p` is an eps-embedding as defined\nby:\n\n(1 - eps) ||u - v||^2 < ||p(u) - p(v)||^2 < (1 + eps) ||u - v||^2\n\nWhere u and v are any rows taken from a dataset of shape [n_samples,\nn_features], eps is in ]0, 1[ and p is a projection by a random Gaussian\nN(0, 1) matrix with shape [n_components, n_features] (or a sparse\nAchlioptas matrix).\n\nThe minimum number of components to guarantee the eps-embedding is\ngiven by:\n\nn_components >= 4 log(n_samples) / (eps^2 / 2 - eps^3 / 3)\n\nNote that the number of dimensions is independent of the original\nnumber of features but instead depends on the size of the dataset:\nthe larger the dataset, the higher is the minimal dimensionality of\nan eps-embedding.\n\nRead more in the :ref:`User Guide <johnson_lindenstrauss>`.\n",
      "id": "sklearn.random_projection.johnson_lindenstrauss_min_dim",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.random_projection.johnson_lindenstrauss_min_dim",
      "parameters": [
        {
          "description": "Number of samples. If an array is given, it will compute a safe number of components array-wise. ",
          "name": "n_samples",
          "type": "int"
        },
        {
          "default": "0.1",
          "description": "Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        }
      ],
      "returns": {
        "description": "The minimal number of components to guarantee with good probability an eps-embedding with n_samples.  Examples --------  >>> johnson_lindenstrauss_min_dim(1e6, eps=0.5) 663  >>> johnson_lindenstrauss_min_dim(1e6, eps=[0.5, 0.1, 0.01]) array([    663,   11841, 1112658])  >>> johnson_lindenstrauss_min_dim([1e4, 1e5, 1e6], eps=0.1) array([ 7894,  9868, 11841])  References ----------  .. [1] https://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma  .. [2] Sanjoy Dasgupta and Anupam Gupta, 1999, \"An elementary proof of the Johnson-Lindenstrauss Lemma.\" http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.3654  '",
        "name": "n_components",
        "type": "int"
      },
      "tags": [
        "random_projection"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.normalized_mutual_info_score",
      "description": "\"Normalized Mutual Information between two clusterings.\n\nNormalized Mutual Information (NMI) is an normalization of the Mutual\nInformation (MI) score to scale the results between 0 (no mutual\ninformation) and 1 (perfect correlation). In this function, mutual\ninformation is normalized by ``sqrt(H(labels_true) * H(labels_pred))``\n\nThis measure is not adjusted for chance. Therefore\n:func:`adjusted_mustual_info_score` might be preferred.\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n",
      "id": "sklearn.metrics.cluster.supervised.normalized_mutual_info_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.normalized_mutual_info_score",
      "parameters": [
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling  See also -------- adjusted_rand_score: Adjusted Rand Index adjusted_mutual_info_score: Adjusted Mutual Information (adjusted against chance)  Examples --------  Perfect labelings are both homogeneous and complete, hence have score 1.0::  >>> from sklearn.metrics.cluster import normalized_mutual_info_score >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1]) 1.0 >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  If classes members are completely split across different clusters, the assignment is totally in-complete, hence the NMI is null::  >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3]) 0.0  \"",
        "name": "nmi",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.log_loss",
      "description": "'Log loss, aka logistic loss or cross-entropy loss.\n\nThis is the loss function used in (multinomial) logistic regression\nand extensions of it such as neural networks, defined as the negative\nlog-likelihood of the true labels given a probabilistic classifier\\'s\npredictions. The log loss is only defined for two or more labels.\nFor a single sample with true label yt in {0,1} and\nestimated probability yp that yt = 1, the log loss is\n\n-log P(yt|yp) = -(yt log(yp) + (1 - yt) log(1 - yp))\n\nRead more in the :ref:`User Guide <log_loss>`.\n",
      "id": "sklearn.metrics.classification.log_loss",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.log_loss",
      "parameters": [
        {
          "description": "Ground truth (correct) labels for n_samples samples. ",
          "name": "y_true",
          "type": "array-like"
        },
        {
          "description": "Predicted probabilities, as returned by a classifier\\'s predict_proba method. If ``y_pred.shape = (n_samples,)`` the probabilities provided are assumed to be that of the positive class. The labels in ``y_pred`` are assumed to be ordered alphabetically, as done by :class:`preprocessing.LabelBinarizer`. ",
          "name": "y_pred",
          "shape": "n_samples, n_classes",
          "type": "array-like"
        },
        {
          "description": "Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)). ",
          "name": "eps",
          "type": "float"
        },
        {
          "default": "True",
          "description": "If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses. ",
          "name": "normalize",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "default": "None",
          "description": "If not provided, labels will be inferred from y_true. If ``labels`` is ``None`` and ``y_pred`` has shape (n_samples,) the labels are assumed to be binary and are inferred from ``y_true``. .. versionadded:: 0.18 ",
          "name": "labels",
          "optional": "true",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " Examples -------- >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],  # doctest: +ELLIPSIS ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]]) 0.21616...  References ---------- C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.  Notes ----- The logarithm used is the natural logarithm (base-e). '",
        "name": "loss",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cross_validation.train_test_split",
      "description": "'Split arrays or matrices into random train and test subsets\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.train_test_split` instead.\n\nQuick utility that wraps input validation and\n``next(iter(ShuffleSplit(n_samples)))`` and application to input\ndata into a single call for splitting (and optionally subsampling)\ndata in a oneliner.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.train_test_split",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cross_validation.train_test_split",
      "parameters": [
        {
          "description": "Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. ",
          "name": "*arrays",
          "shape": "0",
          "type": "sequence"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. If train size is also None, test size is set to 0.25. ",
          "name": "test_size",
          "type": "float"
        },
        {
          "description": "If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. ",
          "name": "train_size",
          "type": "float"
        },
        {
          "description": "Pseudo-random number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If not None, data is split in a stratified fashion, using this as the labels array.  .. versionadded:: 0.17 *stratify* splitting ",
          "name": "stratify",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "List containing train-test split of inputs.  .. versionadded:: 0.16 If the input is sparse, the output will be a ``scipy.sparse.csr_matrix``. Else, output type is the same as the input type.  Examples -------- >>> import numpy as np >>> from sklearn.cross_validation import train_test_split >>> X, y = np.arange(10).reshape((5, 2)), range(5) >>> X array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]]) >>> list(y) [0, 1, 2, 3, 4]  >>> X_train, X_test, y_train, y_test = train_test_split( ...     X, y, test_size=0.33, random_state=42) ... >>> X_train array([[4, 5], [0, 1], [6, 7]]) >>> y_train [2, 0, 3] >>> X_test array([[2, 3], [8, 9]]) >>> y_test [1, 4]  '",
        "name": "splitting",
        "type": "list"
      },
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.neighbors.graph.kneighbors_graph",
      "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "id": "sklearn.neighbors.graph.kneighbors_graph",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.neighbors.graph.kneighbors_graph",
      "parameters": [
        {
          "description": "Sample data, in the form of a numpy array or a precomputed :class:`BallTree`. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of neighbors for each sample. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, and 'distance' will return the distances between neighbors according to the given metric. ",
          "name": "mode",
          "optional": "true",
          "type": "'connectivity', 'distance'"
        },
        {
          "description": "The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is 'euclidean' ('minkowski' metric with the p param equal to 2.)  include_self: bool, default=False. Whether or not to mark each sample as the first nearest neighbor to itself. If `None`, then True is used for mode='connectivity' and False for mode='distance' as this will preserve backwards compatibilty. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.  metric_params: dict, optional additional keyword arguments for the metric function. ",
          "name": "p",
          "type": "int"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import kneighbors_graph >>> A = kneighbors_graph(X, 2, mode='connectivity', include_self=True) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- radius_neighbors_graph \"",
        "name": "A",
        "shape": "n_samples, n_samples",
        "type": "sparse"
      },
      "tags": [
        "neighbors",
        "graph"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.lib.arraysetops.in1d",
      "description": "'\nTest whether each element of a 1-D array is also present in a second array.\n\nReturns a boolean array the same length as `ar1` that is True\nwhere an element of `ar1` is in `ar2` and False otherwise.\n",
      "id": "numpy.lib.arraysetops.in1d",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.lib.arraysetops.in1d",
      "parameters": [
        {
          "description": "Input array. ar2 : array_like The values against which to test each value of `ar1`.",
          "name": "ar1",
          "type": ""
        },
        {
          "description": "If True, the input arrays are both assumed to be unique, which can speed up the calculation.  Default is False.",
          "name": "assume_unique",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If True, the values in the returned array are inverted (that is, False where an element of `ar1` is in `ar2` and True otherwise). Default is False. ``np.in1d(a, b, invert=True)`` is equivalent to (but is faster than) ``np.invert(in1d(a, b))``.  .. versionadded:: 1.8.0 ",
          "name": "invert",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The values `ar1[in1d]` are in `ar2`.  See Also -------- numpy.lib.arraysetops : Module with a number of other functions for performing set operations on arrays.  Notes ----- `in1d` can be considered as an element-wise function version of the python keyword `in`, for 1-D sequences. ``in1d(a, b)`` is roughly equivalent to ``np.array([item in b for item in a])``. However, this idea fails if `ar2` is a set, or similar (non-sequence) container:  As ``ar2`` is converted to an array, in those cases ``asarray(ar2)`` is an object array rather than the expected array of contained values.  .. versionadded:: 1.4.0  Examples -------- >>> test = np.array([0, 1, 2, 5, 0]) >>> states = [0, 2] >>> mask = np.in1d(test, states) >>> mask array([ True, False,  True, False,  True], dtype=bool) >>> test[mask] array([0, 2, 0]) >>> mask = np.in1d(test, states, invert=True) >>> mask array([False,  True, False,  True, False], dtype=bool) >>> test[mask] array([1, 5]) '",
        "name": "in1d",
        "type": ""
      },
      "tags": [
        "lib",
        "arraysetops"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Estimated robust location ",
          "name": "location_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "Estimated robust covariance matrix ",
          "name": "covariance_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "Estimated pseudo inverse matrix. (stored only if store_precision is True) ",
          "name": "precision_",
          "shape": "n_features, n_features",
          "type": "array-like"
        },
        {
          "description": "A mask of the observations that have been used to compute the robust estimates of location and shape.  See Also -------- EmpiricalCovariance, MinCovDet ",
          "name": "support_",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "category": "covariance.outlier_detection",
      "common_name": "Elliptic Envelope",
      "description": "'An object for detecting outliers in a Gaussian distributed dataset.\n\nRead more in the :ref:`User Guide <outlier_detection>`.\n",
      "id": "sklearn.covariance.outlier_detection.EllipticEnvelope",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply a correction to raw Minimum Covariance Determinant estimates.\n\nCorrection using the empirical correction factor suggested\nby Rousseeuw and Van Driessen in [Rouseeuw1984]_.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.correct_covariance",
          "name": "correct_covariance",
          "parameters": [
            {
              "description": "The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates. ",
              "name": "data",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Corrected robust covariance estimate.  '",
            "name": "covariance_corrected",
            "shape": "n_features, n_features",
            "type": "array-like"
          }
        },
        {
          "description": "'Compute the decision function of the given observations.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools. ",
              "name": "raw_values",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "The values of the decision function for each observations. It is equal to the Mahalanobis distances if `raw_values` is True. By default (``raw_values=True``), it is equal to the cubic root of the shifted Mahalanobis distances. In that case, the threshold for being an outlier is 0, which ensures a compatibility with other outlier detection tools such as the One-Class SVM.  '",
            "name": "decision",
            "shape": "n_samples, ",
            "type": "array-like"
          }
        },
        {
          "description": "\"Computes the Mean Squared Error between two covariance estimators.\n(In the sense of the Frobenius norm).\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.error_norm",
          "name": "error_norm",
          "parameters": [
            {
              "description": "The covariance to compare with. ",
              "name": "comp_cov",
              "shape": "n_features, n_features",
              "type": "array-like"
            },
            {
              "description": "The type of norm used to compute the error. Available error types: - 'frobenius' (default): sqrt(tr(A^t.A)) - 'spectral': sqrt(max(eigenvalues(A^t.A)) where A is the error ``(comp_cov - self.covariance_)``. ",
              "name": "norm",
              "type": "str"
            },
            {
              "description": "If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled. ",
              "name": "scaling",
              "type": "bool"
            },
            {
              "description": "Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned. ",
              "name": "squared",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "`self` and `comp_cov` covariance estimators.  \"",
            "name": "The Mean Squared Error (in the sense of the Frobenius norm) between"
          }
        },
        {
          "description": "None",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Getter for the precision matrix.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "The precision matrix associated to the current covariance object.  '",
            "name": "precision_",
            "type": "array-like"
          }
        },
        {
          "description": "'Computes the squared Mahalanobis distances of given observations.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.mahalanobis",
          "name": "mahalanobis",
          "parameters": [
            {
              "description": "The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit. ",
              "name": "observations",
              "shape": "n_observations, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Squared Mahalanobis distances of the observations.  '",
            "name": "mahalanobis_distance",
            "shape": "n_observations,",
            "type": "array"
          }
        },
        {
          "description": "\"Outlyingness of observations in X according to the fitted model.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each observations, tells whether or not it should be considered as an outlier according to the fitted model.  threshold : float, The values of the less outlying point's decision function.  \"",
            "name": "is_outliers",
            "shape": "n_samples, ",
            "type": "array"
          }
        },
        {
          "description": "\"Re-weight raw Minimum Covariance Determinant estimates.\n\nRe-weight observations using Rousseeuw's method (equivalent to\ndeleting outlying observations from the data set before\ncomputing location and covariance estimates). [Rouseeuw1984]_\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.reweight_covariance",
          "name": "reweight_covariance",
          "parameters": [
            {
              "description": "The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates. ",
              "name": "data",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Re-weighted robust location estimate.  covariance_reweighted : array-like, shape (n_features, n_features) Re-weighted robust covariance estimate.  support_reweighted : array-like, type boolean, shape (n_samples,) A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.  \"",
            "name": "location_reweighted",
            "shape": "n_features, ",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.covariance.outlier_detection.EllipticEnvelope.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.covariance.outlier_detection.EllipticEnvelope",
      "parameters": [
        {
          "description": "Specify if the estimated precision is stored. ",
          "name": "store_precision",
          "type": "bool"
        },
        {
          "description": "If True, the support of robust location and covariance estimates is computed, and a covariance estimate is recomputed from it, without centering the data. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, the robust location and covariance are directly computed with the FastMCD algorithm without additional treatment. ",
          "name": "assume_centered",
          "type": ""
        },
        {
          "description": "The proportion of points to be included in the support of the raw MCD estimate. Default is ``None``, which implies that the minimum value of support_fraction will be used within the algorithm: `[n_sample + n_features + 1] / 2`. ",
          "name": "support_fraction",
          "type": "float"
        },
        {
          "description": "The amount of contamination of the data set, i.e. the proportion of outliers in the data set. ",
          "name": "contamination",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/covariance/outlier_detection.pyc:104",
      "tags": [
        "covariance",
        "outlier_detection"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "A dictionary mapping feature names to feature indices. ",
          "name": "vocabulary_",
          "type": "dict"
        },
        {
          "description": "A list of length n_features containing the feature names (e.g., \"f=ham\" and \"f=spam\"). ",
          "name": "feature_names_",
          "type": "list"
        }
      ],
      "category": "feature_extraction.dict_vectorizer",
      "common_name": "Dict Vectorizer",
      "description": "'Transforms lists of feature-value mappings to vectors.\n\nThis transformer turns lists of mappings (dict-like objects) of feature\nnames to feature values into Numpy arrays or scipy.sparse matrices for use\nwith scikit-learn estimators.\n\nWhen feature values are strings, this transformer will do a binary one-hot\n(aka one-of-K) coding: one boolean-valued feature is constructed for each\nof the possible string values that the feature can take on. For instance,\na feature \"f\" that can take on the values \"ham\" and \"spam\" will become two\nfeatures in the output, one signifying \"f=ham\", the other \"f=spam\".\n\nHowever, note that this transformer will only do a binary one-hot encoding\nwhen feature values are of type string. If categorical features are\nrepresented as numeric values such as int, the DictVectorizer can be\nfollowed by OneHotEncoder to complete binary one-hot encoding.\n\nFeatures that do not occur in a sample (mapping) will have a zero value\nin the resulting array/matrix.\n\nRead more in the :ref:`User Guide <dict_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Learn a list of feature name -> indices mappings.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).",
              "name": "X",
              "type": ""
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Learn a list of feature name -> indices mappings and transform X.\n\nLike fit(X) followed by transform(X), but does not require\nmaterializing X in memory.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).",
              "name": "X",
              "type": ""
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "Feature vectors; always 2-d. '",
            "name": "Xa",
            "type": "array, sparse matrix"
          }
        },
        {
          "description": "'Returns a list of feature names, ordered by their indices.\n\nIf one-of-K coding is applied to categorical features, this will\ninclude the constructed feature names but not the original ones.\n'",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.get_feature_names",
          "name": "get_feature_names",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Transform array or sparse matrix X back to feature mappings.\n\nX must have been produced by this DictVectorizer's transform or\nfit_transform method; it may only have passed through transformers\nthat preserve the number of features and their order.\n\nIn the case of one-hot/one-of-K coding, the constructed feature\nnames and values are returned rather than the original ones.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Sample matrix.",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Constructor for feature mappings. Must conform to the collections.Mapping API. ",
              "name": "dict_type",
              "optional": "true",
              "type": "callable"
            }
          ],
          "returns": {
            "description": "Feature mappings for the samples in X. \"",
            "name": "D",
            "type": "list"
          }
        },
        {
          "description": "\"Restrict the features to those in support using feature selection.\n\nThis function modifies the estimator in-place.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.restrict",
          "name": "restrict",
          "parameters": [
            {
              "description": "Boolean mask or list of indices (as returned by the get_support member of feature selectors).",
              "name": "support",
              "type": "array-like"
            },
            {
              "description": "Whether support is a list of indices. ",
              "name": "indices",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": " Examples -------- >>> from sklearn.feature_extraction import DictVectorizer >>> from sklearn.feature_selection import SelectKBest, chi2 >>> v = DictVectorizer() >>> D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}] >>> X = v.fit_transform(D) >>> support = SelectKBest(chi2, k=2).fit(X, [0, 1]) >>> v.get_feature_names() ['bar', 'baz', 'foo'] >>> v.restrict(support.get_support()) # doctest: +ELLIPSIS DictVectorizer(dtype=..., separator='=', sort=True, sparse=True) >>> v.get_feature_names() ['bar', 'foo'] \"",
            "name": "self"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform feature->value dicts to array or sparse matrix.\n\nNamed features not encountered during fit or fit_transform will be\nsilently ignored.\n",
          "id": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).",
              "name": "X",
              "type": ""
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "Feature vectors; always 2-d. '",
            "name": "Xa",
            "type": "array, sparse matrix"
          }
        }
      ],
      "name": "sklearn.feature_extraction.dict_vectorizer.DictVectorizer",
      "parameters": [
        {
          "description": "The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument.",
          "name": "dtype",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Separator string used when constructing new features for one-hot coding.",
          "name": "separator",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Whether transform should produce scipy.sparse matrices. True by default.",
          "name": "sparse",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether ``feature_names_`` and ``vocabulary_`` should be sorted when fitting. True by default. ",
          "name": "sort",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/dict_vectorizer.pyc:27",
      "tags": [
        "feature_extraction",
        "dict_vectorizer"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by a boolean mask. ",
          "name": "estimators_samples_",
          "type": "list"
        },
        {
          "description": "The subset of drawn features for each base estimator. ",
          "name": "estimators_features_",
          "type": "list"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Prediction computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_prediction_` might contain NaN. ",
          "name": "oob_prediction_",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "category": "ensemble.bagging",
      "common_name": "Bagging Regressor",
      "description": "'A Bagging regressor.\n\nA Bagging regressor is an ensemble meta-estimator that fits base\nregressors each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n",
      "id": "sklearn.ensemble.bagging.BaggingRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Build a Bagging ensemble of estimators from the training\nset (X, y).\n",
          "id": "sklearn.ensemble.bagging.BaggingRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.bagging.BaggingRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the estimators in the ensemble.\n",
          "id": "sklearn.ensemble.bagging.BaggingRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.bagging.BaggingRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.bagging.BaggingRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.bagging.BaggingRegressor",
      "parameters": [
        {
          "default": "None",
          "description": "The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "default": "10",
          "description": "The number of base estimators in the ensemble. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1.0",
          "description": "The number of samples to draw from X to train each base estimator. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. ",
          "name": "max_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1.0",
          "description": "The number of features to draw from X to train each base estimator. - If int, then draw `max_features` features. - If float, then draw `max_features * X.shape[1]` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Whether samples are drawn with replacement. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "Whether features are drawn with replacement. ",
          "name": "bootstrap_features",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use out-of-bag samples to estimate the generalization error. ",
          "name": "oob_score",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/bagging.pyc:796",
      "tags": [
        "ensemble",
        "bagging"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the cost function formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "``sparse_coef_`` is a readonly property derived from ``coef_`` ",
          "name": "sparse_coef_",
          "shape": "n_features, 1",
          "type": "scipy"
        },
        {
          "description": "independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance. ",
          "name": "n_iter_",
          "shape": "n_targets,",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Lasso",
      "description": "\"Linear Model trained with L1 prior as regularizer (aka the Lasso)\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nTechnically the Lasso model is optimizing the same objective function as\nthe Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).\n\nRead more in the :ref:`User Guide <lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.Lasso",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDecision function of the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "The predicted decision function '",
            "name": "T",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Fit model with coordinate descent.\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data ",
              "name": "X",
              "type": "ndarray"
            },
            {
              "description": "Target ",
              "name": "y",
              "shape": "n_samples,",
              "type": "ndarray"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  Notes -----  Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary.  To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. \"",
              "name": "check_input",
              "type": "boolean"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.Lasso.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.Lasso",
      "parameters": [
        {
          "description": "Constant that multiplies the L1 term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by the :class:`LinearRegression` object. For numerical reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised. Given this, you should use the :class:`LinearRegression` object. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always ``True`` to preserve sparsity. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to ``True``, forces the coefficients to be positive. ",
          "name": "positive",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:787",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Parameter vector (w in the problem formulation). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Estimated number of non-zero coefficients giving the best mean squared error over the cross-validation folds. ",
          "name": "n_nonzero_coefs_",
          "type": "int"
        },
        {
          "description": "Number of active features across every target for the model refit with the best hyperparameters got by cross-validating across all folds.  See also -------- orthogonal_mp orthogonal_mp_gram lars_path Lars LassoLars OrthogonalMatchingPursuit LarsCV LassoLarsCV decomposition.sparse_encode ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.omp",
      "common_name": "Orthogonal Matching Pursuit CV",
      "description": "'Cross-validated Orthogonal Matching Pursuit model (OMP)\n",
      "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.omp.OrthogonalMatchingPursuitCV",
      "parameters": [
        {
          "description": "Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway. ",
          "name": "copy",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to `False`. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Maximum numbers of iterations to perform, therefore maximum features to include. 10% of ``n_features`` but at least 5 if available. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Sets the verbosity amount  Read more in the :ref:`User Guide <omp>`. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/omp.pyc:750",
      "tags": [
        "linear_model",
        "omp"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Stores the embedding vectors. ",
          "name": "embedding_",
          "shape": "n_samples, n_components",
          "type": "array-like"
        },
        {
          "description": "Kullback-Leibler divergence after optimization. ",
          "name": "kl_divergence_",
          "type": "float"
        }
      ],
      "category": "manifold.t_sne",
      "common_name": "TSNE",
      "description": "'t-distributed Stochastic Neighbor Embedding.\n\nt-SNE [1] is a tool to visualize high-dimensional data. It converts\nsimilarities between data points to joint probabilities and tries\nto minimize the Kullback-Leibler divergence between the joint\nprobabilities of the low-dimensional embedding and the\nhigh-dimensional data. t-SNE has a cost function that is not convex,\ni.e. with different initializations we can get different results.\n\nIt is highly recommended to use another dimensionality reduction\nmethod (e.g. PCA for dense data or TruncatedSVD for sparse data)\nto reduce the number of dimensions to a reasonable amount (e.g. 50)\nif the number of features is very high. This will suppress some\nnoise and speed up the computation of pairwise distances between\nsamples. For more tips see Laurens van der Maaten\\'s FAQ [2].\n\nRead more in the :ref:`User Guide <t_sne>`.\n",
      "id": "sklearn.manifold.t_sne.TSNE",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit X into an embedded space.\n",
          "id": "sklearn.manifold.t_sne.TSNE.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "If the metric is 'precomputed' X must be a square distance matrix. Otherwise it contains a sample per row. If the method is 'exact', X may be a sparse matrix of type 'csr', 'csc' or 'coo'. \"",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ]
        },
        {
          "description": "\"Fit X into an embedded space and return that transformed\noutput.\n",
          "id": "sklearn.manifold.t_sne.TSNE.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "If the metric is 'precomputed' X must be a square distance matrix. Otherwise it contains a sample per row. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Embedding of the training data in low-dimensional space. \"",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.manifold.t_sne.TSNE.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.manifold.t_sne.TSNE.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.manifold.t_sne.TSNE",
      "parameters": [
        {
          "description": "Dimension of the embedded space. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter. ",
          "name": "perplexity",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high. ",
          "name": "early_exaggeration",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The learning rate can be a critical parameter. It should be between 100 and 1000. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high. If the cost function gets stuck in a bad local minimum increasing the learning rate helps sometimes. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations for the optimization. Should be at least 200. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Only used if method=\\'exact\\' Maximum number of iterations without progress before we abort the optimization. If method=\\'barnes_hut\\' this parameter is fixed to a value of 30 and cannot be changed.  .. versionadded:: 0.17 parameter *n_iter_without_progress* to control stopping criteria. ",
          "name": "n_iter_without_progress",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Only used if method=\\'exact\\' If the gradient norm is below this threshold, the optimization will be aborted. If method=\\'barnes_hut\\' this parameter is fixed to a value of 1e-3 and cannot be changed. ",
          "name": "min_grad_norm",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is \"euclidean\" which is interpreted as squared euclidean distance. ",
          "name": "metric",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Initialization of embedding. Possible options are \\'random\\', \\'pca\\', and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization. ",
          "name": "init",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Pseudo Random Number generator seed control. If None, use the numpy.random singleton. Note that different initializations might result in different local minima of the cost function. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=\\'exact\\' will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples.  .. versionadded:: 0.17 Approximate optimization *method* via the Barnes-Hut. ",
          "name": "method",
          "type": "string"
        },
        {
          "description": "Only used if method=\\'barnes_hut\\' This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. \\'angle\\' is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below \\'angle\\' then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error.  ",
          "name": "angle",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/manifold/t_sne.pyc:497",
      "tags": [
        "manifold",
        "t_sne"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "Non-negative components of the data. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Frobenius norm of the matrix difference between the training data and the reconstructed data from the fit produced by the model. ``|| X - WH ||_2`` ",
          "name": "reconstruction_err_",
          "type": "number"
        },
        {
          "description": "Actual number of iterations. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.nmf",
      "common_name": "NMF",
      "description": "'Non-Negative Matrix Factorization (NMF)\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is::\n\n0.5 * ||X - WH||_Fro^2\n+ alpha * l1_ratio * ||vec(W)||_1\n+ alpha * l1_ratio * ||vec(H)||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\nWhere::\n\n||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)\n||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\nThe objective function is minimized with an alternating minimization of W\nand H.\n\nRead more in the :ref:`User Guide <NMF>`.\n",
      "id": "sklearn.decomposition.nmf.NMF",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Learn a NMF model for the data X.\n",
          "id": "sklearn.decomposition.nmf.NMF.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data matrix to be decomposed ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "\"Learn a NMF model for the data X and returns the transformed data.\n\nThis is more efficient than calling fit followed by transform.\n",
          "id": "sklearn.decomposition.nmf.NMF.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Data matrix to be decomposed  W : array-like, shape (n_samples, n_components) If init='custom', it is used as initial guess for the solution.  H : array-like, shape (n_components, n_features) If init='custom', it is used as initial guess for the solution. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Transformed data. \"",
            "name": "W: array, shape (n_samples, n_components)"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.nmf.NMF.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Transform data back to its original space.\n",
          "id": "sklearn.decomposition.nmf.NMF.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "Transformed data matrix ",
              "name": "W",
              "shape": "n_samples, n_components",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Data matrix of original shape  .. versionadded:: 0.18 '",
            "name": "X: {array-like, sparse matrix}, shape (n_samples, n_features)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.nmf.NMF.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform the data X according to the fitted NMF model\n",
          "id": "sklearn.decomposition.nmf.NMF.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Data matrix to be transformed by the model ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Transformed data '",
            "name": "W: array, shape (n_samples, n_components)"
          }
        }
      ],
      "name": "sklearn.decomposition.nmf.NMF",
      "parameters": [
        {
          "description": "Number of components, if n_components is not set all features are kept. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Method used to initialize the procedure. Default: \\'nndsvdar\\' if n_components < n_features, otherwise random. Valid options:  - \\'random\\': non-negative random matrices, scaled with: sqrt(X.mean() / n_components)  - \\'nndsvd\\': Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)  - \\'nndsvda\\': NNDSVD with zeros filled with the average of X (better when sparsity is not desired)  - \\'nndsvdar\\': NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)  - \\'custom\\': use custom matrices W and H ",
          "name": "init",
          "type": ""
        },
        {
          "description": "Numerical solver to use: \\'pg\\' is a Projected Gradient solver (deprecated). \\'cd\\' is a Coordinate Descent solver (recommended).  .. versionadded:: 0.17 Coordinate Descent solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. ",
          "name": "solver",
          "type": ""
        },
        {
          "description": "Tolerance value used in stopping conditions. ",
          "name": "tol",
          "type": "double"
        },
        {
          "description": "Number of iterations to compute. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "Random number generator seed control. ",
          "name": "random_state",
          "type": "integer"
        },
        {
          "description": "Constant that multiplies the regularization terms. Set it to zero to have no regularization.  .. versionadded:: 0.17 *alpha* used in the Coordinate Descent solver.  l1_ratio : double, default: 0. The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2.  .. versionadded:: 0.17 Regularization parameter *l1_ratio* used in the Coordinate Descent solver. ",
          "name": "alpha",
          "type": "double"
        },
        {
          "description": "If true, randomize the order of coordinates in the CD solver.  .. versionadded:: 0.17 *shuffle* parameter used in the Coordinate Descent solver. ",
          "name": "shuffle",
          "type": "boolean"
        },
        {
          "description": "Number of iterations in NLS subproblem. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "nls_max_iter",
          "type": "integer"
        },
        {
          "description": "Where to enforce sparsity in the model. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "sparseness",
          "type": ""
        },
        {
          "description": "Degree of sparseness, if sparseness is not None. Larger values mean more sparseness. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "beta",
          "type": "double"
        },
        {
          "description": "Degree of correctness to maintain, if sparsity is not None. Smaller values mean larger error. Used only in the deprecated \\'pg\\' solver.  .. versionchanged:: 0.17 Deprecated Projected Gradient solver. Use Coordinate Descent solver instead. ",
          "name": "eta",
          "type": "double"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/nmf.pyc:808",
      "tags": [
        "decomposition",
        "nmf"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The subset of drawn samples (i.e., the in-bag samples) for each base estimator. ",
          "name": "estimators_samples_",
          "type": "list"
        },
        {
          "description": "The actual number of samples ",
          "name": "max_samples_",
          "type": "integer"
        }
      ],
      "category": "ensemble.iforest",
      "common_name": "Isolation Forest",
      "description": "'Isolation Forest Algorithm\n\nReturn the anomaly score of each sample using the IsolationForest algorithm\n\nThe IsolationForest \\'isolates\\' observations by randomly selecting a feature\nand then randomly selecting a split value between the maximum and minimum\nvalues of the selected feature.\n\nSince recursive partitioning can be represented by a tree structure, the\nnumber of splittings required to isolate a sample is equivalent to the path\nlength from the root node to the terminating node.\n\nThis path length, averaged over a forest of such random trees, is a\nmeasure of normality and our decision function.\n\nRandom partitioning produces noticeably shorter paths for anomalies.\nHence, when a forest of random trees collectively produce shorter path\nlengths for particular samples, they are highly likely to be anomalies.\n\nRead more in the :ref:`User Guide <isolation_forest>`.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.ensemble.iforest.IsolationForest",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Average anomaly score of X of the base classifiers.\n\nThe anomaly score of an input sample is computed as\nthe mean anomaly score of the trees in the forest.\n\nThe measure of normality of an observation given a tree is the depth\nof the leaf containing this observation, which is equivalent to\nthe number of splittings required to isolate this point. In case of\nseveral observations n_left in the leaf, the average path length of\na n_left samples isolation tree is added.\n",
          "id": "sklearn.ensemble.iforest.IsolationForest.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The anomaly score of the input samples. The lower, the more abnormal.  '",
            "name": "scores",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit estimator.\n",
          "id": "sklearn.ensemble.iforest.IsolationForest.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The input samples. Use ``dtype=np.float32`` for maximum efficiency. Sparse matrices are also supported, use sparse ``csc_matrix`` for maximum efficiency. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.iforest.IsolationForest.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict if a particular sample is an outlier or not.\n",
          "id": "sklearn.ensemble.iforest.IsolationForest.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each observations, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model. '",
            "name": "is_inlier",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.iforest.IsolationForest.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.iforest.IsolationForest",
      "parameters": [
        {
          "default": "100",
          "description": "The number of base estimators in the ensemble. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\"auto\"",
          "description": "The number of samples to draw from X to train each base estimator. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. - If \"auto\", then `max_samples=min(256, n_samples)`. If max_samples is larger than the number of samples provided, all samples will be used for all trees (no sampling). ",
          "name": "max_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.1",
          "description": "The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function. ",
          "name": "contamination",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "1.0",
          "description": "The number of features to draw from X to train each base estimator.  - If int, then draw `max_features` features. - If float, then draw `max_features * X.shape[1]` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "If True, individual trees are fit on random subsets of the training data sampled with replacement. If False, sampling without replacement is performed. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process.  ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/iforest.pyc:25",
      "tags": [
        "ensemble",
        "iforest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "tree.tree",
      "common_name": "Extra Tree Regressor",
      "description": "'An extremely randomized tree regressor.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nSee also\n--------\nExtraTreeClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\nMachine Learning, 63(1), 3-42, 2006.\n'",
      "id": "sklearn.tree.tree.ExtraTreeRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nReturns the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within ``[0; self.tree_.node_count)``, possibly with gaps in the numbering. \"",
            "name": "X_leaves",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Return the decision path in the tree\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  \"",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "\"Build a decision tree regressor from the training set (X, y).\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (real numbers). Use ``dtype=np.float64`` and ``order='C'`` for maximum efficiency. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  X_idx_sorted : array-like, shape = [n_samples, n_features], optional The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don't use this parameter unless you know what to do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Returns self. \"",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "The predicted classes, or the predict values. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.tree.tree.ExtraTreeRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.tree.tree.ExtraTreeRegressor",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc:1083",
      "tags": [
        "tree",
        "tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Indices of support vectors. ",
          "name": "support_",
          "shape": "n_SV",
          "type": "array-like"
        },
        {
          "description": "Support vectors. ",
          "name": "support_vectors_",
          "shape": "n_SV, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of support vectors for each class. ",
          "name": "n_support_",
          "shape": "n_class",
          "type": "array-like"
        },
        {
          "description": "Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the section about multi-class classification in the SVM section of the User Guide for details. ",
          "name": "dual_coef_",
          "shape": "n_class-1, n_SV",
          "type": "array"
        },
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is a readonly property derived from `dual_coef_` and `support_vectors_`. ",
          "name": "coef_",
          "shape": "n_class-1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "n_class * (n_class-1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "SVC",
      "description": "'C-Support Vector Classification.\n\nThe implementation is based on libsvm. The fit time complexity\nis more than quadratic with the number of samples which makes it hard\nto scale to dataset with more than a couple of 10000 samples.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided\nkernel functions and how `gamma`, `coef0` and `degree` affect each\nother, see the corresponding section in the narrative documentation:\n:ref:`svm_kernels`.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n",
      "id": "sklearn.svm.classes.SVC",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Distance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.classes.SVC.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. If decision_function_shape='ovr', the shape is (n_samples, n_classes) \"",
            "name": "X",
            "shape": "n_samples, n_classes * (n_classes-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.classes.SVC.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.SVC.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform classification on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.classes.SVC.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train] ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Class labels for samples in X. '",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.svm.classes.SVC.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.SVC.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.SVC",
      "parameters": [
        {
          "default": "1.0",
          "description": "Penalty parameter C of the error term. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "\\'rbf\\'",
          "description": "Specifies the kernel type to be used in the algorithm. It must be one of \\'linear\\', \\'poly\\', \\'rbf\\', \\'sigmoid\\', \\'precomputed\\' or a callable. If none is given, \\'rbf\\' will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape ``(n_samples, n_samples)``. ",
          "name": "kernel",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "3",
          "description": "Degree of the polynomial kernel function (\\'poly\\'). Ignored by all other kernels. ",
          "name": "degree",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Kernel coefficient for \\'rbf\\', \\'poly\\' and \\'sigmoid\\'. If gamma is \\'auto\\' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in \\'poly\\' and \\'sigmoid\\'. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether to enable probability estimates. This must be enabled prior to calling `fit`, and will slow down that method. ",
          "name": "probability",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "True",
          "description": "Whether to use the shrinking heuristic. ",
          "name": "shrinking",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1e-3",
          "description": "Tolerance for stopping criterion. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specify the size of the kernel cache (in MB). ",
          "name": "cache_size",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict, \\'balanced\\'"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "default": "-1",
          "description": "Hard limit on iterations within solver, or -1 for no limit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to return a one-vs-rest (\\'ovr\\') decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (\\'ovo\\') decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). The default of None will currently behave as \\'ovo\\' for backward compatibility and raise a deprecation warning, but will change \\'ovr\\' in 0.19.  .. versionadded:: 0.17 *decision_function_shape=\\'ovr\\'* is recommended.  .. versionchanged:: 0.17 Deprecated *decision_function_shape=\\'ovo\\' and None*. ",
          "name": "decision_function_shape",
          "type": ""
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data for probability estimation. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:387",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Indices of support vectors. ",
          "name": "support_",
          "shape": "n_SV",
          "type": "array-like"
        },
        {
          "description": "Support vectors. ",
          "name": "support_vectors_",
          "shape": "nSV, n_features",
          "type": "array-like"
        },
        {
          "description": "Coefficients of the support vector in the decision function. ",
          "name": "dual_coef_",
          "shape": "1, n_SV",
          "type": "array"
        },
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  `coef_` is readonly property derived from `dual_coef_` and `support_vectors_`. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        },
        {
          "description": "Individual weights for each sample ",
          "name": "sample_weight",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "category": "svm.classes",
      "common_name": "SVR",
      "description": "\"Epsilon-Support Vector Regression.\n\nThe free parameters in the model are C and epsilon.\n\nThe implementation is based on libsvm.\n\nRead more in the :ref:`User Guide <svm_regression>`.\n",
      "id": "sklearn.svm.classes.SVR",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nDistance of the samples X to the separating hyperplane.\n",
          "id": "sklearn.svm.classes.SVR.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is [n_samples_test, n_samples_train]. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the decision function of the sample for each class in the model. '",
            "name": "X",
            "shape": "n_samples, n_class * (n_class-1",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit the SVM model according to the given training data.\n",
          "id": "sklearn.svm.classes.SVR.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=\"precomputed\", the expected shape of X is (n_samples, n_samples). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (class labels in classification, real numbers in regression) ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  Notes ------ If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied.  If X is a dense array, then the other methods will not support sparse matrices as input. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.SVR.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Perform regression on samples in X.\n\nFor an one-class model, +1 or -1 is returned.\n",
          "id": "sklearn.svm.classes.SVR.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "For kernel=\"precomputed\", the expected shape of X is (n_samples_test, n_samples_train). ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "'",
            "name": "y_pred",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.svm.classes.SVR.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.SVR.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.svm.classes.SVR",
      "parameters": [
        {
          "default": "1.0",
          "description": "Penalty parameter C of the error term. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "0.1",
          "description": "Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. ",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "'rbf'",
          "description": "Specifies the kernel type to be used in the algorithm. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or a callable. If none is given, 'rbf' will be used. If a callable is given it is used to precompute the kernel matrix. ",
          "name": "kernel",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "3",
          "description": "Degree of the polynomial kernel function ('poly'). Ignored by all other kernels. ",
          "name": "degree",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "'auto'",
          "description": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. If gamma is 'auto' then 1/n_features will be used instead.  coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'. ",
          "name": "gamma",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to use the shrinking heuristic. ",
          "name": "shrinking",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1e-3",
          "description": "Tolerance for stopping criterion. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specify the size of the kernel cache (in MB). ",
          "name": "cache_size",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "default": "-1",
          "description": "Hard limit on iterations within solver, or -1 for no limit. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:696",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "ensemble.forest",
      "common_name": "Forest Classifier",
      "description": "'Base class for forest of trees-based classifiers.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n'",
      "id": "sklearn.ensemble.forest.ForestClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class for X.\n\nThe predicted class of an input sample is a vote by the trees in\nthe forest, weighted by their probability estimates. That is,\nthe predicted class is the one with highest mean probability\nestimate across the trees.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the trees in the\nforest.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample are computed as\nthe mean predicted class probabilities of the trees in the forest. The\nclass probability of a single tree is the fraction of samples of the same\nclass in a leaf.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.ForestClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.ForestClassifier",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:378",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "ensemble.forest",
      "common_name": "Forest Regressor",
      "description": "'Base class for forest of trees-based regressors.\n\nWarning: This class should not be used directly. Use derived classes\ninstead.\n'",
      "id": "sklearn.ensemble.forest.ForestRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the trees in the forest.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.ForestRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.ForestRegressor",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:637",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        }
      ],
      "category": "ensemble.forest",
      "common_name": "Random Trees Embedding",
      "description": "'An ensemble of totally random trees.\n\nAn unsupervised transformation of a dataset to a high-dimensional\nsparse representation. A datapoint is coded according to which leaf of\neach tree it is sorted into. Using a one-hot encoding of the leaves,\nthis leads to a binary coding with as many ones as there are trees in\nthe forest.\n\nThe dimensionality of the resulting representation is\n``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\nthe number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n\nRead more in the :ref:`User Guide <random_trees_embedding>`.\n",
      "id": "sklearn.ensemble.forest.RandomTreesEmbedding",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Fit estimator.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The input samples. Use ``dtype=np.float32`` for maximum efficiency. Sparse matrices are also supported, use sparse ``csc_matrix`` for maximum efficiency. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self.  '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit estimator and transform dataset.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Input data used to build forests. Use ``dtype=np.float32`` for maximum efficiency. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed dataset. '",
            "name": "X_transformed",
            "shape": "n_samples, n_out",
            "type": "sparse"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform dataset.\n",
          "id": "sklearn.ensemble.forest.RandomTreesEmbedding.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Input data to be transformed. Use ``dtype=np.float32`` for maximum efficiency. Sparse matrices are also supported, use sparse ``csr_matrix`` for maximum efficiency. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Transformed dataset. '",
            "name": "X_transformed",
            "shape": "n_samples, n_out",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.RandomTreesEmbedding",
      "parameters": [
        {
          "default": "10",
          "description": "Number of trees in the forest. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "5",
          "description": "The maximum depth of each tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` is the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` is the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether or not to return a sparse CSR matrix, as default behavior, or to return a dense array compatible with dense pipeline operators. ",
          "name": "sparse_output",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:1515",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "components extracted from the data ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldn't have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix ",
          "name": "inner_stats_",
          "type": "tuple"
        },
        {
          "description": "Number of iterations run. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "decomposition.dict_learning",
      "common_name": "Mini Batch Dictionary Learning",
      "description": "\"Mini-batch dictionary learning\n\nFinds a dictionary (a set of atoms) that can best be used to represent data\nusing a sparse code.\n\nSolves the optimization problem::\n\n(U^*,V^*) = argmin 0.5 || Y - U V ||_2^2 + alpha * || U ||_1\n(U,V)\nwith || V_k ||_2 = 1 for all  0 <= k < n_components\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n",
      "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model from data in X.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Updates the model using the data in X as a mini-batch.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features.  iter_offset: integer, optional The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Encode the data as a sparse combination of the dictionary atoms.\n\nCoding method is determined by the object parameter\n`transform_algorithm`.\n",
          "id": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Test data to be transformed, must have the same number of features as the data used to train the model. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Transformed data  '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.decomposition.dict_learning.MiniBatchDictionaryLearning",
      "parameters": [
        {
          "description": "number of dictionary elements to extract ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "sparsity controlling parameter ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "total number of iterations to perform ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. ",
          "name": "fit_algorithm",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X' ",
          "name": "transform_algorithm",
          "type": "'lasso_lars', 'lasso_cd', 'lars', 'omp',     'threshold'"
        },
        {
          "description": "Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case. ",
          "name": "transform_n_nonzero_coefs",
          "type": "int"
        },
        {
          "description": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`. ",
          "name": "transform_alpha",
          "type": "float"
        },
        {
          "description": "Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. ",
          "name": "split_sign",
          "type": "bool"
        },
        {
          "description": "number of parallel jobs to run ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "initial value of the dictionary for warm restart scenarios  verbose : degree of verbosity of the printed output ",
          "name": "dict_init",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "number of samples in each mini-batch ",
          "name": "batch_size",
          "type": "int"
        },
        {
          "description": "whether to shuffle the samples before forming batches ",
          "name": "shuffle",
          "type": "bool"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/dict_learning.pyc:1082",
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "parameter vector (w in the formulation formula) ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "independent term in decision function ",
          "name": "intercept_",
          "type": "float"
        },
        {
          "description": "the varying values of the coefficients along the path ",
          "name": "coef_path_",
          "shape": "n_features, n_alphas",
          "type": "array"
        },
        {
          "description": "the estimated regularization parameter alpha ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "the different values of alpha along the path ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "array"
        },
        {
          "description": "all the values of alpha along the path for the different folds ",
          "name": "cv_alphas_",
          "shape": "n_cv_alphas,",
          "type": "array"
        },
        {
          "description": "the mean square error on left-out for each fold along the path (alpha values given by ``cv_alphas``) ",
          "name": "cv_mse_path_",
          "shape": "n_folds, n_cv_alphas",
          "type": "array"
        },
        {
          "description": "the number of iterations run by Lars with the optimal alpha.  See also -------- lars_path, LassoLars, LassoLarsCV",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "linear_model.least_angle",
      "common_name": "Lars CV",
      "description": "\"Cross-validated Least Angle Regression model\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.LarsCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "returns an instance of self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.least_angle.LarsCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.least_angle.LarsCV",
      "parameters": [
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  max_iter: integer, optional Maximum number of iterations to perform. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The maximum number of points on the path used to compute the residuals in the cross-validation ",
          "name": "max_n_alphas",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.  ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.pyc:971",
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Maximum of covariances (in absolute value) at each iteration.         ``n_alphas`` is either ``max_iter``, ``n_features``, or the number of         nodes in the path with correlation greater than ``alpha``, whichever         is smaller. ",
          "name": "alphas_",
          "shape": "n_alphas + 1,",
          "type": "array"
        },
        {
          "description": "Indices of active variables at the end of the path. ",
          "name": "active_",
          "type": "list"
        },
        {
          "description": "If a list is passed it's expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the ``fit_path`` parameter is ``False``. ",
          "name": "coef_path_",
          "shape": "n_features, n_alphas + 1",
          "type": "array"
        },
        {
          "description": "Parameter vector (w in the formulation formula). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "The number of iterations taken by lars_path to find the grid of alphas for each target. ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "linear_model.least_angle",
      "common_name": "Lasso Lars",
      "description": "\"Lasso model fit with Least Angle Regression a.k.a. Lars\n\nIt is a Linear Model trained with an L1 prior as regularizer.\n\nThe optimization objective for Lasso is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.LassoLars",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.least_angle.LassoLars.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit the model using X, y as training data.\n\nparameters\n----------\nX : array-like, shape (n_samples, n_features)\nTraining data.\n\ny : array-like, shape (n_samples,) or (n_samples, n_targets)\nTarget values.\n\nXy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional\nXy = np.dot(X.T, y) that can be precomputed. It is useful\nonly when the Gram matrix is precomputed.\n\nreturns\n-------\nself : object\nreturns an instance of self.\n'",
          "id": "sklearn.linear_model.least_angle.LassoLars.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.least_angle.LassoLars.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.least_angle.LassoLars.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.least_angle.LassoLars.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.least_angle.LassoLars.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.least_angle.LassoLars",
      "parameters": [
        {
          "description": "Constant that multiplies the penalty term. Defaults to 1.0. ``alpha = 0`` is equivalent to an ordinary least square, solved by :class:`LinearRegression`. For numerical reasons, using ``alpha = 0`` with the LassoLars object is not advised and you should prefer the LinearRegression object. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "description": "Sets the verbosity amount ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`.  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "Maximum number of iterations to perform. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the ``tol`` parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If ``True`` the full path is stored in the ``coef_path_`` attribute. If you compute the solution for a large problem or many targets, setting ``fit_path`` to ``False`` will lead to a speedup, especially with a small alpha. ",
          "name": "fit_path",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.pyc:709",
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Best fitted model (copy of the `base_estimator` object). ",
          "name": "estimator_",
          "type": "object"
        },
        {
          "description": "Number of random selection trials until one of the stop criteria is met. It is always ``<= max_trials``. ",
          "name": "n_trials_",
          "type": "int"
        },
        {
          "description": "Boolean mask of inliers classified as ``True``. ",
          "name": "inlier_mask_",
          "shape": "n_samples",
          "type": "bool"
        }
      ],
      "category": "linear_model.ransac",
      "common_name": "RANSAC Regressor",
      "description": "'RANSAC (RANdom SAmple Consensus) algorithm.\n\nRANSAC is an iterative algorithm for the robust estimation of parameters\nfrom a subset of inliers from the complete data set. More information can\nbe found in the general documentation of linear models.\n\nA detailed description of the algorithm can be found in the documentation\nof the ``linear_model`` sub-package.\n\nRead more in the :ref:`User Guide <ransac_regression>`.\n",
      "id": "sklearn.linear_model.ransac.RANSACRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit estimator using RANSAC algorithm.\n",
          "id": "sklearn.linear_model.ransac.RANSACRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.  Raises ------ ValueError If no valid consensus set could be found. This occurs if `is_data_valid` and `is_model_valid` return False for all `max_trials` randomly chosen sub-samples.  '",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.ransac.RANSACRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the estimated model.\n\nThis is a wrapper for `estimator_.predict(X)`.\n",
          "id": "sklearn.linear_model.ransac.RANSACRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the score of the prediction.\n\nThis is a wrapper for `estimator_.score(X, y)`.\n",
          "id": "sklearn.linear_model.ransac.RANSACRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Training data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Score of the prediction. '",
            "name": "z",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.ransac.RANSACRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.ransac.RANSACRegressor",
      "parameters": [
        {
          "description": "Base estimator object which implements the following methods:  * `fit(X, y)`: Fit model to given training data and target values. * `score(X, y)`: Returns the mean accuracy on the given test data, which is used for the stop criterion defined by `stop_score`. Additionally, the score is used to decide which of two equally large consensus sets is chosen as the better one.  If `base_estimator` is None, then ``base_estimator=sklearn.linear_model.LinearRegression()`` is used for target values of dtype float.  Note that the current implementation only supports regression estimators. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "description": "Minimum number of samples chosen randomly from original data. Treated as an absolute number of samples for `min_samples >= 1`, treated as a relative number `ceil(min_samples * X.shape[0]`) for `min_samples < 1`. This is typically chosen as the minimal number of samples necessary to estimate the given `base_estimator`. By default a ``sklearn.linear_model.LinearRegression()`` estimator is assumed and `min_samples` is chosen as ``X.shape[1] + 1``. ",
          "name": "min_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum residual for a data sample to be classified as an inlier. By default the threshold is chosen as the MAD (median absolute deviation) of the target values `y`. ",
          "name": "residual_threshold",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "This function is called with the randomly selected data before the model is fitted to it: `is_data_valid(X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. ",
          "name": "is_data_valid",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "This function is called with the estimated model and the randomly selected data: `is_model_valid(model, X, y)`. If its return value is False the current randomly chosen sub-sample is skipped. Rejecting samples with this function is computationally costlier than with `is_data_valid`. `is_model_valid` should therefore only be used if the estimated model is needed for making the rejection decision. ",
          "name": "is_model_valid",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Maximum number of iterations for random sample selection. ",
          "name": "max_trials",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stop iteration if at least this number of inliers are found. ",
          "name": "stop_n_inliers",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stop iteration if score is greater equal than this threshold. ",
          "name": "stop_score",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "RANSAC iteration stops if at least one outlier-free set of the training data is sampled in RANSAC. This requires to generate at least N samples (iterations)::  N >= log(1 - probability) / log(1 - e**m)  where the probability (confidence) is typically set to high value such as 0.99 (the default) and e is the current fraction of inliers w.r.t. the total number of samples. ",
          "name": "stop_probability",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Metric to reduce the dimensionality of the residuals to 1 for multi-dimensional target values ``y.shape[1] > 1``. By default the sum of absolute differences is used::  lambda dy: np.sum(np.abs(dy), axis=1)  NOTE: residual_metric is deprecated from 0.18 and will be removed in 0.20 Use ``loss`` instead. ",
          "name": "residual_metric",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "String inputs, \"absolute_loss\" and \"squared_loss\" are supported which find the absolute loss and squared loss per sample respectively.  If ``loss`` is a callable, then it should be a function that takes two arrays as inputs, the true and predicted value and returns a 1-D array with the ``i``th value of the array corresponding to the loss on `X[i]`.  If the loss on a sample is greater than the ``residual_threshold``, then this sample is classified as an outlier. ",
          "name": "loss",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/ransac.pyc:54",
      "tags": [
        "linear_model",
        "ransac"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "Weight vector(s). ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "Independent term in decision function. Set to 0.0 if ``fit_intercept = False``. ",
          "name": "intercept_",
          "shape": "n_targets,",
          "type": "float"
        },
        {
          "description": "Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None.  See also -------- Ridge, RidgeClassifierCV ",
          "name": "n_iter_",
          "shape": "n_targets,",
          "type": "array"
        }
      ],
      "category": "linear_model.ridge",
      "common_name": "Ridge Classifier",
      "description": "'Classifier using Ridge regression.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n",
      "id": "sklearn.linear_model.ridge.RidgeClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Fit Ridge regression model.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples,n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weight.  .. versionadded:: 0.17 *sample_weight* support to Classifier. ",
              "name": "sample_weight",
              "shape": "n_samples,",
              "type": "float"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.ridge.RidgeClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.ridge.RidgeClassifier",
      "parameters": [
        {
          "description": "Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the regressors X will be normalized before regression. This parameter is ignored when `fit_intercept` is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use `preprocessing.StandardScaler` before calling `fit` on an estimator with `normalize=False`. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Solver to use in the computational routines:  - \\'auto\\' chooses the solver automatically based on the type of data.  - \\'svd\\' uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than \\'cholesky\\'.  - \\'cholesky\\' uses the standard scipy.linalg.solve function to obtain a closed-form solution.  - \\'sparse_cg\\' uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than \\'cholesky\\' for large-scale data (possibility to set `tol` and `max_iter`).  - \\'lsqr\\' uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure.  - \\'sag\\' uses a Stochastic Average Gradient descent. It also uses an iterative procedure, and is faster than other solvers when both n_samples and n_features are large.  .. versionadded:: 0.17 Stochastic Average Gradient descent solver. ",
          "name": "solver",
          "type": "\\'auto\\', \\'svd\\', \\'cholesky\\', \\'lsqr\\', \\'sparse_cg\\', \\'sag\\'"
        },
        {
          "description": "Precision of the solution. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. Used in \\'sag\\' solver. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/ridge.pyc:645",
      "tags": [
        "linear_model",
        "ridge"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "X block weights vectors. ",
          "name": "x_weights_",
          "shape": "p, n_components",
          "type": "array"
        },
        {
          "description": "Y block weights vectors. ",
          "name": "y_weights_",
          "shape": "q, n_components",
          "type": "array"
        },
        {
          "description": "X block loadings vectors. ",
          "name": "x_loadings_",
          "shape": "p, n_components",
          "type": "array"
        },
        {
          "description": "Y block loadings vectors. ",
          "name": "y_loadings_",
          "shape": "q, n_components",
          "type": "array"
        },
        {
          "description": "X scores. ",
          "name": "x_scores_",
          "shape": "n_samples, n_components",
          "type": "array"
        },
        {
          "description": "Y scores. ",
          "name": "y_scores_",
          "shape": "n_samples, n_components",
          "type": "array"
        },
        {
          "description": "X block to latents rotations. ",
          "name": "x_rotations_",
          "shape": "p, n_components",
          "type": "array"
        },
        {
          "description": "Y block to latents rotations. ",
          "name": "y_rotations_",
          "shape": "q, n_components",
          "type": "array"
        },
        {
          "description": "Number of iterations of the NIPALS inner loop for each component. Not useful if the algorithm provided is \"svd\".  Notes ----- Matrices::  T: x_scores_ U: y_scores_ W: x_weights_ C: y_weights_ P: x_loadings_ Q: y_loadings__  Are computed such that::  X = T P.T + Err and Y = U Q.T + Err T[:, k] = Xk W[:, k] for k in range(n_components) U[:, k] = Yk C[:, k] for k in range(n_components) x_rotations_ = W (P.T W)^(-1) y_rotations_ = C (Q.T C)^(-1)  where Xk and Yk are residual matrices at iteration k.  `Slides explaining PLS <http://www.eigenvector.com/Docs/Wise_pls_properties.pdf>`  For each component k, find weights u, v that optimize::  max corr(Xk u, Yk v) * std(Xk u) std(Yk u), such that ``|u| = |v| = 1``  Note that it maximizes both the correlations between the scores and the intra-block variances.  The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score.  The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling.  This implementation provides the same results that the \"plspm\" package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function ``pls(..., mode = \"canonical\")`` of the \"mixOmics\" package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one. ",
          "name": "n_iter_",
          "type": "array-like"
        }
      ],
      "category": "cross_decomposition.pls_",
      "common_name": "PLS Canonical",
      "description": "' PLSCanonical implements the 2 blocks canonical PLS of the original Wold\nalgorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].\n\nThis class inherits from PLS with mode=\"A\" and deflation_mode=\"canonical\",\nnorm_y_weights=True and algorithm=\"nipals\", but svd should provide similar\nresults up to numerical errors.\n\nRead more in the :ref:`User Guide <cross_decomposition>`.\n",
      "id": "sklearn.cross_decomposition.pls_.PLSCanonical",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit model to data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and n_features is the number of predictors.  Y : array-like of response, shape = [n_samples, n_targets] Target vectors, where n_samples in the number of samples and n_targets is the number of response variables. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Learn and apply the dimension reduction on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization.  Notes ----- This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. '",
              "name": "copy",
              "type": "boolean"
            }
          ]
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply the dimension reduction learned on the train data.\n",
          "id": "sklearn.cross_decomposition.pls_.PLSCanonical.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Training vectors, where n_samples in the number of samples and p is the number of predictors.  Y : array-like of response, shape = [n_samples, q], optional Training vectors, where n_samples in the number of samples and q is the number of response variables. ",
              "name": "X",
              "shape": "n_samples, p",
              "type": "array-like"
            },
            {
              "description": "Whether to copy X and Y, or perform in-place normalization. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "'",
            "name": "x_scores if Y is not given, (x_scores, y_scores) otherwise."
          }
        }
      ],
      "name": "sklearn.cross_decomposition.pls_.PLSCanonical",
      "parameters": [
        {
          "description": "",
          "name": "scale",
          "type": "boolean"
        },
        {
          "description": "The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop. ",
          "name": "algorithm",
          "type": "string"
        },
        {
          "description": "the maximum number of iterations of the NIPALS inner loop (used only if algorithm=\"nipals\") ",
          "name": "max_iter",
          "type": "an"
        },
        {
          "description": "the tolerance used in the iterative algorithm ",
          "name": "tol",
          "type": "non-negative"
        },
        {
          "description": "Whether the deflation should be done on a copy. Let the default value to True unless you don\\'t care about side effect ",
          "name": "copy",
          "type": "boolean"
        },
        {
          "description": "",
          "name": "n_components",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cross_decomposition/pls_.pyc:603",
      "tags": [
        "cross_decomposition",
        "pls_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification"
      ],
      "attributes": [
        {
          "description": "Estimators used for predictions. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "Class labels.",
          "name": "classes_",
          "shape": "`n_classes`",
          "type": "array"
        },
        {
          "description": "Object used to transform multiclass labels to binary labels and vice-versa.",
          "name": "label_binarizer_",
          "type": ""
        },
        {
          "description": "Whether a OneVsRestClassifier is a multilabel classifier.",
          "name": "multilabel_",
          "type": "boolean"
        }
      ],
      "category": "multiclass",
      "common_name": "One Vs Rest Classifier",
      "description": "'One-vs-the-rest (OvR) multiclass/multilabel strategy\n\nAlso known as one-vs-all, this strategy consists in fitting one classifier\nper class. For each classifier, the class is fitted against all the other\nclasses. In addition to its computational efficiency (only `n_classes`\nclassifiers are needed), one advantage of this approach is its\ninterpretability. Since each class is represented by one and one classifier\nonly, it is possible to gain knowledge about the class by inspecting its\ncorresponding classifier. This is the most commonly used strategy for\nmulticlass classification and is a fair default choice.\n\nThis strategy can also be used for multilabel learning, where a classifier\nis used to predict multiple labels for instance, by fitting on a 2-d matrix\nin which cell [i, j] is 1 if sample i has label j and 0 otherwise.\n\nIn the multilabel learning literature, OvR is also known as the binary\nrelevance method.\n\nRead more in the :ref:`User Guide <ovr_classification>`.\n",
      "id": "sklearn.multiclass.OneVsRestClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Returns the distance of each sample from the decision boundary for\neach class. This can only be used with estimators which implement the\ndecision_function method.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Fit underlying estimators.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-class targets. An indicator matrix turns on multilabel classification. ",
              "name": "y",
              "shape": "n_samples, ",
              "type": ""
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Partially fit underlying estimators\n\nShould be used when memory is inefficient to train all data.\nChunks of data can be passed in several iteration.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            },
            {
              "description": "Multi-class targets. An indicator matrix turns on multilabel classification. ",
              "name": "y",
              "shape": "n_samples, ",
              "type": ""
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is only required in the first call of partial_fit and can be omitted in the subsequent calls. ",
              "name": "classes",
              "shape": "n_classes, ",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Predict multi-class targets using underlying estimators.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": ""
            }
          ],
          "returns": {
            "description": "Predicted multi-class targets. '",
            "name": "y",
            "shape": "n_samples, ",
            "type": ""
          }
        },
        {
          "description": "'Probability estimates.\n\nThe returned estimates for all classes are ordered by label of classes.\n\nNote that in the multilabel case, each sample can have any number of\nlabels. This returns the marginal probability that the given sample has\nthe label in question. For example, it is entirely consistent that two\nlabels both have a 90% probability of applying to a given sample.\n\nIn the single label multiclass case, the rows of the returned matrix\nsum to 1.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each class in the model, where classes are ordered as they are in `self.classes_`. '",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": ""
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.multiclass.OneVsRestClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.multiclass.OneVsRestClassifier",
      "parameters": [
        {
          "description": "An estimator object implementing `fit` and one of `decision_function` or `predict_proba`. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/multiclass.pyc:133",
      "tags": [
        "multiclass"
      ],
      "task_type": [
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [
        {
          "description": "probability of each class. ",
          "name": "class_prior_",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "number of training samples observed in each class. ",
          "name": "class_count_",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "mean of each feature per class ",
          "name": "theta_",
          "shape": "n_classes, n_features",
          "type": "array"
        },
        {
          "description": "variance of each feature per class ",
          "name": "sigma_",
          "shape": "n_classes, n_features",
          "type": "array"
        }
      ],
      "category": "naive_bayes",
      "common_name": "Gaussian NB",
      "description": "'\nGaussian Naive Bayes (GaussianNB)\n\nCan perform online updates to model parameters via `partial_fit` method.\nFor details on algorithm used to update feature means and variance online,\nsee Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n\nhttp://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n\nRead more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      "id": "sklearn.naive_bayes.GaussianNB",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Gaussian Naive Bayes according to X, y\n",
          "id": "sklearn.naive_bayes.GaussianNB.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted).  .. versionadded:: 0.17 Gaussian Naive Bayes supports fitting with *sample_weight*. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.naive_bayes.GaussianNB.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Incremental fit on a batch of samples.\n\nThis method is expected to be called several times consecutively\non different chunks of a dataset so as to implement out-of-core\nor online learning.\n\nThis is especially useful when the whole dataset is too big to fit in\nmemory at once.\n\nThis method has some performance and numerical stability overhead,\nhence it is better to call partial_fit on chunks of data that are\nas large as possible (as long as fitting in the memory budget) to\nhide the overhead.\n",
          "id": "sklearn.naive_bayes.GaussianNB.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls. ",
              "name": "classes",
              "optional": "true",
              "shape": "n_classes,",
              "type": "array-like"
            },
            {
              "default": "None",
              "description": "Weights applied to individual samples (1. for unweighted).  .. versionadded:: 0.17 ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'\nPerform classification on an array of test vectors X.\n",
          "id": "sklearn.naive_bayes.GaussianNB.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn log-probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.GaussianNB.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'\nReturn probability estimates for the test vector X.\n",
          "id": "sklearn.naive_bayes.GaussianNB.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.naive_bayes.GaussianNB.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.naive_bayes.GaussianNB.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.naive_bayes.GaussianNB",
      "parameters": [
        {
          "description": "Prior probabilities of the classes. If specified the priors are not adjusted according to the data. ",
          "name": "priors",
          "shape": "n_classes,",
          "type": "array-like"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/naive_bayes.pyc:106",
      "tags": [
        "naive_bayes"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "String describing the type of covariance parameters used by the DP-GMM.  Must be one of 'spherical', 'tied', 'diag', 'full'. ",
          "name": "covariance_type",
          "type": "string"
        },
        {
          "description": "Dimensionality of the Gaussians. ",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "Number of mixture components. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Mixing weights for each mixture component. ",
          "name": "weights_",
          "shape": "`n_components`,",
          "type": "array"
        },
        {
          "description": "Mean parameters for each mixture component. ",
          "name": "means_",
          "shape": "`n_components`, `n_features`",
          "type": "array"
        },
        {
          "description": "Precision (inverse covariance) parameters for each mixture component.  The shape depends on `covariance_type`::  (`n_components`, 'n_features')                if 'spherical', (`n_features`, `n_features`)                  if 'tied', (`n_components`, `n_features`)                if 'diag', (`n_components`, `n_features`, `n_features`)  if 'full' ",
          "name": "precs_",
          "type": "array"
        },
        {
          "description": "True when convergence was reached in fit(), False otherwise.  See Also -------- GMM : Finite Gaussian mixture model fit with EM DPGMM : Infinite Gaussian mixture model, using the dirichlet process, fit with a variational algorithm",
          "name": "converged_",
          "type": "bool"
        }
      ],
      "category": "mixture.dpgmm",
      "common_name": "VBGMM",
      "description": "\"Variational Inference for the Gaussian Mixture Model\n\n.. deprecated:: 0.18\nThis class will be removed in 0.20.\nUse :class:`sklearn.mixture.BayesianGaussianMixture` with parameter\n``weight_concentration_prior_type='dirichlet_distribution'`` instead.\n\nVariational inference for a Gaussian mixture model probability\ndistribution. This class allows for easy and efficient inference\nof an approximate posterior distribution over the parameters of a\nGaussian mixture model with a fixed number of components.\n\nInitialization is with normally-distributed means and identity\ncovariance, for proper convergence.\n\nRead more in the :ref:`User Guide <vbgmm>`.\n",
      "id": "sklearn.mixture.dpgmm.VBGMM",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Akaike information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.aic",
          "name": "aic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "aic: float (the lower the better)"
          }
        },
        {
          "description": "'Bayesian information criterion for the current model fit\nand the proposed data.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.bic",
          "name": "bic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "'",
            "name": "bic: float (the lower the better)"
          }
        },
        {
          "description": "\"Estimate model parameters with the EM algorithm.\n\nA initialization step is performed before entering the\nexpectation-maximization (EM) algorithm. If you want to avoid\nthis step, set the keyword argument init_params to the empty\nstring '' when creating the GMM object. Likewise, if you would\nlike just to do an initialization, set n_iter=0.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Fit and then predict labels for data.\n\nWarning: Due to the final maximization step in the EM algorithm,\nwith low iterations the prediction may not be 100%  accurate.\n\n.. versionadded:: 0.17\n*fit_predict* method in Gaussian Mixture Model.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.fit_predict",
          "name": "fit_predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'returns a lower bound on model evidence based on X and membership'",
          "id": "sklearn.mixture.dpgmm.VBGMM.lower_bound",
          "name": "lower_bound",
          "parameters": []
        },
        {
          "description": "'Predict label for data.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data under each Gaussian\nin the model.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "responsibilities",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Generate random samples from the model.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "List of samples '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the log probability under the model.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Return the likelihood of the data under the model.\n\nCompute the bound on log probability of X under the model\nand return the posterior distribution (responsibilities) of\neach mixture component for each element of X.\n\nThis is done by computing the parameters for the mean-field of\nz for each observation.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X responsibilities : array_like, shape (n_samples, n_components) Posterior probabilities of each mixture component for each observation '",
            "name": "logprob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.dpgmm.VBGMM.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.dpgmm.VBGMM",
      "parameters": [
        {
          "description": "Number of mixture components.  covariance_type: string, default 'diag' String describing the type of covariance parameters to use.  Must be one of 'spherical', 'tied', 'diag', 'full'.  alpha: float, default 1 Real number representing the concentration parameter of the dirichlet distribution. Intuitively, the higher the value of alpha the more likely the variational mixture of Gaussians model will use all components it can. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Convergence threshold. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations to perform before convergence. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "Controls which parameters are updated in the training process.  Can contain any combination of 'w' for weights, 'm' for means, and 'c' for covars. ",
          "name": "params",
          "type": "string"
        },
        {
          "description": "Controls which parameters are updated in the initialization process.  Can contain any combination of 'w' for weights, 'm' for means, and 'c' for covars.  Defaults to 'wmc'. ",
          "name": "init_params",
          "type": "string"
        },
        {
          "description": "Controls output verbosity. ",
          "name": "verbose",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/dpgmm.pyc:657",
      "tags": [
        "mixture",
        "dpgmm"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Coordinates of cluster centers  labels_ : Labels of each point ",
          "name": "cluster_centers_",
          "type": "array"
        },
        {
          "description": "Sum of distances of samples to their closest cluster center. ",
          "name": "inertia_",
          "type": "float"
        }
      ],
      "category": "cluster.k_means_",
      "common_name": "K Means",
      "description": "'K-Means clustering\n\nRead more in the :ref:`User Guide <k_means>`.\n",
      "id": "sklearn.cluster.k_means_.KMeans",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute k-means clustering.\n",
          "id": "sklearn.cluster.k_means_.KMeans.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training instances to cluster. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Compute cluster centers and predict cluster index for each sample.\n\nConvenience method; equivalent to calling fit(X) followed by\npredict(X).\n'",
          "id": "sklearn.cluster.k_means_.KMeans.fit_predict",
          "name": "fit_predict",
          "parameters": []
        },
        {
          "description": "'Compute clustering and transform X to cluster-distance space.\n\nEquivalent to fit(X).transform(X), but more efficiently implemented.\n'",
          "id": "sklearn.cluster.k_means_.KMeans.fit_transform",
          "name": "fit_transform",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.k_means_.KMeans.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the closest cluster each sample in X belongs to.\n\nIn the vector quantization literature, `cluster_centers_` is called\nthe code book and each value returned by `predict` is the index of\nthe closest code in the code book.\n",
          "id": "sklearn.cluster.k_means_.KMeans.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "New data to predict. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Index of the cluster each sample belongs to. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Opposite of the value of X on the K-means objective.\n",
          "id": "sklearn.cluster.k_means_.KMeans.score",
          "name": "score",
          "parameters": [
            {
              "description": "New data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Opposite of the value of X on the K-means objective. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.k_means_.KMeans.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Transform X to a cluster-distance space.\n\nIn the new space, each dimension is the distance to the cluster\ncenters.  Note that even if X is sparse, the array returned by\n`transform` will typically be dense.\n",
          "id": "sklearn.cluster.k_means_.KMeans.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data to transform. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "X transformed in the new space. '",
            "name": "X_new",
            "shape": "n_samples, k",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.cluster.k_means_.KMeans",
      "parameters": [
        {
          "description": "The number of clusters to form as well as the number of centroids to generate. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations of the k-means algorithm for a single run. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. ",
          "name": "n_init",
          "type": "int"
        },
        {
          "description": "Method for initialization, defaults to \\'k-means++\\':  \\'k-means++\\' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.  \\'random\\': choose k observations (rows) at random from data for the initial centroids.  If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. ",
          "name": "init",
          "type": "\\'k-means++\\', \\'random\\' or an ndarray"
        },
        {
          "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn\\'t support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. ",
          "name": "algorithm",
          "type": ""
        },
        {
          "description": "Precompute distances (faster but takes more memory).  \\'auto\\' : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision.  True : always precompute distances  False : never precompute distances ",
          "name": "precompute_distances",
          "type": "\\'auto\\', True, False"
        },
        {
          "description": "Relative tolerance with regards to inertia to declare convergence ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Verbosity mode. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. ",
          "name": "copy_x",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/k_means_.pyc:704",
      "tags": [
        "cluster",
        "k_means_"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Independent term in decision function. ",
          "name": "intercept_",
          "shape": "n_tasks,",
          "type": "array"
        },
        {
          "description": "Parameter vector (W in the cost function formula). ",
          "name": "coef_",
          "shape": "n_tasks, n_features",
          "type": "array"
        },
        {
          "description": "The amount of penalization chosen by cross validation ",
          "name": "alpha_",
          "type": "float"
        },
        {
          "description": "mean square error for the test set on each fold, varying alpha ",
          "name": "mse_path_",
          "shape": "n_alphas, n_folds",
          "type": "array"
        },
        {
          "description": "The grid of alphas used for fitting, for each l1_ratio  l1_ratio_ : float best l1_ratio obtained by cross-validation. ",
          "name": "alphas_",
          "shape": "n_alphas,",
          "type": "numpy"
        },
        {
          "description": "number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha. ",
          "name": "n_iter_",
          "type": "int"
        }
      ],
      "category": "linear_model.coordinate_descent",
      "common_name": "Multi Task Elastic Net CV",
      "description": "\"Multi-task L1/L2 ElasticNet with built-in cross-validation.\n\nThe optimization objective for MultiTaskElasticNet is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n+ alpha * l1_ratio * ||W||_21\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <multi_task_lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nDecision function of the linear model.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Fit linear model with coordinate descent\n\nFit is on grid of alphas and best alpha estimated by cross-validation.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. Pass directly as float64, Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values '",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Returns predicted values. '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.linear_model.coordinate_descent.MultiTaskElasticNetCV",
      "parameters": [
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3``. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "List of alphas where to compute the models. If not provided, set automatically. ",
          "name": "alphas",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Number of alphas along the regularization path  l1_ratio : float or array of floats The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 0 the penalty is an L1/L2 penalty. For l1_ratio = 1 it is an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7, .9, .95, .99, 1]`` ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). ",
          "name": "fit_intercept",
          "type": "boolean"
        },
        {
          "description": "If ``True``, the regressors X will be normalized before regression. This parameter is ignored when ``fit_intercept`` is set to ``False``. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use :class:`preprocessing.StandardScaler` before calling ``fit`` on an estimator with ``normalize=False``.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The maximum number of iterations ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The tolerance for the optimization: if the updates are smaller than ``tol``, the optimization code checks the dual gap for optimality and continues until it is smaller than ``tol``. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "Number of CPUs to use during the cross validation. If ``-1``, use all the CPUs. Note that this is used only if multiple values for l1_ratio are given. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "If set to 'random', a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to 'random') often leads to significantly faster convergence especially when tol is higher than 1e-4. ",
          "name": "selection",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator that selects a random feature to update. Useful only when selection is set to 'random'. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.pyc:1861",
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Contains scores for all parameter combinations in param_grid. Each entry corresponds to one parameter setting. Each named tuple has the attributes:  * ``parameters``, a dict of parameter settings * ``mean_validation_score``, the mean score over the cross-validation folds * ``cv_validation_scores``, the list of scores for each fold ",
          "name": "grid_scores_",
          "type": "list"
        },
        {
          "description": "Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False. ",
          "name": "best_estimator_",
          "type": "estimator"
        },
        {
          "description": "Score of best_estimator on the left out data. ",
          "name": "best_score_",
          "type": "float"
        },
        {
          "description": "Parameter setting that gave the best results on the hold out data. ",
          "name": "best_params_",
          "type": "dict"
        }
      ],
      "category": "grid_search",
      "common_name": "Randomized Search CV",
      "description": "'Randomized search on hyper parameters.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :class:`sklearn.model_selection.RandomizedSearchCV` instead.\n\nRandomizedSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"predict\", \"predict_proba\", \"decision_function\",\n\"transform\" and \"inverse_transform\" if they are implemented in the\nestimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but\nrather a fixed number of parameter settings is sampled from the specified\ndistributions. The number of parameter settings that are tried is\ngiven by n_iter.\n\nIf all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nRead more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "id": "sklearn.grid_search.RandomizedSearchCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run fit on the estimator with randomly drawn parameters.\n",
          "id": "sklearn.grid_search.RandomizedSearchCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning.  '",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.grid_search.RandomizedSearchCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n",
          "id": "sklearn.grid_search.RandomizedSearchCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Input data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- * The long-standing behavior of this method changed in version 0.16. * It no longer uses the metric provided by ``estimator.score`` if the ``scoring`` parameter was set when fitting.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.grid_search.RandomizedSearchCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.grid_search.RandomizedSearchCV",
      "parameters": [
        {
          "description": "A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. Distributions must provide a ``rvs`` method for sampling (such as those from scipy.stats.distributions). If a list is given, it is sampled uniformly. ",
          "name": "param_distributions",
          "type": "dict"
        },
        {
          "description": "Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If ``None``, the ``score`` method of the estimator is used. ",
          "name": "scoring",
          "type": "string"
        },
        {
          "description": "Parameters to pass to the fit method. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Number of jobs to run in parallel. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in \\'2*n_jobs\\' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. ",
          "name": "iid",
          "type": "boolean"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Refit the best estimator with the entire dataset. If \"False\", it is impossible to make predictions using this RandomizedSearchCV instance after fitting. ",
          "name": "refit",
          "type": "boolean"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "type": "integer"
        },
        {
          "description": "Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to \\'raise\\', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.  ",
          "name": "error_score",
          "type": ""
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc:832",
      "tags": [
        "grid_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The number of selected features with cross-validation. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The mask of selected features. ",
          "name": "support_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The feature ranking, such that `ranking_[i]` corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1. ",
          "name": "ranking_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The cross-validation scores such that ``grid_scores_[i]`` corresponds to the CV score of the i-th subset of features. ",
          "name": "grid_scores_",
          "shape": "n_subsets_of_features",
          "type": "array"
        },
        {
          "description": "The external estimator fit on the reduced dataset.  Notes ----- The size of ``grid_scores_`` is equal to ceil((n_features - 1) / step) + 1, where step is the number of features removed at each iteration. ",
          "name": "estimator_",
          "type": "object"
        }
      ],
      "category": "feature_selection.rfe",
      "common_name": "RFECV",
      "description": "'Feature ranking with recursive feature elimination and cross-validated\nselection of the best number of features.\n\nRead more in the :ref:`User Guide <rfe>`.\n",
      "id": "sklearn.feature_selection.rfe.RFECV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the RFE model and automatically tune the number of selected\nfeatures.\n",
          "id": "sklearn.feature_selection.rfe.RFECV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where `n_samples` is the number of samples and `n_features` is the total number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values (integers for classification, real numbers for regression). '",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.feature_selection.rfe.RFECV.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_selection.rfe.RFECV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nGet a mask, or integer index, of the features selected\n",
          "id": "sklearn.feature_selection.rfe.RFECV.get_support",
          "name": "get_support",
          "parameters": [
            {
              "description": "If True, the return value will be an array of integers, rather than a boolean mask. ",
              "name": "indices",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "An index that selects the retained features from a feature vector. If `indices` is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If `indices` is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector. '",
            "name": "support",
            "type": "array"
          }
        },
        {
          "description": "'\nReverse the transformation operation\n",
          "id": "sklearn.feature_selection.rfe.RFECV.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_selected_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "`X` with columns of zeros inserted where features would have been removed by `transform`. '",
            "name": "X_r",
            "shape": "n_samples, n_original_features",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_selection.rfe.RFECV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Reduce X to the selected features.\n",
          "id": "sklearn.feature_selection.rfe.RFECV.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.feature_selection.rfe.RFECV",
      "parameters": [
        {
          "description": "A supervised learning estimator with a `fit` method that updates a `coef_` attribute that holds the fitted parameters. Important features must correspond to high absolute values in the `coef_` array.  For instance, this is the case for most supervised learning algorithms such as Support Vector Classifiers and Generalized Linear Models from the `svm` and `linear_model` modules. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "default": "1",
          "description": "If greater than or equal to 1, then `step` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then `step` corresponds to the percentage (rounded down) of features to remove at each iteration. ",
          "name": "step",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if ``y`` is binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. If the estimator is a classifier or if ``y`` is neither binary nor multiclass, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Controls verbosity of output. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Number of cores to run in parallel while fitting across folds. Defaults to 1 core. If `n_jobs=-1`, then number of jobs is set to number of cores. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_selection/rfe.pyc:263",
      "tags": [
        "feature_selection",
        "rfe"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [],
      "category": "neighbors.regression",
      "common_name": "Radius Neighbors Regressor",
      "description": "\"Regression based on neighbors within a fixed radius.\n\nThe target is predicted by local interpolation of the targets\nassociated of the nearest neighbors in the training set.\n\nRead more in the :ref:`User Guide <regression>`.\n",
      "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values, array of float values, shape = [n_samples] or [n_samples, n_outputs] \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict the target for the provided data\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Target values \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "\"Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.radius_neighbors",
          "name": "radius_neighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "type": "array-like"
            },
            {
              "description": "Limiting distance of neighbors to return. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the ``metric`` constructor parameter.  ind : array, shape (n_samples,) of arrays An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size ``radius`` around the query points.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1, 1, 1]:  >>> import numpy as np >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.6) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> rng = neigh.radius_neighbors([[1., 1., 1.]]) >>> print(np.asarray(rng[0][0])) # doctest: +ELLIPSIS [ 1.5  0.5] >>> print(np.asarray(rng[1][0])) # doctest: +ELLIPSIS [1 2]  The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.  Notes ----- Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, `radius_neighbors` returns arrays of objects, where each object is a 1D array of indices or distances. \"",
            "name": "dist",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.radius_neighbors_graph",
          "name": "radius_neighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Radius of neighborhoods. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.5) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.radius_neighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples",
            "type": "sparse"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.regression.RadiusNeighborsRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.regression.RadiusNeighborsRegressor",
      "parameters": [
        {
          "description": "Range of parameter space to use by default for :meth:`radius_neighbors` queries. ",
          "name": "radius",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  Uniform weights are used by default. ",
          "name": "weights",
          "type": "str"
        },
        {
          "description": "Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDtree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ",
          "name": "algorithm",
          "optional": "true",
          "type": "'auto', 'ball_tree', 'kd_tree', 'brute'"
        },
        {
          "description": "Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ",
          "name": "p",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Additional keyword arguments for the metric function. ",
          "name": "metric_params",
          "optional": "true",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/regression.pyc:168",
      "tags": [
        "neighbors",
        "regression"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "neural network"
      ],
      "attributes": [
        {
          "description": "Class labels for each output.  `loss_` : float The current loss computed with the loss function.  `coefs_` : list, length n_layers - 1 The ith element in the list represents the weight matrix corresponding to layer i.  `intercepts_` : list, length n_layers - 1 The ith element in the list represents the bias vector corresponding to layer i + 1. ",
          "name": "`classes_`",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "The number of iterations the solver has ran. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "Number of layers.  `n_outputs_` : int Number of outputs.  `out_activation_` : string Name of the output activation function. ",
          "name": "n_layers_",
          "type": "int"
        }
      ],
      "category": "neural_network.multilayer_perceptron",
      "common_name": "MLP Classifier",
      "description": "'Multi-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic\ngradient descent.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model to data matrix X and target y.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the multi-layer perceptron classifier\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples,",
            "type": "array-like"
          }
        },
        {
          "description": "'Return the log of probability estimates.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in `self.classes_`. Equivalent to log(predict_proba(X)) '",
            "name": "log_y_prob",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Probability estimates.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted probability of the sample for each class in the model, where classes are ordered as they are in `self.classes_`. '",
            "name": "y_prob",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neural_network.multilayer_perceptron.MLPClassifier",
      "parameters": [
        {
          "description": "The ith element represents the number of neurons in the ith hidden layer. ",
          "name": "hidden_layer_sizes",
          "type": "tuple"
        },
        {
          "description": "Activation function for the hidden layer.  - \\'identity\\', no-op activation, useful to implement linear bottleneck, returns f(x) = x  - \\'logistic\\', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).  - \\'tanh\\', the hyperbolic tan function, returns f(x) = tanh(x).  - \\'relu\\', the rectified linear unit function, returns f(x) = max(0, x) ",
          "name": "activation",
          "type": "\\'identity\\', \\'logistic\\', \\'tanh\\', \\'relu\\'"
        },
        {
          "description": "The solver for weight optimization.  - \\'lbfgs\\' is an optimizer in the family of quasi-Newton methods.  - \\'sgd\\' refers to stochastic gradient descent.  - \\'adam\\' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba  Note: The default solver \\'adam\\' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, \\'lbfgs\\' can converge faster and perform better. ",
          "name": "solver",
          "type": "\\'lbfgs\\', \\'sgd\\', \\'adam\\'"
        },
        {
          "description": "L2 penalty (regularization term) parameter. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Size of minibatches for stochastic optimizers. If the solver is \\'lbfgs\\', the classifier will not use minibatch. When set to \"auto\", `batch_size=min(200, n_samples)` ",
          "name": "batch_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Learning rate schedule for weight updates.  - \\'constant\\' is a constant learning rate given by \\'learning_rate_init\\'.  - \\'invscaling\\' gradually decreases the learning rate ``learning_rate_`` at each time step \\'t\\' using an inverse scaling exponent of \\'power_t\\'. effective_learning_rate = learning_rate_init / pow(t, power_t)  - \\'adaptive\\' keeps the learning rate constant to \\'learning_rate_init\\' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if \\'early_stopping\\' is on, the current learning rate is divided by 5.  Only used when ``solver=\\'sgd\\'``. ",
          "name": "learning_rate",
          "type": "\\'constant\\', \\'invscaling\\', \\'adaptive\\'"
        },
        {
          "description": "Maximum number of iterations. The solver iterates until convergence (determined by \\'tol\\') or this number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "State or seed for random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to shuffle samples in each iteration. Only used when solver=\\'sgd\\' or \\'adam\\'. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless `learning_rate` is set to \\'adaptive\\', convergence is considered to be reached and training stops. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=\\'sgd\\' or \\'adam\\'. ",
          "name": "learning_rate_init",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to \\'invscaling\\'. Only used when solver=\\'sgd\\'. ",
          "name": "power_t",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "Whether to print progress messages to stdout. ",
          "name": "verbose",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=\\'sgd\\'. ",
          "name": "momentum",
          "type": "float"
        },
        {
          "description": "Whether to use Nesterov\\'s momentum. Only used when solver=\\'sgd\\' and momentum > 0. ",
          "name": "nesterovs_momentum",
          "type": "boolean"
        },
        {
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=\\'sgd\\' or \\'adam\\' ",
          "name": "early_stopping",
          "type": "bool"
        },
        {
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True  beta_1 : float, optional, default 0.9 Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=\\'adam\\'  beta_2 : float, optional, default 0.999 Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=\\'adam\\' ",
          "name": "validation_fraction",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Value for numerical stability in adam. Only used when solver=\\'adam\\' ",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc:682",
      "tags": [
        "neural_network",
        "multilayer_perceptron"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The current loss computed with the loss function.  `coefs_` : list, length n_layers - 1 The ith element in the list represents the weight matrix corresponding to layer i.  `intercepts_` : list, length n_layers - 1 The ith element in the list represents the bias vector corresponding to layer i + 1. ",
          "name": "`loss_`",
          "type": "float"
        },
        {
          "description": "The number of iterations the solver has ran. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "Number of layers.  `n_outputs_` : int Number of outputs.  `out_activation_` : string Name of the output activation function. ",
          "name": "n_layers_",
          "type": "int"
        }
      ],
      "category": "neural_network.multilayer_perceptron",
      "common_name": "MLP Regressor",
      "description": "'Multi-layer Perceptron regressor.\n\nThis model optimizes the squared-loss using LBFGS or stochastic gradient\ndescent.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model to data matrix X and target y.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict using the multi-layer perceptron model.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples, n_outputs",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neural_network.multilayer_perceptron.MLPRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neural_network.multilayer_perceptron.MLPRegressor",
      "parameters": [
        {
          "description": "The ith element represents the number of neurons in the ith hidden layer. ",
          "name": "hidden_layer_sizes",
          "type": "tuple"
        },
        {
          "description": "Activation function for the hidden layer.  - \\'identity\\', no-op activation, useful to implement linear bottleneck, returns f(x) = x  - \\'logistic\\', the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).  - \\'tanh\\', the hyperbolic tan function, returns f(x) = tanh(x).  - \\'relu\\', the rectified linear unit function, returns f(x) = max(0, x) ",
          "name": "activation",
          "type": "\\'identity\\', \\'logistic\\', \\'tanh\\', \\'relu\\'"
        },
        {
          "description": "The solver for weight optimization.  - \\'lbfgs\\' is an optimizer in the family of quasi-Newton methods.  - \\'sgd\\' refers to stochastic gradient descent.  - \\'adam\\' refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba  Note: The default solver \\'adam\\' works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, \\'lbfgs\\' can converge faster and perform better. ",
          "name": "solver",
          "type": "\\'lbfgs\\', \\'sgd\\', \\'adam\\'"
        },
        {
          "description": "L2 penalty (regularization term) parameter. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Size of minibatches for stochastic optimizers. If the solver is \\'lbfgs\\', the classifier will not use minibatch. When set to \"auto\", `batch_size=min(200, n_samples)` ",
          "name": "batch_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Learning rate schedule for weight updates.  - \\'constant\\' is a constant learning rate given by \\'learning_rate_init\\'.  - \\'invscaling\\' gradually decreases the learning rate ``learning_rate_`` at each time step \\'t\\' using an inverse scaling exponent of \\'power_t\\'. effective_learning_rate = learning_rate_init / pow(t, power_t)  - \\'adaptive\\' keeps the learning rate constant to \\'learning_rate_init\\' as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if \\'early_stopping\\' is on, the current learning rate is divided by 5.  Only used when solver=\\'sgd\\'. ",
          "name": "learning_rate",
          "type": "\\'constant\\', \\'invscaling\\', \\'adaptive\\'"
        },
        {
          "description": "Maximum number of iterations. The solver iterates until convergence (determined by \\'tol\\') or this number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "State or seed for random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to shuffle samples in each iteration. Only used when solver=\\'sgd\\' or \\'adam\\'. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless `learning_rate` is set to \\'adaptive\\', convergence is considered to be reached and training stops. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=\\'sgd\\' or \\'adam\\'. ",
          "name": "learning_rate_init",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to \\'invscaling\\'. Only used when solver=\\'sgd\\'. ",
          "name": "power_t",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "Whether to print progress messages to stdout. ",
          "name": "verbose",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Momentum for gradient descent update.  Should be between 0 and 1. Only used when solver=\\'sgd\\'. ",
          "name": "momentum",
          "type": "float"
        },
        {
          "description": "Whether to use Nesterov\\'s momentum. Only used when solver=\\'sgd\\' and momentum > 0. ",
          "name": "nesterovs_momentum",
          "type": "boolean"
        },
        {
          "description": "Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=\\'sgd\\' or \\'adam\\' ",
          "name": "early_stopping",
          "type": "bool"
        },
        {
          "description": "The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True  beta_1 : float, optional, default 0.9 Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=\\'adam\\'  beta_2 : float, optional, default 0.999 Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=\\'adam\\' ",
          "name": "validation_fraction",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Value for numerical stability in adam. Only used when solver=\\'adam\\' ",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.pyc:1027",
      "tags": [
        "neural_network",
        "multilayer_perceptron"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "A dict with keys as column headers and values as columns, that can be imported into a pandas ``DataFrame``.  For instance the below given table  +--------------+-------------+-------------------+---+---------------+ | param_kernel | param_gamma | split0_test_score |...|rank_test_score| +==============+=============+===================+===+===============+ |    \\'rbf\\'     |     0.1     |        0.8        |...|       2       | +--------------+-------------+-------------------+---+---------------+ |    \\'rbf\\'     |     0.2     |        0.9        |...|       1       | +--------------+-------------+-------------------+---+---------------+ |    \\'rbf\\'     |     0.3     |        0.7        |...|       1       | +--------------+-------------+-------------------+---+---------------+  will be represented by a ``cv_results_`` dict of::  { \\'param_kernel\\' : masked_array(data = [\\'rbf\\', \\'rbf\\', \\'rbf\\'], mask = False), \\'param_gamma\\'  : masked_array(data = [0.1 0.2 0.3], mask = False), \\'split0_test_score\\'  : [0.8, 0.9, 0.7], \\'split1_test_score\\'  : [0.82, 0.5, 0.7], \\'mean_test_score\\'    : [0.81, 0.7, 0.7], \\'std_test_score\\'     : [0.02, 0.2, 0.], \\'rank_test_score\\'    : [3, 1, 1], \\'split0_train_score\\' : [0.8, 0.9, 0.7], \\'split1_train_score\\' : [0.82, 0.5, 0.7], \\'mean_train_score\\'   : [0.81, 0.7, 0.7], \\'std_train_score\\'    : [0.03, 0.03, 0.04], \\'mean_fit_time\\'      : [0.73, 0.63, 0.43, 0.49], \\'std_fit_time\\'       : [0.01, 0.02, 0.01, 0.01], \\'mean_score_time\\'    : [0.007, 0.06, 0.04, 0.04], \\'std_score_time\\'     : [0.001, 0.002, 0.003, 0.005], \\'params\\' : [{\\'kernel\\' : \\'rbf\\', \\'gamma\\' : 0.1}, ...], }  NOTE that the key ``\\'params\\'`` is used to store a list of parameter settings dict for all the parameter candidates.  The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and ``std_score_time`` are all in seconds. ",
          "name": "cv_results_",
          "type": "dict"
        },
        {
          "description": "Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False. ",
          "name": "best_estimator_",
          "type": "estimator"
        },
        {
          "description": "Score of best_estimator on the left out data. ",
          "name": "best_score_",
          "type": "float"
        },
        {
          "description": "Parameter setting that gave the best results on the hold out data. ",
          "name": "best_params_",
          "type": "dict"
        },
        {
          "description": "The index (of the ``cv_results_`` arrays) which corresponds to the best candidate parameter setting.  The dict at ``search.cv_results_[\\'params\\'][search.best_index_]`` gives the parameter setting for the best model, that gives the highest mean score (``search.best_score_``). ",
          "name": "best_index_",
          "type": "int"
        },
        {
          "description": "Scorer function used on the held out data to choose the best parameters for the model. ",
          "name": "scorer_",
          "type": "function"
        },
        {
          "description": "The number of cross-validation splits (folds/iterations). ",
          "name": "n_splits_",
          "type": "int"
        }
      ],
      "category": "model_selection._search",
      "common_name": "Randomized Search CV",
      "description": "'Randomized search on hyper parameters.\n\nRandomizedSearchCV implements a \"fit\" and a \"score\" method.\nIt also implements \"predict\", \"predict_proba\", \"decision_function\",\n\"transform\" and \"inverse_transform\" if they are implemented in the\nestimator used.\n\nThe parameters of the estimator used to apply these methods are optimized\nby cross-validated search over parameter settings.\n\nIn contrast to GridSearchCV, not all parameter values are tried out, but\nrather a fixed number of parameter settings is sampled from the specified\ndistributions. The number of parameter settings that are tried is\ngiven by n_iter.\n\nIf all parameters are presented as a list,\nsampling without replacement is performed. If at least one parameter\nis given as a distribution, sampling with replacement is used.\nIt is highly recommended to use continuous distributions for continuous\nparameters.\n\nRead more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "id": "sklearn.model_selection._search.RandomizedSearchCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Run fit on the estimator with randomly drawn parameters.\n",
          "id": "sklearn.model_selection._search.RandomizedSearchCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Group labels for the samples used while splitting the dataset into train/test set. '",
              "name": "groups",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.model_selection._search.RandomizedSearchCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns the score on the given data, if the estimator has been refit.\n\nThis uses the score defined by ``scoring`` where provided, and the\n``best_estimator_.score`` method otherwise.\n",
          "id": "sklearn.model_selection._search.RandomizedSearchCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Input data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
              "name": "y",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.model_selection._search.RandomizedSearchCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.model_selection._search.RandomizedSearchCV",
      "parameters": [
        {
          "description": "A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a ``score`` function, or ``scoring`` must be passed. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. Distributions must provide a ``rvs`` method for sampling (such as those from scipy.stats.distributions). If a list is given, it is sampled uniformly. ",
          "name": "param_distributions",
          "type": "dict"
        },
        {
          "description": "Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. If ``None``, the ``score`` method of the estimator is used. ",
          "name": "scoring",
          "type": "string"
        },
        {
          "description": "Parameters to pass to the fit method. ",
          "name": "fit_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Number of jobs to run in parallel. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be:  - None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs  - An int, giving the exact number of total jobs that are spawned  - A string, giving an expression as a function of n_jobs, as in \\'2*n_jobs\\' ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. ",
          "name": "iid",
          "type": "boolean"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Refit the best estimator with the entire dataset. If \"False\", it is impossible to make predictions using this RandomizedSearchCV instance after fitting. ",
          "name": "refit",
          "type": "boolean"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "type": "integer"
        },
        {
          "description": "Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to \\'raise\\', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. ",
          "name": "error_score",
          "type": ""
        },
        {
          "description": "If ``\\'False\\'``, the ``cv_results_`` attribute will not include training scores. ",
          "name": "return_train_score",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc:948",
      "tags": [
        "model_selection",
        "_search"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "clustering",
        "decision tree"
      ],
      "attributes": [
        {
          "description": "cluster labels for each feature. ",
          "name": "labels_",
          "type": "array-like"
        },
        {
          "description": "Number of leaves in the hierarchical tree. ",
          "name": "n_leaves_",
          "type": "int"
        },
        {
          "description": "The estimated number of connected components in the graph. ",
          "name": "n_components_",
          "type": "int"
        },
        {
          "description": "The children of each non-leaf node. Values less than `n_features` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_features` is a non-leaf node and has children `children_[i - n_features]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_features + i`",
          "name": "children_",
          "shape": "n_nodes-1, 2",
          "type": "array-like"
        }
      ],
      "category": "cluster.hierarchical",
      "common_name": "Feature Agglomeration",
      "description": "'Agglomerate features.\n\nSimilar to AgglomerativeClustering, but recursively merges features\ninstead of samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "id": "sklearn.cluster.hierarchical.FeatureAgglomeration",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised",
        "unsupervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the hierarchical clustering on the data\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nInverse the transformation.\nReturn a vector of size nb_features with the values of Xred assigned\nto each group of features\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "The values to be assigned to each cluster of samples ",
              "name": "Xred",
              "shape": "n_samples, n_clusters",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "A vector of size n_samples with the values of Xred assigned to each of the cluster of samples. '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'\nCompute the arithmetic mean along the specified axis.\n\nReturns the average of the array elements.  The average is taken over\nthe flattened array by default, otherwise over the specified axis.\n`float64` intermediate and return values are used for integer inputs.\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.mean",
          "name": "mean",
          "parameters": [
            {
              "description": "Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.  .. versionadded: 1.7.0  If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.",
              "name": "axis",
              "optional": "true",
              "type": ""
            },
            {
              "description": "Type to use in computing the mean.  For integer inputs, the default is `float64`; for floating point inputs, it is the same as the input dtype.",
              "name": "dtype",
              "optional": "true",
              "type": "data-type"
            },
            {
              "description": "Alternate output array in which to place the result.  The default is ``None``; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See `doc.ufuncs` for details.",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            },
            {
              "description": "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. ",
              "name": "keepdims",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned.  See Also -------- average : Weighted average std, var, nanmean, nanstd, nanvar  Notes ----- The arithmetic mean is the sum of the elements along the axis divided by the number of elements.  Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-precision accumulator using the `dtype` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.mean(a) 2.5 >>> np.mean(a, axis=0) array([ 2.,  3.]) >>> np.mean(a, axis=1) array([ 1.5,  3.5])  In single precision, `mean` can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.mean(a) 0.546875  Computing the mean in float64 is more accurate:  >>> np.mean(a, dtype=np.float64) 0.55000000074505806  '",
            "name": "m",
            "type": "ndarray"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'\nTransform a new matrix using the built clustering\n",
          "id": "sklearn.cluster.hierarchical.FeatureAgglomeration.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "A M by N array of M observations in N dimensions or a length M array of M one-dimensional observations. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The pooled values for each feature cluster. '",
            "name": "Y",
            "shape": "n_samples, n_clusters",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.cluster.hierarchical.FeatureAgglomeration",
      "parameters": [
        {
          "description": "The number of clusters to find. ",
          "name": "n_clusters",
          "type": "int"
        },
        {
          "description": "Connectivity matrix. Defines for each feature the neighboring features following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. ",
          "name": "connectivity",
          "optional": "true",
          "type": "array-like"
        },
        {
          "description": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\", \"manhattan\", \"cosine\", or \\'precomputed\\'. If linkage is \"ward\", only \"euclidean\" is accepted. ",
          "name": "affinity",
          "type": "string"
        },
        {
          "description": "Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. ",
          "name": "memory",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. ",
          "name": "compute_full_tree",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion.  - ward minimizes the variance of the clusters being merged. - average uses the average of the distances of each feature of the two sets. - complete or maximum linkage uses the maximum distances between all features of the two sets. ",
          "name": "linkage",
          "optional": "true",
          "type": "\"ward\", \"complete\", \"average\""
        },
        {
          "description": "This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument `axis=1`, and reduce it to an array of size [M]. ",
          "name": "pooling_func",
          "type": "callable"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/cluster/hierarchical.pyc:748",
      "tags": [
        "cluster",
        "hierarchical"
      ],
      "task_type": [
        "modeling",
        "data preprocessing"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble",
        "classification"
      ],
      "attributes": [
        {
          "description": "The base estimator from which the ensemble is grown. ",
          "name": "base_estimator_",
          "type": "estimator"
        },
        {
          "description": "The collection of fitted base estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The subset of drawn samples (i.e., the in-bag samples) for each base estimator. Each subset is defined by a boolean mask. ",
          "name": "estimators_samples_",
          "type": "list"
        },
        {
          "description": "The subset of drawn features for each base estimator. ",
          "name": "estimators_features_",
          "type": "list"
        },
        {
          "description": "The classes labels. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "The number of classes. ",
          "name": "n_classes_",
          "type": "int"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. ",
          "name": "oob_decision_function_",
          "shape": "n_samples, n_classes",
          "type": "array"
        }
      ],
      "category": "ensemble.bagging",
      "common_name": "Bagging Classifier",
      "description": "'A Bagging classifier.\n\nA Bagging classifier is an ensemble meta-estimator that fits base\nclassifiers each on random subsets of the original dataset and then\naggregate their individual predictions (either by voting or by averaging)\nto form a final prediction. Such a meta-estimator can typically be used as\na way to reduce the variance of a black-box estimator (e.g., a decision\ntree), by introducing randomization into its construction procedure and\nthen making an ensemble out of it.\n\nThis algorithm encompasses several works from the literature. When random\nsubsets of the dataset are drawn as random subsets of the samples, then\nthis algorithm is known as Pasting [1]_. If samples are drawn with\nreplacement, then the method is known as Bagging [2]_. When random subsets\nof the dataset are drawn as random subsets of the features, then the method\nis known as Random Subspaces [3]_. Finally, when base estimators are built\non subsets of both samples and features, then the method is known as\nRandom Patches [4]_.\n\nRead more in the :ref:`User Guide <bagging>`.\n",
      "id": "sklearn.ensemble.bagging.BaggingClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Build a Bagging ensemble of estimators from the training\nset (X, y).\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class for X.\n\nThe predicted class of an input sample is computed as the class with\nthe highest mean predicted probability. If base estimators do not\nimplement a ``predict_proba`` method, then it resorts to voting.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the base\nestimators in the ensemble.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe mean predicted class probabilities of the base estimators in the\nensemble. If base estimators do not implement a ``predict_proba``\nmethod, then it resorts to voting and the predicted class probabilities\nof an input sample represents the proportion of estimators predicting\neach class.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrices are accepted only if they are supported by the base estimator. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.bagging.BaggingClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.ensemble.bagging.BaggingClassifier",
      "parameters": [
        {
          "default": "None",
          "description": "The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "default": "10",
          "description": "The number of base estimators in the ensemble. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1.0",
          "description": "The number of samples to draw from X to train each base estimator. - If int, then draw `max_samples` samples. - If float, then draw `max_samples * X.shape[0]` samples. ",
          "name": "max_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1.0",
          "description": "The number of features to draw from X to train each base estimator. - If int, then draw `max_features` features. - If float, then draw `max_features * X.shape[1]` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "Whether samples are drawn with replacement. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "Whether features are drawn with replacement. ",
          "name": "bootstrap_features",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use out-of-bag samples to estimate the generalization error. ",
          "name": "oob_score",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.  .. versionadded:: 0.17 *warm_start* constructor parameter. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/bagging.pyc:427",
      "tags": [
        "ensemble",
        "bagging"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "classification",
        "decision tree"
      ],
      "attributes": [],
      "category": "feature_extraction.text",
      "common_name": "Hashing Vectorizer",
      "description": "u'Convert a collection of text documents to a matrix of token occurrences\n\nIt turns a collection of text documents into a scipy.sparse matrix holding\ntoken occurrence counts (or binary occurrence information), possibly\nnormalized as token frequencies if norm=\\'l1\\' or projected on the euclidean\nunit sphere if norm=\\'l2\\'.\n\nThis text vectorizer implementation uses the hashing trick to find the\ntoken string name to feature integer index mapping.\n\nThis strategy has several advantages:\n\n- it is very low memory scalable to large datasets as there is no need to\nstore a vocabulary dictionary in memory\n\n- it is fast to pickle and un-pickle as it holds no state besides the\nconstructor parameters\n\n- it can be used in a streaming (partial fit) or parallel pipeline as there\nis no state computed during fit.\n\nThere are also a couple of cons (vs using a CountVectorizer with an\nin-memory vocabulary):\n\n- there is no way to compute the inverse transform (from feature indices to\nstring feature names) which can be a problem when trying to introspect\nwhich features are most important to a model.\n\n- there can be collisions: distinct tokens can be mapped to the same\nfeature index. However in practice this is rarely an issue if n_features\nis large enough (e.g. 2 ** 18 for text classification problems).\n\n- no IDF weighting as this would render the transformer stateful.\n\nThe hash function employed is the signed 32-bit version of Murmurhash3.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.text.HashingVectorizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "u'Return a callable that handles preprocessing and tokenization'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.build_analyzer",
          "name": "build_analyzer",
          "parameters": []
        },
        {
          "description": "u'Return a function to preprocess the text before tokenization'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor",
          "name": "build_preprocessor",
          "parameters": []
        },
        {
          "description": "u'Return a function that splits a string into a sequence of tokens'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer",
          "name": "build_tokenizer",
          "parameters": []
        },
        {
          "description": "u'Decode the input into a string of unicode symbols\n\nThe decoding strategy depends on the vectorizer parameters.\n'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.decode",
          "name": "decode",
          "parameters": []
        },
        {
          "description": "u'Does nothing: this transformer is stateless.'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.fit",
          "name": "fit",
          "parameters": []
        },
        {
          "description": "u'Transform a sequence of documents to a document-term matrix.\n",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "Document-term matrix.  '",
            "name": "X",
            "shape": "n_samples, self.n_features",
            "type": "scipy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "u'Build or fetch the effective stop words list'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.get_stop_words",
          "name": "get_stop_words",
          "parameters": []
        },
        {
          "description": "u'Does nothing: this transformer is stateless.\n\nThis method is just there to mark the fact that this transformer\ncan work in a streaming setup.\n\n'",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.partial_fit",
          "name": "partial_fit",
          "parameters": []
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "u'Transform a sequence of documents to a document-term matrix.\n",
          "id": "sklearn.feature_extraction.text.HashingVectorizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed. ",
              "name": "X",
              "type": "iterable"
            },
            {
              "description": "",
              "name": "y",
              "type": ""
            }
          ],
          "returns": {
            "description": "Document-term matrix.  '",
            "name": "X",
            "shape": "n_samples, self.n_features",
            "type": "scipy"
          }
        }
      ],
      "name": "sklearn.feature_extraction.text.HashingVectorizer",
      "parameters": [
        {
          "description": "If \\'filename\\', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  If \\'file\\', the sequence items must have a \\'read\\' method (file-like object) that is called to fetch the bytes in memory.  Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. ",
          "name": "input",
          "type": "string"
        },
        {
          "description": "If bytes or files are given to analyze, this encoding is used to decode. ",
          "name": "encoding",
          "type": "string"
        },
        {
          "description": "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is \\'strict\\', meaning that a UnicodeDecodeError will be raised. Other values are \\'ignore\\' and \\'replace\\'. ",
          "name": "decode_error",
          "type": "\\'strict\\', \\'ignore\\', \\'replace\\'"
        },
        {
          "description": "Remove accents during the preprocessing step. \\'ascii\\' is a fast method that only works on characters that have an direct ASCII mapping. \\'unicode\\' is a slightly slower method that works on any characters. None (default) does nothing. ",
          "name": "strip_accents",
          "type": "\\'ascii\\', \\'unicode\\', None"
        },
        {
          "description": "Whether the feature should be made of word or character n-grams. Option \\'char_wb\\' creates character n-grams only from text inside word boundaries.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. ",
          "name": "analyzer",
          "type": "string"
        },
        {
          "description": "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. ",
          "name": "preprocessor",
          "type": "callable"
        },
        {
          "description": "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == \\'word\\'``. ",
          "name": "tokenizer",
          "type": "callable"
        },
        {
          "description": "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. ",
          "name": "ngram_range",
          "type": "tuple"
        },
        {
          "description": "If \\'english\\', a built-in stop word list for English is used.  If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == \\'word\\'``. ",
          "name": "stop_words",
          "type": "string"
        },
        {
          "description": "Convert all characters to lowercase before tokenizing. ",
          "name": "lowercase",
          "type": "boolean"
        },
        {
          "description": "Regular expression denoting what constitutes a \"token\", only used if ``analyzer == \\'word\\'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ",
          "name": "token_pattern",
          "type": "string"
        },
        {
          "description": "The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners. ",
          "name": "n_features",
          "type": "integer"
        },
        {
          "description": "Norm used to normalize term vectors. None for no normalization. ",
          "name": "norm",
          "optional": "true",
          "type": ""
        },
        {
          "description": "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. ",
          "name": "binary",
          "type": "boolean"
        },
        {
          "description": "Type of the matrix returned by fit_transform() or transform(). ",
          "name": "dtype",
          "optional": "true",
          "type": "type"
        },
        {
          "description": "Whether output matrices should contain non-negative values only; effectively calls abs on the matrix prior to returning it. When True, output values can be interpreted as frequencies. When False, output values will have expected value zero.  See also -------- CountVectorizer, TfidfVectorizer  '",
          "name": "non_negative",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc:284",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [],
      "category": "tree.tree",
      "common_name": "Extra Tree Classifier",
      "description": "'An extremely randomized tree classifier.\n\nExtra-trees differ from classic decision trees in the way they are built.\nWhen looking for the best split to separate the samples of a node into two\ngroups, random splits are drawn for each of the `max_features` randomly\nselected features and the best split among those is chosen. When\n`max_features` is set 1, this amounts to building a totally random\ndecision tree.\n\nWarning: Extra-trees should only be used within ensemble methods.\n\nRead more in the :ref:`User Guide <tree>`.\n\nSee also\n--------\nExtraTreeRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n\nReferences\n----------\n\n.. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\nMachine Learning, 63(1), 3-42, 2006.\n'",
      "id": "sklearn.tree.tree.ExtraTreeClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nReturns the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within ``[0; self.tree_.node_count)``, possibly with gaps in the numbering. \"",
            "name": "X_leaves",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Return the decision path in the tree\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  \"",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "\"Build a decision tree classifier from the training set (X, y).\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels) as integers or strings. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  X_idx_sorted : array-like, shape = [n_samples, n_features], optional The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don't use this parameter unless you know what to do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Returns self. \"",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "The predicted classes, or the predict values. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities of the input samples X.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "\"Predict class probabilities of the input samples X.\n\nThe predicted class probability is the fraction of samples of the same\nclass in a leaf.\n\ncheck_input : boolean, (default=True)\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. \"",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.tree.tree.ExtraTreeClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.tree.tree.ExtraTreeClassifier",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc:1033",
      "tags": [
        "tree",
        "tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.locally_linear.locally_linear_embedding",
      "description": "\"Perform a Locally Linear Embedding analysis on the data.\n\nRead more in the :ref:`User Guide <locally_linear_embedding>`.\n",
      "id": "sklearn.manifold.locally_linear.locally_linear_embedding",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.locally_linear.locally_linear_embedding",
      "parameters": [
        {
          "description": "Sample data, shape = (n_samples, n_features), in the form of a numpy array, sparse array, precomputed tree, or NearestNeighbors object. ",
          "name": "X",
          "type": "array-like, sparse matrix, BallTree, KDTree, NearestNeighbors"
        },
        {
          "description": "number of neighbors to consider for each point. ",
          "name": "n_neighbors",
          "type": "integer"
        },
        {
          "description": "number of coordinates for the manifold. ",
          "name": "n_components",
          "type": "integer"
        },
        {
          "description": "regularization constant, multiplies the trace of the local covariance matrix of the distances. ",
          "name": "reg",
          "type": "float"
        },
        {
          "description": "",
          "name": "eigen_solver",
          "type": "string"
        },
        {
          "description": "",
          "name": "auto",
          "type": "algorithm"
        },
        {
          "description": "For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results. ",
          "name": "arpack",
          "type": "use"
        },
        {
          "description": "decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems. ",
          "name": "dense",
          "type": "use"
        },
        {
          "description": "Tolerance for 'arpack' method Not used if eigen_solver=='dense'. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "maximum number of iterations for the arpack solver. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "",
          "name": "method",
          "type": "'standard', 'hessian', 'modified', 'ltsa'"
        },
        {
          "description": "see reference [1]_",
          "name": "standard",
          "type": "use"
        },
        {
          "description": "n_neighbors > n_components * (1 + (n_components + 1) / 2. see reference [2]_",
          "name": "hessian",
          "type": "use"
        },
        {
          "description": "see reference [3]_",
          "name": "modified",
          "type": "use"
        },
        {
          "description": "see reference [4]_ ",
          "name": "ltsa",
          "type": "use"
        },
        {
          "description": "Tolerance for Hessian eigenmapping method. Only used if method == 'hessian' ",
          "name": "hessian_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Tolerance for modified LLE method. Only used if method == 'modified'  random_state: numpy.RandomState or int, optional The generator or seed used to determine the starting vector for arpack iterations.  Defaults to numpy.random. ",
          "name": "modified_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Embedding vectors.  squared_error : float Reconstruction error for the embedding vectors. Equivalent to ``norm(Y - W Y, 'fro')**2``, where W are the reconstruction weights.  References ----------  .. [1] `Roweis, S. & Saul, L. Nonlinear dimensionality reduction by locally linear embedding.  Science 290:2323 (2000).` .. [2] `Donoho, D. & Grimes, C. Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data. Proc Natl Acad Sci U S A.  100:5591 (2003).` .. [3] `Zhang, Z. & Wang, J. MLLE: Modified Locally Linear Embedding Using Multiple Weights.` http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382 .. [4] `Zhang, Z. & Zha, H. Principal manifolds and nonlinear dimensionality reduction via tangent space alignment. Journal of Shanghai Univ.  8:406 (2004)` \"",
        "name": "Y",
        "shape": "n_samples, n_components",
        "type": "array-like"
      },
      "tags": [
        "manifold",
        "locally_linear"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.omp.orthogonal_mp",
      "description": "\"Orthogonal Matching Pursuit (OMP)\n\nSolves n_targets Orthogonal Matching Pursuit problems.\nAn instance of the problem has the form:\n\nWhen parametrized by the number of non-zero coefficients using\n`n_nonzero_coefs`:\nargmin ||y - X\\\\gamma||^2 subject to ||\\\\gamma||_0 <= n_{nonzero coefs}\n\nWhen parametrized by error using the parameter `tol`:\nargmin ||\\\\gamma||_0 subject to ||y - X\\\\gamma||^2 <= tol\n\nRead more in the :ref:`User Guide <omp>`.\n",
      "id": "sklearn.linear_model.omp.orthogonal_mp",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.omp.orthogonal_mp",
      "parameters": [
        {
          "description": "Input data. Columns are assumed to have unit norm. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Input targets ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. ",
          "name": "n_nonzero_coefs",
          "type": "int"
        },
        {
          "description": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Whether to perform precomputations. Improves performance when n_targets or n_samples is very large.  copy_X : bool, optional Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway. ",
          "name": "precompute",
          "type": "True, False, 'auto'"
        },
        {
          "description": "Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation. ",
          "name": "return_path",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Coefficients of the OMP solution. If `return_path=True`, this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features.  n_iters : array-like or int Number of active features across every target. Returned only if `return_n_iter` is set to True.  See also -------- OrthogonalMatchingPursuit orthogonal_mp_gram lars_path decomposition.sparse_encode  Notes ----- Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf  \"",
        "name": "coef",
        "shape": "n_features,",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "omp"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.least_angle.lars_path",
      "description": "'Compute Least Angle Regression or Lasso path using LARS algorithm [1]\n\nThe optimization objective for the case method=\\'lasso\\' is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nin the case of method=\\'lars\\', the objective function is only known in\nthe form of an implicit equation (see discussion in [1])\n\nRead more in the :ref:`User Guide <least_angle_regression>`.\n",
      "id": "sklearn.linear_model.least_angle.lars_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.least_angle.lars_path",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "Input targets. ",
          "name": "y",
          "type": "array"
        },
        {
          "description": "Restrict coefficients to be >= 0. When using this option together with method \\'lasso\\' the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha (neither will they when using method \\'lar\\' ..). Only coefficients up to the smallest alpha value (``alphas_[alphas_ > 0.].min()`` when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent lasso_path function. ",
          "name": "positive",
          "type": "boolean"
        },
        {
          "default": "500",
          "description": "Maximum number of iterations to perform, set to infinity for no limit.  Gram : None, \\'auto\\', array, shape: (n_features, n_features), optional Precomputed Gram matrix (X\\' * X), if ``\\'auto\\'``, the Gram matrix is precomputed from the given X, if there are more samples than features. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "0",
          "description": "Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso. ",
          "name": "alpha_min",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "\\'lar\\'",
          "description": "Specifies the returned model. Select ``\\'lar\\'`` for Least Angle Regression, ``\\'lasso\\'`` for the Lasso. ",
          "name": "method",
          "optional": "true",
          "type": "\\'lar\\', \\'lasso\\'"
        },
        {
          "default": "``np.finfo(np.float",
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems.  copy_X : bool, optional (default=True) If ``False``, ``X`` is overwritten.  copy_Gram : bool, optional (default=True) If ``False``, ``Gram`` is overwritten. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Controls output verbosity. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "default": "True",
          "description": "If ``return_path==True`` returns the entire path, else returns only the last point of the path. ",
          "name": "return_path",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "Whether to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Maximum of covariances (in absolute value) at each iteration. ``n_alphas`` is either ``max_iter``, ``n_features`` or the number of nodes in the path with ``alpha >= alpha_min``, whichever is smaller.  active : array, shape [n_alphas] Indices of active variables at the end of the path.  coefs : array, shape (n_features, n_alphas + 1) Coefficients along the path  n_iter : int Number of iterations run. Returned only if return_n_iter is set to True.  See also -------- lasso_path LassoLars Lars LassoLarsCV LarsCV sklearn.decomposition.sparse_encode  References ---------- .. [1] \"Least Angle Regression\", Effron et al. http://statweb.stanford.edu/~tibs/ftp/lars.pdf  .. [2] `Wikipedia entry on the Least-angle regression <https://en.wikipedia.org/wiki/Least-angle_regression>`_  .. [3] `Wikipedia entry on the Lasso <https://en.wikipedia.org/wiki/Lasso_(statistics)>`_  '",
        "name": "alphas",
        "shape": "n_alphas + 1",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "least_angle"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.isolve.iterative.gmres",
      "description": "\"\nUse Generalized Minimal RESidual iteration to solve A x = b.\n",
      "id": "scipy.sparse.linalg.isolve.iterative.gmres",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.isolve.iterative.gmres",
      "parameters": [
        {
          "description": "The real or complex N-by-N matrix of the linear system.",
          "name": "A",
          "type": "sparse matrix, dense matrix, LinearOperator"
        },
        {
          "description": "Right hand side of the linear system. Has shape (N,) or (N,1). ",
          "name": "b",
          "type": "array, matrix"
        }
      ],
      "returns": {
        "description": "The converged solution. info : int Provides convergence information: * 0  : successful exit * >0 : convergence to tolerance not achieved, number of iterations * <0 : illegal input or breakdown  Other parameters ---------------- x0 : {array, matrix} Starting guess for the solution (a vector of zeros by default). tol : float Tolerance to achieve. The algorithm terminates when either the relative or the absolute residual is below `tol`. restart : int, optional Number of iterations between restarts. Larger values increase iteration cost, but may be necessary for convergence. Default is 20. maxiter : int, optional Maximum number of iterations (restart cycles).  Iteration will stop after maxiter steps even if the specified tolerance has not been achieved. xtype : {'f','d','F','D'} This parameter is DEPRECATED --- avoid using it.  The type of the result.  If None, then it will be determined from A.dtype.char and b.  If A does not have a typecode method then it will compute A.matvec(x0) to get a typecode.   To save the extra computation when A does not have a typecode attribute use xtype=0 for the same type as b or use xtype='f','d','F',or 'D'. This parameter has been superseded by LinearOperator. M : {sparse matrix, dense matrix, LinearOperator} Inverse of the preconditioner of A.  M should approximate the inverse of A and be easy to solve for (see Notes).  Effective preconditioning dramatically improves the rate of convergence, which implies that fewer iterations are needed to reach a given error tolerance.  By default, no preconditioner is used. callback : function User-supplied function to call after each iteration.  It is called as callback(rk), where rk is the current residual vector. restrt : int, optional DEPRECATED - use `restart` instead.  See Also -------- LinearOperator  Notes ----- A preconditioner, P, is chosen such that P is close to A but easy to solve for. The preconditioner parameter required by this routine is ``M = P^-1``. The inverse should preferably not be calculated explicitly.  Rather, use the following template to produce M::  # Construct a linear operator that computes P^-1 * x. import scipy.sparse.linalg as spla M_x = lambda x: spla.spsolve(P, x) M = spla.LinearOperator((n, n), M_x)  \"",
        "name": "x",
        "type": "array, matrix"
      },
      "tags": [
        "sparse",
        "linalg",
        "isolve",
        "iterative"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.v_measure_score",
      "description": "'V-measure cluster labeling given a ground truth.\n\nThis score is identical to :func:`normalized_mutual_info_score`.\n\nThe V-measure is the harmonic mean between homogeneity and completeness::\n\nv = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won\\'t change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nRead more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "id": "sklearn.metrics.cluster.supervised.v_measure_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.v_measure_score",
      "parameters": [
        {
          "description": "ground truth class labels to be used as a reference ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "cluster labels to evaluate ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling  References ----------  .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A conditional entropy-based external cluster evaluation measure <http://aclweb.org/anthology/D/D07/D07-1043.pdf>`_  See also -------- homogeneity_score completeness_score  Examples --------  Perfect labelings are both homogeneous and complete, hence have score 1.0::  >>> from sklearn.metrics.cluster import v_measure_score >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]) 1.0 >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  Labelings that assign all classes members to the same clusters are complete be not homogeneous, hence penalized::  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1])) ...                                                  # doctest: +ELLIPSIS 0.8... >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1])) ...                                                  # doctest: +ELLIPSIS 0.66...  Labelings that have pure clusters with members coming from the same classes are homogeneous but un-necessary splits harms completeness and thus penalize V-measure as well::  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2])) ...                                                  # doctest: +ELLIPSIS 0.8... >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3])) ...                                                  # doctest: +ELLIPSIS 0.66...  If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null::  >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3])) ...                                                  # doctest: +ELLIPSIS 0.0...  Clusters that include samples from totally different classes totally destroy the homogeneity of the labeling, hence::  >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0])) ...                                                  # doctest: +ELLIPSIS 0.0...  '",
        "name": "v_measure",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_regression",
      "description": "'Generate a random regression problem.\n\nThe input set can either be well conditioned (by default) or have a low\nrank-fat tail singular profile. See :func:`make_low_rank_matrix` for\nmore details.\n\nThe output is generated by applying a (potentially biased) random linear\nregression model with `n_informative` nonzero regressors to the previously\ngenerated input and some gaussian centered noise with some adjustable\nscale.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_regression",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_regression",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "100",
          "description": "The number of features. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "10",
          "description": "The number of informative features, i.e., the number of features used to build the linear model used to generate the output. ",
          "name": "n_informative",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar. ",
          "name": "n_targets",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.0",
          "description": "The bias term in the underlying linear model. ",
          "name": "bias",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "if not None: The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice. if None: The input set is well conditioned, centered and gaussian with unit variance. ",
          "name": "effective_rank",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.5",
          "description": "The relative importance of the fat noisy tail of the singular values profile if `effective_rank` is not None. ",
          "name": "tail_strength",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "0.0",
          "description": "The standard deviation of the gaussian noise applied to the output. ",
          "name": "noise",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Shuffle the samples and the features. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "If True, the coefficients of the underlying linear model are returned. ",
          "name": "coef",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The input samples.  y : array of shape [n_samples] or [n_samples, n_targets] The output values.  coef : array of shape [n_features] or [n_features, n_targets], optional The coefficient of the underlying linear model. It is returned only if coef is True. '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.spectral_embedding_.spectral_embedding",
      "description": "\"Project the sample on the first eigenvectors of the graph Laplacian.\n\nThe adjacency matrix is used to compute a normalized graph Laplacian\nwhose spectrum (especially the eigenvectors associated to the\nsmallest eigenvalues) has an interpretation in terms of minimal\nnumber of cuts necessary to split the graph into comparably sized\ncomponents.\n\nThis embedding can also 'work' even if the ``adjacency`` variable is\nnot strictly the adjacency matrix of a graph but more generally\nan affinity or similarity matrix between samples (for instance the\nheat kernel of a euclidean distance matrix or a k-NN matrix).\n\nHowever care must taken to always make the affinity matrix symmetric\nso that the eigenvector decomposition works as expected.\n\nRead more in the :ref:`User Guide <spectral_embedding>`.\n",
      "id": "sklearn.manifold.spectral_embedding_.spectral_embedding",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.spectral_embedding_.spectral_embedding",
      "parameters": [
        {
          "description": "The adjacency matrix of the graph to embed. ",
          "name": "adjacency",
          "type": "array-like"
        },
        {
          "description": "The dimension of the projection subspace. ",
          "name": "n_components",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. ",
          "name": "eigen_solver",
          "type": "None, 'arpack', 'lobpcg', or 'amg'"
        },
        {
          "description": "A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition when eigen_solver == 'amg'. By default, arpack is used. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. ",
          "name": "eigen_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector. ",
          "name": "drop_first",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If True, then compute normalized Laplacian. ",
          "name": "norm_laplacian",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The reduced samples.  Notes ----- Spectral embedding is most useful when the graph has one connected component. If there graph has many components, the first few eigenvectors will simply uncover the connected components of the graph.  References ---------- * https://en.wikipedia.org/wiki/LOBPCG  * Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method Andrew V. Knyazev http://dx.doi.org/10.1137%2FS1064827500366124 \"",
        "name": "embedding",
        "shape": "n_samples, n_components",
        "type": "array"
      },
      "tags": [
        "manifold",
        "spectral_embedding_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "numpy.core.fromnumeric.partition",
      "description": "\"\nReturn a partitioned copy of an array.\n\nCreates a copy of the array with its elements rearranged in such a way that\nthe value of the element in kth position is in the position it would be in\na sorted array. All elements smaller than the kth element are moved before\nthis element and all equal or greater are moved behind it. The ordering of\nthe elements in the two partitions is undefined.\n\n.. versionadded:: 1.8.0\n",
      "id": "numpy.core.fromnumeric.partition",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "numpy.core.fromnumeric.partition",
      "parameters": [
        {
          "description": "Array to be sorted.",
          "name": "a",
          "type": "array"
        },
        {
          "description": "Element index to partition by. The kth value of the element will be in its final sorted position and all smaller elements will be moved before it and all equal or greater elements behind it. The order all elements in the partitions is undefined. If provided with a sequence of kth it will partition all elements indexed by kth  of them into their sorted position at once.",
          "name": "kth",
          "type": "int"
        },
        {
          "description": "Axis along which to sort. If None, the array is flattened before sorting. The default is -1, which sorts along the last axis.",
          "name": "axis",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Selection algorithm. Default is 'introselect'.",
          "name": "kind",
          "optional": "true",
          "type": "'introselect'"
        },
        {
          "description": "When `a` is an array with fields defined, this argument specifies which fields to compare first, second, etc.  A single field can be specified as a string.  Not all fields need be specified, but unspecified fields will still be used, in the order in which they come up in the dtype, to break ties. ",
          "name": "order",
          "optional": "true",
          "type": "str"
        }
      ],
      "returns": {
        "description": "Array of the same type and shape as `a`.  See Also -------- ndarray.partition : Method to sort an array in-place. argpartition : Indirect partition. sort : Full sorting  Notes ----- The various selection algorithms are characterized by their average speed, worst case performance, work space size, and whether they are stable. A stable sort keeps items with the same key in the same relative order. The available algorithms have the following properties:  ================= ======= ============= ============ ======= kind            speed   worst case    work space  stable ================= ======= ============= ============ ======= 'introselect'        1        O(n)           0         no ================= ======= ============= ============ =======  All the partition algorithms make temporary copies of the data when partitioning along any but the last axis.  Consequently, partitioning along the last axis is faster and uses less space than partitioning along any other axis.  The sort order for complex numbers is lexicographic. If both the real and imaginary parts are non-nan then the order is determined by the real parts except when they are equal, in which case the order is determined by the imaginary parts.  Examples -------- >>> a = np.array([3, 4, 2, 1]) >>> np.partition(a, 3) array([2, 1, 3, 4])  >>> np.partition(a, (1, 3)) array([1, 2, 3, 4])  \"",
        "name": "partitioned_array",
        "type": "ndarray"
      },
      "tags": [
        "core",
        "fromnumeric"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cross_validation.permutation_test_score",
      "description": "\"Evaluate the significance of a cross-validated score with permutations\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.permutation_test_score` instead.\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.cross_validation.permutation_test_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cross_validation.permutation_test_score",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like of shape at least 2D The data to fit. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of times to permute ``y``. ",
          "name": "n_permutations",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Labels constrain the permutation among groups of samples with a same label. ",
          "name": "labels",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "A random number generator instance to define the state of the random permutations generator. ",
          "name": "random_state",
          "type": ""
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "The true score without permuting targets.  permutation_scores : array, shape (n_permutations,) The scores obtained for each permutations.  pvalue : float The returned value equals p-value if `scoring` returns bigger numbers for better scores (e.g., accuracy_score). If `scoring` is rather a loss function (i.e. when lower is better such as with `mean_squared_error`) then this is actually the complement of the p-value:  1 - p-value.  Notes ----- This function implements Test 1 in:  Ojala and Garriga. Permutation Tests for Studying Classifier Performance.  The Journal of Machine Learning Research (2010) vol. 11  \"",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "cross_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.dict_learning.dict_learning_online",
      "description": "\"Solves a dictionary learning matrix factorization problem online.\n\nFinds the best dictionary and the corresponding sparse code for\napproximating the data matrix X by solving::\n\n(U^*, V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1\n(U,V)\nwith || V_k ||_2 = 1 for all  0 <= k < n_components\n\nwhere V is the dictionary and U is the sparse code. This is\naccomplished by repeatedly iterating over mini-batches by slicing\nthe input data.\n\nRead more in the :ref:`User Guide <DictionaryLearning>`.\n",
      "id": "sklearn.decomposition.dict_learning.dict_learning_online",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.dict_learning.dict_learning_online",
      "parameters": [
        {
          "description": "Data matrix. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Number of dictionary atoms to extract. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "Sparsity controlling parameter. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Number of iterations to perform. ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "Whether to also return the code U or just the dictionary V. ",
          "name": "return_code",
          "type": "boolean"
        },
        {
          "description": "Initial value for the dictionary for warm restart scenarios.  callback : Callable that gets invoked every five iterations. ",
          "name": "dict_init",
          "shape": "n_components, n_features",
          "type": "array"
        },
        {
          "description": "The number of samples to take in each batch.  verbose : Degree of output the procedure will print. ",
          "name": "batch_size",
          "type": "int"
        },
        {
          "description": "Whether to shuffle the data before splitting it in batches. ",
          "name": "shuffle",
          "type": "boolean"
        },
        {
          "description": "Number of parallel jobs to run, or -1 to autodetect. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. ",
          "name": "method",
          "type": "'lars', 'cd'"
        },
        {
          "description": "Number of previous iterations completed on the dictionary used for initialization. ",
          "name": "iter_offset",
          "type": "int"
        },
        {
          "description": "Pseudo number generator state used for random sampling. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Return the inner statistics A (dictionary covariance) and B (data approximation). Useful to restart the algorithm in an online setting. If return_inner_stats is True, return_code is ignored ",
          "name": "return_inner_stats",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix ",
          "name": "inner_stats",
          "type": "tuple"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "the sparse code (only returned if `return_code=True`)  dictionary : array of shape (n_components, n_features), the solutions to the dictionary learning problem  n_iter : int Number of iterations run. Returned only if `return_n_iter` is set to `True`.  See also -------- dict_learning DictionaryLearning MiniBatchDictionaryLearning SparsePCA MiniBatchSparsePCA  \"",
        "name": "code",
        "shape": "n_samples, n_components",
        "type": "array"
      },
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.ensemble.partial_dependence.plot_partial_dependence",
      "description": "\"Partial dependence plots for ``features``.\n\nThe ``len(features)`` plots are arranged in a grid with ``n_cols``\ncolumns. Two-way partial dependence plots are plotted as contour\nplots.\n\nRead more in the :ref:`User Guide <partial_dependence>`.\n",
      "id": "sklearn.ensemble.partial_dependence.plot_partial_dependence",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.ensemble.partial_dependence.plot_partial_dependence",
      "parameters": [
        {
          "description": "A fitted gradient boosting model. X : array-like, shape=(n_samples, n_features) The data on which ``gbrt`` was trained.",
          "name": "gbrt",
          "type": ""
        },
        {
          "description": "If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be < len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names.",
          "name": "features",
          "type": "seq"
        },
        {
          "description": "Name of each feature; feature_names[i] holds the name of the feature with index i.",
          "name": "feature_names",
          "type": "seq"
        },
        {
          "description": "The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in ``gbrt.classes_``.",
          "name": "label",
          "type": "object"
        },
        {
          "description": "The number of columns in the grid plot (default: 3).",
          "name": "n_cols",
          "type": "int"
        },
        {
          "description": "The lower and upper percentile used to create the extreme values for the PDP axes.",
          "name": "percentiles",
          "type": ""
        },
        {
          "description": "The number of equally spaced points on the axes.",
          "name": "grid_resolution",
          "type": "int"
        },
        {
          "description": "The number of CPUs to use to compute the PDs. -1 means 'all CPUs'. Defaults to 1.",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Verbose output during PD computations. Defaults to 0.",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "An axis object onto which the plots will be drawn.",
          "name": "ax",
          "type": ""
        },
        {
          "description": "Dict with keywords passed to the ``matplotlib.pyplot.plot`` call. For one-way partial dependence plots.",
          "name": "line_kw",
          "type": "dict"
        },
        {
          "description": "Dict with keywords passed to the ``matplotlib.pyplot.plot`` call. For two-way partial dependence plots.",
          "name": "contour_kw",
          "type": "dict"
        },
        {
          "description": "Dict with keywords passed to the figure() call. Note that all keywords not recognized above will be automatically included here. ",
          "name": "fig_kw",
          "type": "dict"
        }
      ],
      "returns": {
        "description": "The Matplotlib Figure object. axs : seq of Axis objects A seq of Axis objects, one for each subplot.  Examples -------- >>> from sklearn.datasets import make_friedman1 >>> from sklearn.ensemble import GradientBoostingRegressor >>> X, y = make_friedman1() >>> clf = GradientBoostingRegressor(n_estimators=10).fit(X, y) >>> fig, axs = plot_partial_dependence(clf, X, [0, (0, 1)]) #doctest: +SKIP ... \"",
        "name": "fig",
        "type": "figure"
      },
      "tags": [
        "ensemble",
        "partial_dependence"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.recall_score",
      "description": "\"Compute the recall\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe best value is 1 and the worst value is 0.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.classification.recall_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.recall_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order.  .. versionchanged:: 0.17 parameter *labels* improved for multiclass problem. ",
          "name": "labels",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only. ",
          "name": "pos_label",
          "type": "str"
        },
        {
          "description": "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``'binary'``: Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary. ``'micro'``: Calculate metrics globally by counting the total true positives, false negatives and false positives. ``'macro'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``'weighted'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. ``'samples'``: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`). ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Recall of the positive class in binary classification or weighted average of the recall of each class for the multiclass task.  Examples -------- >>> from sklearn.metrics import recall_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> recall_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS 0.33... >>> recall_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS 0.33... >>> recall_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS 0.33... >>> recall_score(y_true, y_pred, average=None) array([ 1.,  0.,  0.])   \"",
        "name": "recall",
        "shape": "n_unique_labels",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.manifold.mds.smacof",
      "description": "'\nComputes multidimensional scaling using SMACOF (Scaling by Majorizing a\nComplicated Function) algorithm\n\nThe SMACOF algorithm is a multidimensional scaling algorithm: it minimizes\na objective function, the *stress*, using a majorization technique. The\nStress Majorization, also known as the Guttman Transform, guarantees a\nmonotone convergence of Stress, and is more powerful than traditional\ntechniques such as gradient descent.\n\nThe SMACOF algorithm for metric MDS can summarized by the following steps:\n\n1. Set an initial start configuration, randomly or not.\n2. Compute the stress\n3. Compute the Guttman Transform\n4. Iterate 2 and 3 until convergence.\n\nThe nonmetric algorithm adds a monotonic regression steps before computing\nthe stress.\n",
      "id": "sklearn.manifold.mds.smacof",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.manifold.mds.smacof",
      "parameters": [
        {
          "description": "similarities between the points ",
          "name": "similarities",
          "shape": "n_samples, n_samples",
          "type": "symmetric"
        },
        {
          "description": "compute metric or nonmetric SMACOF algorithm ",
          "name": "metric",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "number of dimension in which to immerse the similarities overridden if initial array is provided. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "if None, randomly chooses the initial configuration if ndarray, initialize the SMACOF algorithm with this array ",
          "name": "init",
          "optional": "true",
          "shape": "n_samples, n_components",
          "type": "None or ndarray of shape (n_samples, n_components)"
        },
        {
          "description": "Number of time the smacof algorithm will be run with different initialisation. The final results will be the best output of the n_init consecutive runs in terms of stress. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": " The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations of the SMACOF algorithm for a single run ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "level of verbosity ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "relative tolerance w.r.t stress to declare converge ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Coordinates of the n_samples points in a n_components-space  stress : float The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points)  n_iter : int The number of iterations corresponding to the best stress. Returned only if `return_n_iter` is set to True.  Notes ----- \"Modern Multidimensional Scaling - Theory and Applications\" Borg, I.; Groenen P. Springer Series in Statistics (1997)  \"Nonmetric multidimensional scaling: a numerical method\" Kruskal, J. Psychometrika, 29 (1964)  \"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\" Kruskal, J. Psychometrika, 29, (1964) '",
        "name": "X",
        "type": "ndarray"
      },
      "tags": [
        "manifold",
        "mds"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.mean_shift_.mean_shift",
      "description": "'Perform mean shift clustering of data using a flat kernel.\n\nRead more in the :ref:`User Guide <mean_shift>`.\n",
      "id": "sklearn.cluster.mean_shift_.mean_shift",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.mean_shift_.mean_shift",
      "parameters": [
        {
          "description": "Kernel bandwidth.  If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently. ",
          "name": "bandwidth",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Point used as initial kernel locations. If None and bin_seeding=False, each data point is used as a seed. If None and bin_seeding=True, see bin_seeding. ",
          "name": "seeds",
          "shape": "n_seeds, n_features",
          "type": "array-like"
        },
        {
          "description": "If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None. ",
          "name": "bin_seeding",
          "type": "boolean"
        },
        {
          "description": "To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. ",
          "name": "min_bin_freq",
          "type": "int"
        },
        {
          "description": "If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. ",
          "name": "cluster_all",
          "type": "boolean"
        },
        {
          "description": "Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.  .. versionadded:: 0.17 Parallel Execution using *n_jobs*. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "returns": {
        "description": "cluster_centers : array, shape=[n_clusters, n_features] Coordinates of cluster centers.  labels : array, shape=[n_samples] Cluster labels for each point.  Notes ----- See examples/cluster/plot_mean_shift.py for an example.  '",
        "name": ""
      },
      "tags": [
        "cluster",
        "mean_shift_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._validation.validation_curve",
      "description": "'Validation curve.\n\nDetermine training and test scores for varying parameter values.\n\nCompute scores for an estimator with different values of a specified\nparameter. This is similar to grid search with one parameter. However, this\nwill also compute training scores and is merely a utility for plotting the\nresults.\n\nRead more in the :ref:`User Guide <learning_curve>`.\n",
      "id": "sklearn.model_selection._validation.validation_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._validation.validation_curve",
      "parameters": [
        {
          "description": "An object of that type which is cloned for each validation.  X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
          "name": "y",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Name of the parameter that will be varied. ",
          "name": "param_name",
          "type": "string"
        },
        {
          "description": "The values of the parameter that will be evaluated. ",
          "name": "param_range",
          "shape": "n_values,",
          "type": "array-like"
        },
        {
          "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
          "name": "groups",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of jobs to run in parallel (default 1). ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like \\'2*n_jobs\\'. ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Scores on training sets.  test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set.  Notes ----- See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`  '",
        "name": "train_scores",
        "shape": "n_ticks, n_cv_folds",
        "type": "array"
      },
      "tags": [
        "model_selection",
        "_validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "A mapping of terms to feature indices. ",
          "name": "vocabulary_",
          "type": "dict"
        },
        {
          "description": "Terms that were ignored because they either:  - occurred in too many documents (`max_df`) - occurred in too few documents (`min_df`) - were cut off by feature selection (`max_features`).  This is only available if no vocabulary was given.  See also -------- HashingVectorizer, TfidfVectorizer ",
          "name": "stop_words_",
          "type": "set"
        }
      ],
      "category": "feature_extraction.text",
      "common_name": "Count Vectorizer",
      "description": "u'Convert a collection of text documents to a matrix of token counts\n\nThis implementation produces a sparse representation of the counts using\nscipy.sparse.coo_matrix.\n\nIf you do not provide an a-priori dictionary and you do not use an analyzer\nthat does some kind of feature selection then the number of features will\nbe equal to the vocabulary size found by analyzing the data.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.text.CountVectorizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "u'Return a callable that handles preprocessing and tokenization'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.build_analyzer",
          "name": "build_analyzer",
          "parameters": []
        },
        {
          "description": "u'Return a function to preprocess the text before tokenization'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.build_preprocessor",
          "name": "build_preprocessor",
          "parameters": []
        },
        {
          "description": "u'Return a function that splits a string into a sequence of tokens'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.build_tokenizer",
          "name": "build_tokenizer",
          "parameters": []
        },
        {
          "description": "u'Decode the input into a string of unicode symbols\n\nThe decoding strategy depends on the vectorizer parameters.\n'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.decode",
          "name": "decode",
          "parameters": []
        },
        {
          "description": "u'Learn a vocabulary dictionary of all tokens in the raw documents.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "An iterable which yields either str, unicode or file objects. ",
              "name": "raw_documents",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "u'Learn the vocabulary dictionary and return term-document matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "An iterable which yields either str, unicode or file objects. ",
              "name": "raw_documents",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "Document-term matrix. '",
            "name": "X",
            "type": "array"
          }
        },
        {
          "description": "u'Array mapping from feature integer indices to feature name'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.get_feature_names",
          "name": "get_feature_names",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "u'Build or fetch the effective stop words list'",
          "id": "sklearn.feature_extraction.text.CountVectorizer.get_stop_words",
          "name": "get_stop_words",
          "parameters": []
        },
        {
          "description": "u'Return terms per document with nonzero entries in X.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array, sparse matrix"
            }
          ],
          "returns": {
            "description": "List of arrays of terms. '",
            "name": "X_inv",
            "type": "list"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "u'Transform documents to document-term matrix.\n\nExtract token counts out of raw text documents using the vocabulary\nfitted with fit or the one provided to the constructor.\n",
          "id": "sklearn.feature_extraction.text.CountVectorizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "An iterable which yields either str, unicode or file objects. ",
              "name": "raw_documents",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "Document-term matrix. '",
            "name": "X",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.feature_extraction.text.CountVectorizer",
      "parameters": [
        {
          "description": "If \\'filename\\', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  If \\'file\\', the sequence items must have a \\'read\\' method (file-like object) that is called to fetch the bytes in memory.  Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. ",
          "name": "input",
          "type": "string"
        },
        {
          "description": "If bytes or files are given to analyze, this encoding is used to decode. ",
          "name": "encoding",
          "type": "string"
        },
        {
          "description": "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is \\'strict\\', meaning that a UnicodeDecodeError will be raised. Other values are \\'ignore\\' and \\'replace\\'. ",
          "name": "decode_error",
          "type": "\\'strict\\', \\'ignore\\', \\'replace\\'"
        },
        {
          "description": "Remove accents during the preprocessing step. \\'ascii\\' is a fast method that only works on characters that have an direct ASCII mapping. \\'unicode\\' is a slightly slower method that works on any characters. None (default) does nothing. ",
          "name": "strip_accents",
          "type": "\\'ascii\\', \\'unicode\\', None"
        },
        {
          "description": "Whether the feature should be made of word or character n-grams. Option \\'char_wb\\' creates character n-grams only from text inside word boundaries.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. ",
          "name": "analyzer",
          "type": "string"
        },
        {
          "description": "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. ",
          "name": "preprocessor",
          "type": "callable"
        },
        {
          "description": "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == \\'word\\'``. ",
          "name": "tokenizer",
          "type": "callable"
        },
        {
          "description": "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. ",
          "name": "ngram_range",
          "type": "tuple"
        },
        {
          "description": "If \\'english\\', a built-in stop word list for English is used.  If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == \\'word\\'``.  If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. ",
          "name": "stop_words",
          "type": "string"
        },
        {
          "description": "Convert all characters to lowercase before tokenizing. ",
          "name": "lowercase",
          "type": "boolean"
        },
        {
          "description": "Regular expression denoting what constitutes a \"token\", only used if ``analyzer == \\'word\\'``. The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ",
          "name": "token_pattern",
          "type": "string"
        },
        {
          "description": "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. ",
          "name": "max_df",
          "type": "float"
        },
        {
          "description": "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. ",
          "name": "min_df",
          "type": "float"
        },
        {
          "description": "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.  This parameter is ignored if vocabulary is not None. ",
          "name": "max_features",
          "type": "int"
        },
        {
          "description": "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. ",
          "name": "vocabulary",
          "optional": "true",
          "type": ""
        },
        {
          "description": "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. ",
          "name": "binary",
          "type": "boolean"
        },
        {
          "description": "Type of the matrix returned by fit_transform() or transform(). ",
          "name": "dtype",
          "optional": "true",
          "type": "type"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc:511",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The classes labels. ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "The number of classes. ",
          "name": "n_classes_",
          "type": "int"
        },
        {
          "description": "Weights for each estimator in the boosted ensemble. ",
          "name": "estimator_weights_",
          "type": "array"
        },
        {
          "description": "Classification error for each estimator in the boosted ensemble. ",
          "name": "estimator_errors_",
          "type": "array"
        },
        {
          "description": "The feature importances if supported by the ``base_estimator``.  See also -------- AdaBoostRegressor, GradientBoostingClassifier, DecisionTreeClassifier ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        }
      ],
      "category": "ensemble.weight_boosting",
      "common_name": "Ada Boost Classifier",
      "description": "'An AdaBoost classifier.\n\nAn AdaBoost [1] classifier is a meta-estimator that begins by fitting a\nclassifier on the original dataset and then fits additional copies of the\nclassifier on the same dataset but where the weights of incorrectly\nclassified instances are adjusted such that subsequent classifiers focus\nmore on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME [2].\n\nRead more in the :ref:`User Guide <adaboost>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Compute the decision function of ``X``.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of outputs is the same of that of the `classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. '",
            "name": "score",
            "shape": "n_samples, k",
            "type": "array"
          }
        },
        {
          "description": "'Build a boosted classifier from the training set (X, y).\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "The target values (class labels). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, the sample weights are initialized to ``1 / n_samples``. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict classes for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe weighted mean predicted class log-probabilities of the classifiers\nin the ensemble.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The class probabilities of the input samples. The order of outputs is the same of that of the `classes_` attribute. '",
            "name": "p",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The class probabilities of the input samples. The order of outputs is the same of that of the `classes_` attribute. '",
            "name": "p",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Compute decision function of ``X`` for each boosting iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each boosting iteration.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_decision_function",
          "name": "staged_decision_function",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of outputs is the same of that of the `classes_` attribute. Binary classification is a special cases with ``k == 1``, otherwise ``k==n_classes``. For binary classification, values closer to -1 or 1 mean more like the first or second class in ``classes_``, respectively. '",
            "name": "score",
            "shape": "n_samples, k",
            "type": "generator"
          }
        },
        {
          "description": "'Return staged predictions for X.\n\nThe predicted class of an input sample is computed as the weighted mean\nprediction of the classifiers in the ensemble.\n\nThis generator method yields the ensemble prediction after each\niteration of boosting and therefore allows monitoring, such as to\ndetermine the prediction on a test set after each boost.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_predict",
          "name": "staged_predict",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample is computed as\nthe weighted mean predicted class probabilities of the classifiers\nin the ensemble.\n\nThis generator method yields the ensemble predicted class probabilities\nafter each iteration of boosting and therefore allows monitoring, such\nas to determine the predicted class probabilities on a test set after\neach boost.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_predict_proba",
          "name": "staged_predict_proba",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "The class probabilities of the input samples. The order of outputs is the same of that of the `classes_` attribute. '",
            "name": "p",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'Return staged scores for X, y.\n\nThis generator method yields the ensemble score after each iteration of\nboosting and therefore allows monitoring, such as to determine the\nscore on a test set after each boost.\n",
          "id": "sklearn.ensemble.weight_boosting.AdaBoostClassifier.staged_score",
          "name": "staged_score",
          "parameters": [
            {
              "description": "The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "z",
            "type": "float"
          }
        }
      ],
      "name": "sklearn.ensemble.weight_boosting.AdaBoostClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "DecisionTreeClassifier",
          "description": "The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper `classes_` and `n_classes_` attributes. ",
          "name": "base_estimator",
          "optional": "true",
          "type": "object"
        },
        {
          "default": "50",
          "description": "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "1.",
          "description": "Learning rate shrinks the contribution of each classifier by ``learning_rate``. There is a trade-off between ``learning_rate`` and ``n_estimators``. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "\\'SAMME.R\\'",
          "description": "If \\'SAMME.R\\' then use the SAMME.R real boosting algorithm. ``base_estimator`` must support calculation of class probabilities. If \\'SAMME\\' then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations. ",
          "name": "algorithm",
          "optional": "true",
          "type": "\\'SAMME\\', \\'SAMME.R\\'"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc:295",
      "tags": [
        "ensemble",
        "weight_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by ``explained_variance_``. ",
          "name": "components_",
          "type": "array"
        },
        {
          "description": "The amount of variance explained by each of the selected components.  .. versionadded:: 0.18 ",
          "name": "explained_variance_",
          "type": "array"
        },
        {
          "description": "Percentage of variance explained by each of the selected components.  If ``n_components`` is not set then all components are stored and the sum of explained variances is equal to 1.0. ",
          "name": "explained_variance_ratio_",
          "type": "array"
        },
        {
          "description": "Per-feature empirical mean, estimated from the training set.  Equal to `X.mean(axis=1)`. ",
          "name": "mean_",
          "type": "array"
        },
        {
          "description": "The estimated number of components. When n_components is set to \\'mle\\' or a number between 0 and 1 (with svd_solver == \\'full\\') this number is estimated from input data. Otherwise it equals the parameter n_components, or n_features if n_components is None. ",
          "name": "n_components_",
          "type": "int"
        },
        {
          "description": "The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See \"Pattern Recognition and Machine Learning\" by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf. It is required to computed the estimated data covariance and score samples.  References ---------- For n_components == \\'mle\\', this class uses the method of `Thomas P. Minka: Automatic Choice of Dimensionality for PCA. NIPS 2000: 598-604`  Implements the probabilistic PCA model from: M. Tipping and C. Bishop, Probabilistic Principal Component Analysis, Journal of the Royal Statistical Society, Series B, 61, Part 3, pp. 611-622 via the score and score_samples methods. See http://www.miketipping.com/papers/met-mppca.pdf  For svd_solver == \\'arpack\\', refer to `scipy.sparse.linalg.svds`.  For svd_solver == \\'randomized\\', see: `Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909)` `A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert`  ",
          "name": "noise_variance_",
          "type": "float"
        }
      ],
      "category": "decomposition.pca",
      "common_name": "PCA",
      "description": "'Principal component analysis (PCA)\n\nLinear dimensionality reduction using Singular Value Decomposition of the\ndata to project it to a lower dimensional space.\n\nIt uses the LAPACK implementation of the full SVD or a randomized truncated\nSVD by the method of Halko et al. 2009, depending on the shape of the input\ndata and the number of components to extract.\n\nIt can also use the scipy.sparse.linalg ARPACK implementation of the\ntruncated SVD.\n\nNotice that this class does not support sparse input. See\n:class:`TruncatedSVD` for an alternative with sparse data.\n\nRead more in the :ref:`User Guide <PCA>`.\n",
      "id": "sklearn.decomposition.pca.PCA",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the model with X.\n",
          "id": "sklearn.decomposition.pca.PCA.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the instance itself. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit the model with X and apply the dimensionality reduction on X.\n",
          "id": "sklearn.decomposition.pca.PCA.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        },
        {
          "description": "'Compute data covariance with the generative model.\n\n``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\nwhere  S**2 contains the explained variances, and sigma2 contains the\nnoise variances.\n",
          "id": "sklearn.decomposition.pca.PCA.get_covariance",
          "name": "get_covariance",
          "parameters": [],
          "returns": {
            "description": "Estimated covariance of data. '",
            "name": "cov",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.decomposition.pca.PCA.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Compute data precision matrix with the generative model.\n\nEquals the inverse of the covariance but computed with\nthe matrix inversion lemma for efficiency.\n",
          "id": "sklearn.decomposition.pca.PCA.get_precision",
          "name": "get_precision",
          "parameters": [],
          "returns": {
            "description": "Estimated precision of data. '",
            "name": "precision",
            "shape": "n_features, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Transform data back to its original space.\n\nIn other words, return an input X_original whose transform would be X.\n",
          "id": "sklearn.decomposition.pca.PCA.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "New data, where n_samples is the number of samples and n_components is the number of components. ",
              "name": "X",
              "shape": "n_samples, n_components",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Notes ----- If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening. '",
            "name": "X_original array-like, shape (n_samples, n_features)"
          }
        },
        {
          "description": "'Return the average log-likelihood of all samples.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n",
          "id": "sklearn.decomposition.pca.PCA.score",
          "name": "score",
          "parameters": [
            {
              "description": "The data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Average log-likelihood of the samples under the current model '",
            "name": "ll: float"
          }
        },
        {
          "description": "'Return the log-likelihood of each sample.\n\nSee. \"Pattern Recognition and Machine Learning\"\nby C. Bishop, 12.2.1 p. 574\nor http://www.miketipping.com/papers/met-mppca.pdf\n",
          "id": "sklearn.decomposition.pca.PCA.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "The data. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Log-likelihood of each sample under the current model '",
            "name": "ll: array, shape (n_samples,)"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.decomposition.pca.PCA.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Apply dimensionality reduction to X.\n\nX is projected on the first principal components previously extracted\nfrom a training set.\n",
          "id": "sklearn.decomposition.pca.PCA.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "New data, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": " Examples --------  >>> import numpy as np >>> from sklearn.decomposition import IncrementalPCA >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) >>> ipca = IncrementalPCA(n_components=2, batch_size=3) >>> ipca.fit(X) IncrementalPCA(batch_size=3, copy=True, n_components=2, whiten=False) >>> ipca.transform(X) # doctest: +SKIP '",
            "name": "X_new",
            "shape": "n_samples, n_components",
            "type": "array-like"
          }
        }
      ],
      "name": "sklearn.decomposition.pca.PCA",
      "parameters": [
        {
          "description": "Number of components to keep. if n_components is not set all components are kept::  n_components == min(n_samples, n_features)  if n_components == \\'mle\\' and svd_solver == \\'full\\', Minka\\'s MLE is used to guess the dimension if ``0 < n_components < 1`` and svd_solver == \\'full\\', select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components n_components cannot be equal to n_features for svd_solver == \\'arpack\\'. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "When True (False by default) the `components_` vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances.  Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. ",
          "name": "whiten",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "auto : the solver is selected by a default policy based on `X.shape` and `n_components`: if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient \\'randomized\\' method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards. full : run exact full SVD calling the standard LAPACK solver via `scipy.linalg.svd` and select the components by postprocessing arpack : run SVD truncated to n_components calling ARPACK solver via `scipy.sparse.linalg.svds`. It requires strictly 0 < n_components < X.shape[1] randomized : run randomized SVD by the method of Halko et al.  .. versionadded:: 0.18.0 ",
          "name": "svd_solver",
          "type": "string"
        },
        {
          "description": "Tolerance for singular values computed by svd_solver == \\'arpack\\'.  .. versionadded:: 0.18.0 ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of iterations for the power method computed by svd_solver == \\'randomized\\'.  .. versionadded:: 0.18.0 ",
          "name": "iterated_power",
          "type": "int"
        },
        {
          "description": "Pseudo Random Number generator seed control. If None, use the numpy.random singleton. Used by svd_solver == \\'arpack\\' or \\'randomized\\'.  .. versionadded:: 0.18.0 ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/decomposition/pca.pyc:106",
      "tags": [
        "decomposition",
        "pca"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel.  ``coef_`` is a readonly property derived from ``raw_coef_`` that follows the internal memory layout of liblinear. ",
          "name": "coef_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "1",
          "type": "array"
        }
      ],
      "category": "svm.classes",
      "common_name": "Linear SVC",
      "description": "'Linear Support Vector Classification.\n\nSimilar to SVC with parameter kernel=\\'linear\\', but implemented in terms of\nliblinear rather than libsvm, so it has more flexibility in the choice of\npenalties and loss functions and should scale better to large numbers of\nsamples.\n\nThis class supports both dense and sparse input and the multiclass support\nis handled according to a one-vs-the-rest scheme.\n\nRead more in the :ref:`User Guide <svm_classification>`.\n",
      "id": "sklearn.svm.classes.LinearSVC",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.svm.classes.LinearSVC.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.svm.classes.LinearSVC.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit the model according to the given training data.\n",
          "id": "sklearn.svm.classes.LinearSVC.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples in the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target vector relative to X ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.svm.classes.LinearSVC.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.svm.classes.LinearSVC.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.svm.classes.LinearSVC.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.svm.classes.LinearSVC.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.svm.classes.LinearSVC.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.svm.classes.LinearSVC.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.svm.classes.LinearSVC.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.svm.classes.LinearSVC",
      "parameters": [
        {
          "default": "1.0",
          "description": "Penalty parameter C of the error term. ",
          "name": "C",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Specifies the loss function. \\'hinge\\' is the standard SVM loss (used e.g. by the SVC class) while \\'squared_hinge\\' is the square of the hinge loss. ",
          "name": "loss",
          "type": "string"
        },
        {
          "description": "Specifies the norm used in the penalization. The \\'l2\\' penalty is the standard used in SVC. The \\'l1\\' leads to ``coef_`` vectors that are sparse. ",
          "name": "penalty",
          "type": "string"
        },
        {
          "description": "Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. ",
          "name": "dual",
          "type": "bool"
        },
        {
          "default": "1e-4",
          "description": "Tolerance for stopping criteria.  multi_class: string, \\'ovr\\' or \\'crammer_singer\\' (default=\\'ovr\\') Determines the multi-class strategy if `y` contains more than two classes. ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while ``\"crammer_singer\"`` optimizes a joint objective over all classes. While `crammer_singer` is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual will be ignored. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). ",
          "name": "fit_intercept",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "1",
          "description": "When self.fit_intercept is True, instance vector x becomes ``[x, self.intercept_scaling]``, i.e. a \"synthetic\" feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. ",
          "name": "intercept_scaling",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Set the parameter C of class i to ``class_weight[i]*C`` for SVC. If not given, all classes are supposed to have weight one. The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict, \\'balanced\\'"
        },
        {
          "description": "Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The maximum number of iterations to be run. ",
          "name": "max_iter",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/svm/classes.pyc:14",
      "tags": [
        "svm",
        "classes"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.random.choice",
      "description": "\"\nchoice(a, size=None, replace=True, p=None)\n\nGenerates a random sample from a given 1-D array\n\n.. versionadded:: 1.7.0\n",
      "id": "sklearn.utils.random.choice",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.random.choice",
      "parameters": [
        {
          "description": "If an ndarray, a random sample is generated from its elements. If an int, the random sample is generated as if a was np.arange(n) ",
          "name": "a",
          "type": ""
        },
        {
          "description": "Output shape. Default is None, in which case a single value is returned. ",
          "name": "size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether the sample is with or without replacement. ",
          "name": "replace",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The probabilities associated with each entry in a. If not given the sample assumes a uniform distribution over all entries in a. ",
          "name": "p",
          "optional": "true",
          "type": ""
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`.  ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated random samples  Raises ------- ValueError If a is an int and less than zero, if a or p are not 1-dimensional, if a is an array-like of size 0, if p is not a vector of probabilities, if a and p have different lengths, or if replace=False and the sample size is greater than the population size  See Also --------- randint, shuffle, permutation  Examples --------- Generate a uniform random sample from np.arange(5) of size 3:  >>> np.random.choice(5, 3)  # doctest: +SKIP array([0, 3, 4]) >>> #This is equivalent to np.random.randint(0,5,3)  Generate a non-uniform random sample from np.arange(5) of size 3:  >>> np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0])  # doctest: +SKIP array([3, 3, 0])  Generate a uniform random sample from np.arange(5) of size 3 without replacement:  >>> np.random.choice(5, 3, replace=False)  # doctest: +SKIP array([3,1,0]) >>> #This is equivalent to np.random.shuffle(np.arange(5))[:3]  Generate a non-uniform random sample from np.arange(5) of size 3 without replacement:  >>> np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0]) ... # doctest: +SKIP array([2, 3, 0])  Any of the above can be repeated with an arbitrary array-like instead of just integers. For instance:  >>> aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher'] >>> np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3]) ... # doctest: +SKIP array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], dtype='|S11')  \"",
        "name": "samples",
        "shape": "size,",
        "type": ""
      },
      "tags": [
        "utils",
        "random"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_X_y",
      "description": "'Input validation for standard estimators.\n\nChecks X and y for consistent length, enforces X 2d and y 1d.\nStandard input checks are only applied to y, such as checking that y\ndoes not have np.nan or np.inf targets. For multi-label y, set\nmulti_output=True to allow 2d and sparse y.  If the dtype of X is\nobject, attempt converting to float, raising on failure.\n",
      "id": "sklearn.utils.validation.check_X_y",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_X_y",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "type": "nd-array"
        },
        {
          "description": "Labels. ",
          "name": "y",
          "type": "nd-array"
        },
        {
          "description": "String[s] representing allowed sparse matrix formats, such as \\'csc\\', \\'csr\\', etc.  None means that sparse matrix input will raise an error. If the input is sparse but not in the allowed format, it will be converted to the first listed format. ",
          "name": "accept_sparse",
          "type": "string"
        },
        {
          "description": "Data type of result. If None, the dtype of the input is preserved. If \"numeric\", dtype is preserved unless array.dtype is object. If dtype is a list of types, conversion on the first type is only performed if the dtype of the input is not in the list. ",
          "name": "dtype",
          "type": "string"
        },
        {
          "description": "Whether an array will be forced to be fortran or c-style. ",
          "name": "order",
          "type": ""
        },
        {
          "description": "Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion. ",
          "name": "copy",
          "type": "boolean"
        },
        {
          "description": "Whether to raise an error on np.inf and np.nan in X. This parameter does not influence whether y can have np.inf or np.nan values.  ensure_2d : boolean (default=True) Whether to make X at least 2d. ",
          "name": "force_all_finite",
          "type": "boolean"
        },
        {
          "description": "Whether to allow X.ndim > 2. ",
          "name": "allow_nd",
          "type": "boolean"
        },
        {
          "description": "Whether to allow 2-d y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True. ",
          "name": "multi_output",
          "type": "boolean"
        },
        {
          "description": "Make sure that X has a minimum number of samples in its first axis (rows for a 2D array). ",
          "name": "ensure_min_samples",
          "type": "int"
        },
        {
          "description": "Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0 disables this check. ",
          "name": "ensure_min_features",
          "type": "int"
        },
        {
          "description": "Whether to ensure that y has a numeric type. If dtype of y is object, it is converted to float64. Should only be used for regression algorithms. ",
          "name": "y_numeric",
          "type": "boolean"
        },
        {
          "description": "Raise DataConversionWarning if the dtype of the input data structure does not match the requested dtype, causing a memory copy. ",
          "name": "warn_on_dtype",
          "type": "boolean"
        },
        {
          "description": "If passed, include the name of the estimator in warning messages. ",
          "name": "estimator",
          "type": "str"
        }
      ],
      "returns": {
        "description": "The converted and validated X.  y_converted : object The converted and validated y. '",
        "name": "X_converted",
        "type": "object"
      },
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.mutual_info_.mutual_info_classif",
      "description": "'Estimate mutual information for a discrete target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.mutual_info_.mutual_info_classif",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.mutual_info_.mutual_info_classif",
      "parameters": [
        {
          "description": "Feature matrix. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Target vector. ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If \\'auto\\', it is assigned to False for dense `X` and to True for sparse `X`. ",
          "name": "discrete_features",
          "type": "\\'auto\\', bool, array_like"
        },
        {
          "description": "Number of neighbors to use for MI estimation for continuous variables, see [2]_ and [3]_. Higher values reduce variance of the estimation, but could introduce a bias. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "Whether to make a copy of the given data. If set to False, the initial data will be overwritten. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Estimated mutual information between each feature and the target.  Notes ----- 1. The term \"discrete features\" is used instead of naming them \"categorical\", because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that. 2. True mutual information can\\'t be negative. If its estimate turns out to be negative, it is replaced by zero.  References ---------- .. [1] `Mutual Information <https://en.wikipedia.org/wiki/Mutual_information>`_ on Wikipedia. .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual information\". Phys. Rev. E 69, 2004. .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous Data Sets\". PLoS ONE 9(2), 2014. .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16 '",
        "name": "mi",
        "shape": "n_features,",
        "type": "ndarray"
      },
      "tags": [
        "feature_selection",
        "mutual_info_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._validation.permutation_test_score",
      "description": "\"Evaluate the significance of a cross-validated score with permutations\n\nRead more in the :ref:`User Guide <cross_validation>`.\n",
      "id": "sklearn.model_selection._validation.permutation_test_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._validation.permutation_test_score",
      "parameters": [
        {
          "description": "The object to use to fit the data.  X : array-like of shape at least 2D The data to fit. ",
          "name": "estimator",
          "type": "estimator"
        },
        {
          "description": "The target variable to try to predict in the case of supervised learning. ",
          "name": "y",
          "type": "array-like"
        },
        {
          "description": "Labels to constrain permutation within groups, i.e. ``y`` values are permuted among samples with the same group identifier. When not specified, ``y`` values are permuted among all samples.  When a grouped cross-validator is used, the group labels are also passed on to the ``split`` method of the cross-validator. The cross-validator uses them for grouping the samples  while splitting the dataset into train/test set. ",
          "name": "groups",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of times to permute ``y``. ",
          "name": "n_permutations",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The number of CPUs to use to do the computation. -1 means 'all CPUs'. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "A random number generator instance to define the state of the random permutations generator. ",
          "name": "random_state",
          "type": ""
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "The true score without permuting targets.  permutation_scores : array, shape (n_permutations,) The scores obtained for each permutations.  pvalue : float The returned value equals p-value if `scoring` returns bigger numbers for better scores (e.g., accuracy_score). If `scoring` is rather a loss function (i.e. when lower is better such as with `mean_squared_error`) then this is actually the complement of the p-value:  1 - p-value.  Notes ----- This function implements Test 1 in:  Ojala and Garriga. Permutation Tests for Studying Classifier Performance.  The Journal of Machine Learning Research (2010) vol. 11  \"",
        "name": "score",
        "type": "float"
      },
      "tags": [
        "model_selection",
        "_validation"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.classification",
      "common_name": "Radius Neighbors Classifier",
      "description": "\"Classifier implementing a vote among neighbors within a given radius\n\nRead more in the :ref:`User Guide <classification>`.\n",
      "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data and y as target values\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. ",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            },
            {
              "description": "Target values of shape = [n_samples] or [n_samples, n_outputs]  \"",
              "name": "y",
              "type": "array-like, sparse matrix"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict the class labels for the provided data\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Class labels for each data sample.  \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "\"Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.radius_neighbors",
          "name": "radius_neighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "type": "array-like"
            },
            {
              "description": "Limiting distance of neighbors to return. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the ``metric`` constructor parameter.  ind : array, shape (n_samples,) of arrays An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size ``radius`` around the query points.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1, 1, 1]:  >>> import numpy as np >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.6) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> rng = neigh.radius_neighbors([[1., 1., 1.]]) >>> print(np.asarray(rng[0][0])) # doctest: +ELLIPSIS [ 1.5  0.5] >>> print(np.asarray(rng[1][0])) # doctest: +ELLIPSIS [1 2]  The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.  Notes ----- Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, `radius_neighbors` returns arrays of objects, where each object is a 1D array of indices or distances. \"",
            "name": "dist",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.radius_neighbors_graph",
          "name": "radius_neighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Radius of neighborhoods. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.5) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.radius_neighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples",
            "type": "sparse"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.classification.RadiusNeighborsClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.classification.RadiusNeighborsClassifier",
      "parameters": [
        {
          "description": "Range of parameter space to use by default for :meth`radius_neighbors` queries. ",
          "name": "radius",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "weight function used in prediction.  Possible values:  - 'uniform' : uniform weights.  All points in each neighborhood are weighted equally. - 'distance' : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. - [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.  Uniform weights are used by default. ",
          "name": "weights",
          "type": "str"
        },
        {
          "description": "Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDtree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ",
          "name": "algorithm",
          "optional": "true",
          "type": "'auto', 'ball_tree', 'kd_tree', 'brute'"
        },
        {
          "description": "Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ",
          "name": "p",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. ",
          "name": "outlier_label",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Additional keyword arguments for the metric function. ",
          "name": "metric_params",
          "optional": "true",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/classification.pyc:227",
      "tags": [
        "neighbors",
        "classification"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features. ",
          "name": "coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "The intercept term. ",
          "name": "intercept_",
          "shape": "1,",
          "type": "array"
        },
        {
          "description": "Averaged weights assigned to the features. ",
          "name": "average_coef_",
          "shape": "n_features,",
          "type": "array"
        },
        {
          "description": "The averaged intercept term. ",
          "name": "average_intercept_",
          "shape": "1,",
          "type": "array"
        }
      ],
      "category": "linear_model.stochastic_gradient",
      "common_name": "SGD Regressor",
      "description": "\"Linear model fitted by minimizing a regularized empirical loss with SGD\n\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is\nestimated each sample at a time and the model is updated along the way with\na decreasing strength schedule (aka learning rate).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nThis implementation works with data represented as dense numpy arrays of\nfloating point values for the features.\n\nRead more in the :ref:`User Guide <sgd>`.\n",
      "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'DEPRECATED:  and will be removed in 0.19.\n\nPredict using the linear model\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_features,",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "1,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples (1. for unweighted). ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict using the linear model\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted target values per element in X. '",
            "name": "array, shape (n_samples,)"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.linear_model.stochastic_gradient.SGDRegressor",
      "parameters": [
        {
          "description": "The loss function to be used. Defaults to 'squared_loss' which refers to the ordinary least squares fit. 'huber' modifies 'squared_loss' to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. 'epsilon_insensitive' ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. 'squared_epsilon_insensitive' is the same but becomes squared loss past a tolerance of epsilon. ",
          "name": "loss",
          "type": "str"
        },
        {
          "description": "The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. ",
          "name": "penalty",
          "type": "str"
        },
        {
          "description": "Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to 'optimal'.  l1_ratio : float The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The number of passes over the training data (aka epochs). The number of iterations is set to 1 if using partial_fit. Defaults to 5. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether or not the training data should be shuffled after each epoch. Defaults to True. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is 'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For 'huber', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. ",
          "name": "epsilon",
          "type": "float"
        },
        {
          "description": "The learning rate schedule:  - 'constant': eta = eta0 - 'optimal': eta = 1.0 / (alpha * (t + t0)) [default] - 'invscaling': eta = eta0 / pow(t, power_t)  where t0 is chosen by a heuristic proposed by Leon Bottou.  eta0 : double, optional The initial learning rate [default 0.01]. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The exponent for inverse scaling learning rate [default 0.25]. ",
          "name": "power_t",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So ``average=10`` will begin averaging after seeing 10 samples. ",
          "name": "average",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc:1096",
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "A mapping of terms to feature indices. ",
          "name": "vocabulary_",
          "type": "dict"
        },
        {
          "description": "The learned idf vector (global term weights) when ``use_idf`` is set to True, None otherwise. ",
          "name": "idf_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Terms that were ignored because they either:  - occurred in too many documents (`max_df`) - occurred in too few documents (`min_df`) - were cut off by feature selection (`max_features`).  This is only available if no vocabulary was given.  See also -------- CountVectorizer Tokenize the documents and count the occurrences of token and return them as a sparse matrix  TfidfTransformer Apply Term Frequency Inverse Document Frequency normalization to a sparse matrix of occurrence counts. ",
          "name": "stop_words_",
          "type": "set"
        }
      ],
      "category": "feature_extraction.text",
      "common_name": "Tfidf Vectorizer",
      "description": "u'Convert a collection of raw documents to a matrix of TF-IDF features.\n\nEquivalent to CountVectorizer followed by TfidfTransformer.\n\nRead more in the :ref:`User Guide <text_feature_extraction>`.\n",
      "id": "sklearn.feature_extraction.text.TfidfVectorizer",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "u'Return a callable that handles preprocessing and tokenization'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.build_analyzer",
          "name": "build_analyzer",
          "parameters": []
        },
        {
          "description": "u'Return a function to preprocess the text before tokenization'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.build_preprocessor",
          "name": "build_preprocessor",
          "parameters": []
        },
        {
          "description": "u'Return a function that splits a string into a sequence of tokens'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.build_tokenizer",
          "name": "build_tokenizer",
          "parameters": []
        },
        {
          "description": "u'Decode the input into a string of unicode symbols\n\nThe decoding strategy depends on the vectorizer parameters.\n'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.decode",
          "name": "decode",
          "parameters": []
        },
        {
          "description": "u'Learn vocabulary and idf from training set.\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "an iterable which yields either str, unicode or file objects ",
              "name": "raw_documents",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": ""
          }
        },
        {
          "description": "u'Learn vocabulary and idf, return term-document matrix.\n\nThis is equivalent to fit followed by transform, but more efficiently\nimplemented.\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "an iterable which yields either str, unicode or file objects ",
              "name": "raw_documents",
              "type": "iterable"
            }
          ],
          "returns": {
            "description": "Tf-idf-weighted document-term matrix. '",
            "name": "X",
            "type": "sparse"
          }
        },
        {
          "description": "u'Array mapping from feature integer indices to feature name'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names",
          "name": "get_feature_names",
          "parameters": []
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "u'Build or fetch the effective stop words list'",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.get_stop_words",
          "name": "get_stop_words",
          "parameters": []
        },
        {
          "description": "u'Return terms per document with nonzero entries in X.\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.inverse_transform",
          "name": "inverse_transform",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array, sparse matrix"
            }
          ],
          "returns": {
            "description": "List of arrays of terms. '",
            "name": "X_inv",
            "type": "list"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "u'Transform documents to document-term matrix.\n\nUses the vocabulary and document frequencies (df) learned by fit (or\nfit_transform).\n",
          "id": "sklearn.feature_extraction.text.TfidfVectorizer.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "an iterable which yields either str, unicode or file objects ",
              "name": "raw_documents",
              "type": "iterable"
            },
            {
              "description": "Whether to copy X and operate on the copy or perform in-place operations. ",
              "name": "copy",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Tf-idf-weighted document-term matrix. '",
            "name": "X",
            "type": "sparse"
          }
        }
      ],
      "name": "sklearn.feature_extraction.text.TfidfVectorizer",
      "parameters": [
        {
          "description": "If \\'filename\\', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze.  If \\'file\\', the sequence items must have a \\'read\\' method (file-like object) that is called to fetch the bytes in memory.  Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. ",
          "name": "input",
          "type": "string"
        },
        {
          "description": "If bytes or files are given to analyze, this encoding is used to decode. ",
          "name": "encoding",
          "type": "string"
        },
        {
          "description": "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is \\'strict\\', meaning that a UnicodeDecodeError will be raised. Other values are \\'ignore\\' and \\'replace\\'. ",
          "name": "decode_error",
          "type": "\\'strict\\', \\'ignore\\', \\'replace\\'"
        },
        {
          "description": "Remove accents during the preprocessing step. \\'ascii\\' is a fast method that only works on characters that have an direct ASCII mapping. \\'unicode\\' is a slightly slower method that works on any characters. None (default) does nothing. ",
          "name": "strip_accents",
          "type": "\\'ascii\\', \\'unicode\\', None"
        },
        {
          "description": "Whether the feature should be made of word or character n-grams.  If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. ",
          "name": "analyzer",
          "type": "string"
        },
        {
          "description": "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. ",
          "name": "preprocessor",
          "type": "callable"
        },
        {
          "description": "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == \\'word\\'``. ",
          "name": "tokenizer",
          "type": "callable"
        },
        {
          "description": "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. ",
          "name": "ngram_range",
          "type": "tuple"
        },
        {
          "description": "If a string, it is passed to _check_stop_list and the appropriate stop list is returned. \\'english\\' is currently the only supported string value.  If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == \\'word\\'``.  If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. ",
          "name": "stop_words",
          "type": "string"
        },
        {
          "description": "Convert all characters to lowercase before tokenizing. ",
          "name": "lowercase",
          "type": "boolean"
        },
        {
          "description": "Regular expression denoting what constitutes a \"token\", only used if ``analyzer == \\'word\\'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). ",
          "name": "token_pattern",
          "type": "string"
        },
        {
          "description": "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. ",
          "name": "max_df",
          "type": "float"
        },
        {
          "description": "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. ",
          "name": "min_df",
          "type": "float"
        },
        {
          "description": "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.  This parameter is ignored if vocabulary is not None. ",
          "name": "max_features",
          "type": "int"
        },
        {
          "description": "Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. ",
          "name": "vocabulary",
          "optional": "true",
          "type": ""
        },
        {
          "description": "If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.) ",
          "name": "binary",
          "type": "boolean"
        },
        {
          "description": "Type of the matrix returned by fit_transform() or transform(). ",
          "name": "dtype",
          "optional": "true",
          "type": "type"
        },
        {
          "description": "Norm used to normalize term vectors. None for no normalization. ",
          "name": "norm",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Enable inverse-document-frequency reweighting. ",
          "name": "use_idf",
          "type": "boolean"
        },
        {
          "description": "Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions. ",
          "name": "smooth_idf",
          "type": "boolean"
        },
        {
          "description": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf). ",
          "name": "sublinear_tf",
          "type": "boolean"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc:1096",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "Hash function g(p,x) for a tree is an array of 32 randomly generated float arrays with the same dimension as the data set. This array is stored in GaussianRandomProjectionHash object and can be obtained from ``components_`` attribute. ",
          "name": "hash_functions_",
          "type": "list"
        },
        {
          "description": "Each tree (corresponding to a hash function) contains an array of sorted hashed values. The array representation may change in future versions. ",
          "name": "trees_",
          "shape": "n_estimators, n_samples",
          "type": "array"
        },
        {
          "description": "Original indices of sorted hashed values in the fitted index.  References ----------  .. [1] M. Bawa, T. Condie and P. Ganesan, \"LSH Forest: Self-Tuning Indexes for Similarity Search\", WWW \\'05 Proceedings of the 14th international conference on World Wide Web,  651-660, 2005. ",
          "name": "original_indices_",
          "shape": "n_estimators, n_samples",
          "type": "array"
        }
      ],
      "category": "neighbors.approximate",
      "common_name": "LSH Forest",
      "description": "'Performs approximate nearest neighbor search using LSH forest.\n\nLSH Forest: Locality Sensitive Hashing forest [1] is an alternative\nmethod for vanilla approximate nearest neighbor search methods.\nLSH forest data structure has been implemented using sorted\narrays and binary search and 32 bit fixed-length hashes.\nRandom projection is used as the hash family which approximates\ncosine distance.\n\nThe cosine distance is defined as ``1 - cosine_similarity``: the lowest\nvalue is 0 (identical point) but it is bounded above by 2 for the farthest\npoints. Its value does not depend on the norm of the vector points but\nonly on their relative angles.\n\nRead more in the :ref:`User Guide <approximate_nearest_neighbors>`.\n",
      "id": "sklearn.neighbors.approximate.LSHForest",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit the LSH forest on the data.\n\nThis creates binary hashes of input data points by getting the\ndot product of input points and hash_function then\ntransforming the projection into a binary string array based\non the sign (positive/negative) of the projection.\nA sorted array of binary hashes is created.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns n_neighbors of approximate nearest neighbors.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.kneighbors",
          "name": "kneighbors",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points.  Each row corresponds to a single query. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Number of neighbors required. If not provided, this will return the number specified at the initialization. ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Returns the distances of neighbors if set to True. ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the cosine distances to each point, only present if return_distance=True.  ind : array, shape (n_samples, n_neighbors) Indices of the approximate nearest points in the population matrix. '",
            "name": "dist",
            "shape": "n_samples, n_neighbors",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n",
          "id": "sklearn.neighbors.approximate.LSHForest.kneighbors_graph",
          "name": "kneighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors for each sample. (default is value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=2) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.kneighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- NearestNeighbors.radius_neighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples_fit",
            "type": "sparse"
          }
        },
        {
          "description": "'\nInserts new data into the already fitted LSH Forest.\nCost is proportional to new total size, so additions\nshould be batched.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "New data point to be inserted into the LSH Forest. '",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            }
          ]
        },
        {
          "description": "'Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of some points from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n\nLSH Forest being an approximate method, some true neighbors from the\nindexed dataset might be missing from the results.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.radius_neighbors",
          "name": "radius_neighbors",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single query. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Limiting distance of neighbors to return. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Returns the distances of neighbors if set to True. ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Each element is an array representing the cosine distances to some points found within ``radius`` of the respective query. Only present if ``return_distance=True``.  ind : array, shape (n_samples,) of arrays Each element is an array of indices for neighbors within ``radius`` of the respective query. '",
            "name": "dist",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.radius_neighbors_graph",
          "name": "radius_neighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Radius of neighborhoods. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.5) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.radius_neighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples",
            "type": "sparse"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.approximate.LSHForest.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.approximate.LSHForest",
      "parameters": [
        {
          "description": "Number of trees in the LSH Forest. ",
          "name": "n_estimators",
          "type": "int"
        },
        {
          "description": "lowest hash length to be searched when candidate selection is performed for nearest neighbors. ",
          "name": "min_hash_match",
          "type": "int"
        },
        {
          "description": "Minimum number of candidates evaluated per estimator, assuming enough items meet the `min_hash_match` constraint. ",
          "name": "n_candidates",
          "type": "int"
        },
        {
          "description": "Number of neighbors to be returned from query function when it is not provided to the :meth:`kneighbors` method. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "Radius from the data point to its neighbors. This is the parameter space to use by default for the :meth`radius_neighbors` queries. ",
          "name": "radius",
          "type": "float"
        },
        {
          "description": "A value ranges from 0 to 1. Radius neighbors will be searched until the ratio between total neighbors within the radius and the total candidates becomes less than this value unless it is terminated by hash length reaching `min_hash_match`. ",
          "name": "radius_cutoff_ratio",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/approximate.pyc:110",
      "tags": [
        "neighbors",
        "approximate"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The weights of each mixture components. ",
          "name": "weights_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "The mean of each mixture component. ",
          "name": "means_",
          "shape": "n_components, n_features",
          "type": "array-like"
        },
        {
          "description": "The covariance of each mixture component. The shape depends on `covariance_type`::  (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full' ",
          "name": "covariances_",
          "type": "array-like"
        },
        {
          "description": "The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on `covariance_type`::  (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full' ",
          "name": "precisions_",
          "type": "array-like"
        },
        {
          "description": "The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on `covariance_type`::  (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full' ",
          "name": "precisions_cholesky_",
          "type": "array-like"
        },
        {
          "description": "True when convergence was reached in fit(), False otherwise. ",
          "name": "converged_",
          "type": "bool"
        },
        {
          "description": "Number of step used by the best fit of EM to reach the convergence. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "Log-likelihood of the best fit of EM.  See Also -------- BayesianGaussianMixture : Gaussian mixture model fit with a variational inference.",
          "name": "lower_bound_",
          "type": "float"
        }
      ],
      "category": "mixture.gaussian_mixture",
      "common_name": "Gaussian Mixture",
      "description": "\"Gaussian Mixture.\n\nRepresentation of a Gaussian mixture model probability distribution.\nThis class allows to estimate the parameters of a Gaussian mixture\ndistribution.\n\n.. versionadded:: 0.18\n*GaussianMixture*.\n\nRead more in the :ref:`User Guide <gmm>`.\n",
      "id": "sklearn.mixture.gaussian_mixture.GaussianMixture",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Akaike information criterion for the current model on the input X.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.aic",
          "name": "aic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The lower the better. '",
            "name": "aic: float"
          }
        },
        {
          "description": "'Bayesian information criterion for the current model on the input X.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.bic",
          "name": "bic",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The lower the better. '",
            "name": "bic: float"
          }
        },
        {
          "description": "'Estimate model parameters with the EM algorithm.\n\nThe method fit the model `n_init` times and set the parameters with\nwhich the model has the largest likelihood or lower bound. Within each\ntrial, the method iterates between E-step and M-step for `max_iter`\ntimes until the change of likelihood or lower bound is less than\n`tol`, otherwise, a `ConvergenceWarning` is raised.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the labels for the data samples in X using trained model.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Component labels. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data per each component.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "resp",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        },
        {
          "description": "'Generate random samples from the fitted Gaussian distribution.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "Randomly generated sample  y : array, shape (nsamples,) Component labels  '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the per-sample average log-likelihood of the given data X.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log likelihood of the Gaussian mixture given X. '",
            "name": "log_likelihood",
            "type": "float"
          }
        },
        {
          "description": "'Compute the weighted log probabilities for each sample.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X. '",
            "name": "log_prob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.gaussian_mixture.GaussianMixture.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.gaussian_mixture.GaussianMixture",
      "parameters": [
        {
          "description": "The number of mixture components. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "defaults to 'full'. String describing the type of covariance parameters to use. Must be one of::  'full' (each component has its own general covariance matrix), 'tied' (all components share the same general covariance matrix), 'diag' (each component has its own diagonal covariance matrix), 'spherical' (each component has its own single variance). ",
          "name": "covariance_type",
          "type": "'full', 'tied', 'diag', 'spherical'"
        },
        {
          "description": "The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive. ",
          "name": "reg_covar",
          "type": "float"
        },
        {
          "description": "The number of EM iterations to perform. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "The number of initializations to perform. The best results are kept. ",
          "name": "n_init",
          "type": "int"
        },
        {
          "description": "The method used to initialize the weights, the means and the precisions. Must be one of::  'kmeans' : responsibilities are initialized using kmeans. 'random' : responsibilities are initialized randomly. ",
          "name": "init_params",
          "type": "'kmeans', 'random'"
        },
        {
          "description": "The user-provided initial weights, defaults to None. If it None, weights are initialized using the `init_params` method.  means_init: array-like, shape (n_components, n_features), optional The user-provided initial means, defaults to None, If it None, means are initialized using the `init_params` method.  precisions_init: array-like, optional. The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the 'init_params' method. The shape depends on 'covariance_type'::  (n_components,)                        if 'spherical', (n_features, n_features)               if 'tied', (n_components, n_features)             if 'diag', (n_components, n_features, n_features) if 'full' ",
          "name": "weights_init",
          "optional": "true",
          "shape": "n_components, ",
          "type": "array-like"
        },
        {
          "description": "A random number generator instance. ",
          "name": "random_state",
          "type": ""
        },
        {
          "description": "If 'warm_start' is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several time on similar problems. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "description": "Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Number of iteration done before the next print. ",
          "name": "verbose_interval",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/gaussian_mixture.pyc:435",
      "tags": [
        "mixture",
        "gaussian_mixture"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Feature values in training data (also required for prediction) ",
          "name": "X_train_",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Target values in training data (also required for prediction) ",
          "name": "y_train_",
          "shape": "n_samples, [n_output_dims",
          "type": "array-like"
        },
        {
          "description": "The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters  L_ : array-like, shape = (n_samples, n_samples) Lower-triangular Cholesky decomposition of the kernel in ``X_train_`` ",
          "name": "kernel_",
          "type": "kernel"
        },
        {
          "description": "Dual coefficients of training data points in kernel space ",
          "name": "alpha_",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "The log-marginal-likelihood of ``self.kernel_.theta`` ",
          "name": "log_marginal_likelihood_value_",
          "type": "float"
        }
      ],
      "category": "gaussian_process.gpr",
      "common_name": "Gaussian Process Regressor",
      "description": "'Gaussian process regression (GPR).\n\nThe implementation is based on Algorithm 2.1 of Gaussian Processes\nfor Machine Learning (GPML) by Rasmussen and Williams.\n\nIn addition to standard scikit-learn estimator API,\nGaussianProcessRegressor:\n\n* allows prediction without prior fitting (based on the GP prior)\n* provides an additional method sample_y(X), which evaluates samples\ndrawn from the GPR (prior or posterior) at given inputs\n* exposes a method log_marginal_likelihood(theta), which can be used\nexternally for other ways of selecting hyperparameters, e.g., via\nMarkov chain Monte Carlo.\n\nRead more in the :ref:`User Guide <gaussian_process>`.\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Gaussian process regression model\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples, [n_output_dims",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns log-marginal likelihood of theta for training data.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.log_marginal_likelihood",
          "name": "log_marginal_likelihood",
          "parameters": [
            {
              "description": "Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of ``self.kernel_.theta`` is returned. ",
              "name": "theta",
              "shape": "n_kernel_params,",
              "type": "array-like"
            },
            {
              "description": "If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None. ",
              "name": "eval_gradient",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "Log-marginal likelihood of theta for training data.  log_likelihood_gradient : array, shape = (n_kernel_params,), optional Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True. '",
            "name": "log_likelihood",
            "type": "float"
          }
        },
        {
          "description": "'Predict using the Gaussian process regression model\n\nWe can also predict based on an unfitted model by using the GP prior.\nIn addition to the mean of the predictive distribution, also its\nstandard deviation (return_std=True) or covariance (return_cov=True).\nNote that at most one of the two can be requested.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Query points where the GP is evaluated ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean. ",
              "name": "return_std",
              "type": "bool"
            },
            {
              "description": "If True, the covariance of the joint predictive distribution at the query points is returned along with the mean ",
              "name": "return_cov",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "Mean of predictive distribution a query points  y_std : array, shape = (n_samples,), optional Standard deviation of predictive distribution at query points. Only returned when return_std is True.  y_cov : array, shape = (n_samples, n_samples), optional Covariance of joint predictive distribution a query points. Only returned when return_cov is True. '",
            "name": "y_mean",
            "shape": "n_samples, [n_output_dims",
            "type": "array"
          }
        },
        {
          "description": "'Draw samples from Gaussian process and evaluate at X.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.sample_y",
          "name": "sample_y",
          "parameters": [
            {
              "description": "Query points where the GP samples are evaluated ",
              "name": "X",
              "shape": "n_samples_X, n_features",
              "type": "array-like"
            },
            {
              "description": "The number of samples drawn from the Gaussian process  random_state: RandomState or an int seed (0 by default) A random number generator instance ",
              "name": "n_samples",
              "type": "int"
            }
          ],
          "returns": {
            "description": "Values of n_samples samples drawn from Gaussian process and evaluated at query points. '",
            "name": "y_samples",
            "shape": "n_samples_X, [n_output_dims",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.gaussian_process.gpr.GaussianProcessRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.gpr.GaussianProcessRegressor",
      "parameters": [
        {
          "description": "The kernel specifying the covariance function of the GP. If None is passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that the kernel\\'s hyperparameters are optimized during fitting. ",
          "name": "kernel",
          "type": "kernel"
        },
        {
          "description": "Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations and reduce potential numerical issue during fitting. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge. ",
          "name": "alpha",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Can either be one of the internally supported optimizers for optimizing the kernel\\'s parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature::  def optimizer(obj_func, initial_theta, bounds): # * \\'obj_func\\' is the objective function to be maximized, which #   takes the hyperparameters theta as parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * \\'initial_theta\\': the initial value for theta, which can be #   used by local optimizers # * \\'bounds\\': the bounds on the values of theta .... # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return theta_opt, func_min  Per default, the \\'fmin_l_bfgs_b\\' algorithm from scipy.optimize is used. If None is passed, the kernel\\'s parameters are kept fixed. Available internal optimizers are::  \\'fmin_l_bfgs_b\\' ",
          "name": "optimizer",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The number of restarts of the optimizer for finding the kernel\\'s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel\\'s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed. ",
          "name": "n_restarts_optimizer",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values\\' mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GP\\'s prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default.  copy_X_train : bool, optional (default: True) If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally. ",
          "name": "normalize_y",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/gpr.pyc:20",
      "tags": [
        "gaussian_process",
        "gpr"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Specified theta OR the best set of autocorrelation parameters (the         sought maximizer of the reduced likelihood function). ",
          "name": "theta_",
          "type": "array"
        },
        {
          "description": "The optimal reduced likelihood function value. ",
          "name": "reduced_likelihood_function_value_",
          "type": "array"
        }
      ],
      "category": "gaussian_process.gaussian_process",
      "common_name": "Gaussian Process",
      "description": "\"The legacy Gaussian Process model class.\n\n.. deprecated:: 0.18\nThis class will be removed in 0.20.\nUse the :class:`GaussianProcessRegressor` instead.\n\nRead more in the :ref:`User Guide <gaussian_process>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nThe Gaussian Process model fitting method.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "An array with shape (n_samples, n_features) with the input at which observations were made. ",
              "name": "X",
              "type": "double"
            },
            {
              "description": "An array with shape (n_samples, ) or shape (n_samples, n_targets) with the observations of the output to be predicted. ",
              "name": "y",
              "type": "double"
            }
          ],
          "returns": {
            "description": "A fitted Gaussian Process model object awaiting data to perform predictions. '",
            "name": "gp",
            "type": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'\nThis function evaluates the Gaussian Process model at x.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s) should be made.  eval_MSE : boolean, optional A boolean specifying whether the Mean Squared Error should be evaluated or not. Default assumes evalMSE = False and evaluates only the BLUP (mean prediction). ",
              "name": "X",
              "type": "array"
            },
            {
              "description": "An integer giving the maximum number of points that can be evaluated simultaneously (depending on the available memory). Default is None so that all given points are evaluated at the same time. ",
              "name": "batch_size",
              "optional": "true",
              "type": "integer"
            }
          ],
          "returns": {
            "description": "An array with shape (n_eval, ) if the Gaussian Process was trained on an array of shape (n_samples, ) or an array with shape (n_eval, n_targets) if the Gaussian Process was trained on an array of shape (n_samples, n_targets) with the Best Linear Unbiased Prediction at x.  MSE : array_like, optional (if eval_MSE == True) An array with shape (n_eval, ) or (n_eval, n_targets) as with y, with the Mean Squared Error at x. '",
            "name": "y",
            "shape": "n_samples, ",
            "type": "array"
          }
        },
        {
          "description": "'\nThis function determines the BLUP parameters and evaluates the reduced\nlikelihood function for the given autocorrelation parameters theta.\n\nMaximizing this function wrt the autocorrelation parameters theta is\nequivalent to maximizing the likelihood of the assumed joint Gaussian\ndistribution of the observations y evaluated onto the design of\nexperiments X.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.reduced_likelihood_function",
          "name": "reduced_likelihood_function",
          "parameters": [
            {
              "description": "An array containing the autocorrelation parameters at which the Gaussian Process model parameters should be determined. Default uses the built-in autocorrelation parameters (ie ``theta = self.theta_``). ",
              "name": "theta",
              "optional": "true",
              "type": "array"
            }
          ],
          "returns": {
            "description": "The value of the reduced likelihood function associated to the given autocorrelation parameters theta.  par : dict A dictionary containing the requested Gaussian Process model parameters:  sigma2 Gaussian Process variance. beta Generalized least-squares regression weights for Universal Kriging or given beta0 for Ordinary Kriging. gamma Gaussian Process weights. C Cholesky decomposition of the correlation matrix [R]. Ft Solution of the linear equation system : [R] x Ft = F G QR decomposition of the matrix Ft. '",
            "name": "reduced_likelihood_function_value",
            "type": "double"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.gaussian_process.gaussian_process.GaussianProcess.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.gaussian_process.GaussianProcess",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "description": "A regression function returning an array of outputs of the linear regression functional basis. The number of observations n_samples should be greater than the size p of this basis. Default assumes a simple constant regression trend. Available built-in regression models are::  'constant', 'linear', 'quadratic' ",
          "name": "regr",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "A stationary autocorrelation function returning the autocorrelation between two points x and x'. Default assumes a squared-exponential autocorrelation model. Built-in correlation models are::  'absolute_exponential', 'squared_exponential', 'generalized_exponential', 'cubic', 'linear'  beta0 : double array_like, optional The regression weight vector to perform Ordinary Kriging (OK). Default assumes Universal Kriging (UK) so that the vector beta of regression weights is estimated using the maximum likelihood principle. ",
          "name": "corr",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "A string specifying whether the Cholesky decomposition of the correlation matrix should be stored in the class (storage_mode = 'full') or not (storage_mode = 'light'). Default assumes storage_mode = 'full', so that the Cholesky decomposition of the correlation matrix is stored. This might be a useful parameter when one is not interested in the MSE and only plan to estimate the BLUP, for which the correlation matrix is not required. ",
          "name": "storage_mode",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "A boolean specifying the verbose level. Default is verbose = False.  theta0 : double array_like, optional An array with shape (n_features, ) or (1, ). The parameters in the autocorrelation model. If thetaL and thetaU are also specified, theta0 is considered as the starting point for the maximum likelihood estimation of the best set of parameters. Default assumes isotropic autocorrelation model with theta0 = 1e-1.  thetaL : double array_like, optional An array with shape matching theta0's. Lower bound on the autocorrelation parameters for maximum likelihood estimation. Default is None, so that it skips maximum likelihood estimation and it uses theta0.  thetaU : double array_like, optional An array with shape matching theta0's. Upper bound on the autocorrelation parameters for maximum likelihood estimation. Default is None, so that it skips maximum likelihood estimation and it uses theta0. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Input X and observations y are centered and reduced wrt means and standard deviations estimated from the n_samples observations provided. Default is normalize = True so that data is normalized to ease maximum likelihood estimation. ",
          "name": "normalize",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Introduce a nugget effect to allow smooth predictions from noisy data.  If nugget is an ndarray, it must be the same length as the number of data points used for the fit. The nugget is added to the diagonal of the assumed training covariance; in this way it acts as a Tikhonov regularization in the problem.  In the special case of the squared exponential correlation function, the nugget mathematically represents the variance of the input values. Default assumes a nugget close to machine precision for the sake of robustness (nugget = 10. * MACHINE_EPSILON). ",
          "name": "nugget",
          "optional": "true",
          "type": "double"
        },
        {
          "description": "A string specifying the optimization algorithm to be used. Default uses 'fmin_cobyla' algorithm from scipy.optimize. Available optimizers are::  'fmin_cobyla', 'Welch'  'Welch' optimizer is dued to Welch et al., see reference [WBSWM1992]_. It consists in iterating over several one-dimensional optimizations instead of running one single multi-dimensional optimization. ",
          "name": "optimizer",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The number of times the Maximum Likelihood Estimation should be performed from a random starting point. The first MLE always uses the specified starting point (theta0), the next starting points are picked at random according to an exponential distribution (log-uniform on [thetaL, thetaU]). Default does not use random starting point (random_start = 1).  random_state: integer or numpy.RandomState, optional The generator used to shuffle the sequence of coordinates of theta in the Welch optimizer. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.  ",
          "name": "random_start",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/gaussian_process.pyc:64",
      "tags": [
        "gaussian_process",
        "gaussian_process"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers. ",
          "name": "kernel_",
          "type": "kernel"
        },
        {
          "description": "The log-marginal-likelihood of ``self.kernel_.theta`` ",
          "name": "log_marginal_likelihood_value_",
          "type": "float"
        },
        {
          "description": "Unique class labels. ",
          "name": "classes_",
          "shape": "n_classes,",
          "type": "array-like"
        },
        {
          "description": "The number of classes in the training data  .. versionadded:: 0.18",
          "name": "n_classes_",
          "type": "int"
        }
      ],
      "category": "gaussian_process.gpc",
      "common_name": "Gaussian Process Classifier",
      "description": "'Gaussian process classification (GPC) based on Laplace approximation.\n\nThe implementation is based on Algorithm 3.1, 3.2, and 5.1 of\nGaussian Processes for Machine Learning (GPML) by Rasmussen and\nWilliams.\n\nInternally, the Laplace approximation is used for approximating the\nnon-Gaussian posterior by a Gaussian.\n\nCurrently, the implementation is restricted to using the logistic link\nfunction. For multi-class classification, several binary one-versus rest\nclassifiers are fitted. Note that this class thus does not implement\na true multi-class Laplace approximation.\n",
      "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Fit Gaussian process classification model\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values, must be binary ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Returns log-marginal likelihood of theta for training data.\n\nIn the case of multi-class classification, the mean log-marginal\nlikelihood of the one-versus-rest classifiers are returned.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.log_marginal_likelihood",
          "name": "log_marginal_likelihood",
          "parameters": [
            {
              "description": "Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the  hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of ``self.kernel_.theta`` is returned. ",
              "name": "theta",
              "shape": "n_kernel_params,",
              "type": "array-like"
            },
            {
              "description": "If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None. ",
              "name": "eval_gradient",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "Log-marginal likelihood of theta for training data.  log_likelihood_gradient : array, shape = (n_kernel_params,), optional Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True. '",
            "name": "log_likelihood",
            "type": "float"
          }
        },
        {
          "description": "'Perform classification on an array of test vectors X.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Predicted target values for X, values are from ``classes_`` '",
            "name": "C",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Return probability estimates for the test vector X.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute `classes_`. '",
            "name": "C",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.gaussian_process.gpc.GaussianProcessClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.gaussian_process.gpc.GaussianProcessClassifier",
      "parameters": [
        {
          "description": "The kernel specifying the covariance function of the GP. If None is passed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note that the kernel\\'s hyperparameters are optimized during fitting. ",
          "name": "kernel",
          "type": "kernel"
        },
        {
          "description": "Can either be one of the internally supported optimizers for optimizing the kernel\\'s parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the  signature::  def optimizer(obj_func, initial_theta, bounds): # * \\'obj_func\\' is the objective function to be maximized, which #   takes the hyperparameters theta as parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * \\'initial_theta\\': the initial value for theta, which can be #   used by local optimizers # * \\'bounds\\': the bounds on the values of theta .... # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return theta_opt, func_min  Per default, the \\'fmin_l_bfgs_b\\' algorithm from scipy.optimize is used. If None is passed, the kernel\\'s parameters are kept fixed. Available internal optimizers are::  \\'fmin_l_bfgs_b\\' ",
          "name": "optimizer",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The number of restarts of the optimizer for finding the kernel\\'s parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernel\\'s initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed. ",
          "name": "n_restarts_optimizer",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The maximum number of iterations in Newton\\'s method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results. ",
          "name": "max_iter_predict",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization.  copy_X_train : bool, optional (default: True) If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.  multi_class: string, default : \"one_vs_rest\" Specifies how multi-class classification problems are handled. Supported are \"one_vs_rest\" and \"one_vs_one\". In \"one_vs_rest\", one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In \"one_vs_one\", one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that \"one_vs_one\" does not support predicting probability estimates. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/gaussian_process/gpc.pyc:439",
      "tags": [
        "gaussian_process",
        "gpc"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_multilabel_classification",
      "description": "\"Generate a random multilabel classification problem.\n\nFor each sample, the generative process is:\n- pick the number of labels: n ~ Poisson(n_labels)\n- n times, choose a class c: c ~ Multinomial(theta)\n- pick the document length: k ~ Poisson(length)\n- k times, choose a word: w ~ Multinomial(theta_c)\n\nIn the above process, rejection sampling is used to make sure that\nn is never zero or more than `n_classes`, and that the document length\nis never zero. Likewise, we reject classes which have already been chosen.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_multilabel_classification",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_multilabel_classification",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "20",
          "description": "The total number of features. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "5",
          "description": "The number of classes of the classification problem. ",
          "name": "n_classes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with ``n_labels`` as its expected value, but samples are bounded (using rejection sampling) by ``n_classes``, and must be nonzero if ``allow_unlabeled`` is False. ",
          "name": "n_labels",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "50",
          "description": "The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value. ",
          "name": "length",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "True",
          "description": "If ``True``, some instances might not belong to any class. ",
          "name": "allow_unlabeled",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "False",
          "description": "If ``True``, return a sparse feature matrix  .. versionadded:: 0.17 parameter to allow *sparse* output. ",
          "name": "sparse",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If ``dense`` return ``Y`` in the dense binary indicator format. If ``'sparse'`` return ``Y`` in the sparse binary indicator format. ``False`` returns a list of lists of labels. ",
          "name": "return_indicator",
          "type": ""
        },
        {
          "default": "False",
          "description": "If ``True``, return the prior class probability and conditional probabilities of features given classes, from which the data was drawn. ",
          "name": "return_distributions",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated samples.  Y : array or sparse CSR matrix of shape [n_samples, n_classes] The label sets.  p_c : array, shape [n_classes] The probability of each class being drawn. Only returned if ``return_distributions=True``.  p_w_c : array, shape [n_features, n_classes] The probability of each feature being drawn given each class. Only returned if ``return_distributions=True``.  \"",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.svmlight_format.load_svmlight_files",
      "description": "'Load dataset from multiple files in SVMlight format\n\nThis function is equivalent to mapping load_svmlight_file over a list of\nfiles, except that the results are concatenated into a single, flat list\nand the samples vectors are constrained to all have the same number of\nfeatures.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n",
      "id": "sklearn.datasets.svmlight_format.load_svmlight_files",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.svmlight_format.load_svmlight_files",
      "parameters": [
        {
          "description": "(Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will be uncompressed on the fly. If an integer is passed, it is assumed to be a file descriptor. File-likes and file descriptors will not be closed by this function. File-like objects must be opened in binary mode. ",
          "name": "files",
          "type": "iterable"
        },
        {
          "description": "The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files.  This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised. ",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html) ",
          "name": "multilabel",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to \"auto\", a heuristic check is applied to determine this from the file contents. Both kinds of files occur \"in the wild\", but they are unfortunately not self-identifying. Using \"auto\" or True should always be safe. ",
          "name": "zero_based",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, will return the query_id array for each file. ",
          "name": "query_id",
          "type": "boolean"
        },
        {
          "description": "Data type of dataset to be loaded. This will be the data type of the output numpy arrays ``X`` and ``y``. ",
          "name": "dtype",
          "type": "numpy"
        }
      ],
      "returns": {
        "description": "where each (Xi, yi) pair is the result from load_svmlight_file(files[i]).  If query_id is set to True, this will return instead [X1, y1, q1, ..., Xn, yn, qn] where (Xi, yi, qi) is the result from load_svmlight_file(files[i])  Notes ----- When fitting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case if you load the files individually with load_svmlight_file.  See also -------- load_svmlight_file '",
        "name": "[X1, y1, ..., Xn, yn]"
      },
      "tags": [
        "datasets",
        "svmlight_format"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.robust_covariance.select_candidates",
      "description": "'Finds the best pure subset of observations to compute MCD from it.\n\nThe purpose of this function is to find the best sets of n_support\nobservations with respect to a minimization of their covariance\nmatrix determinant. Equivalently, it removes n_samples-n_support\nobservations to construct what we call a pure data set (i.e. not\ncontaining outliers). The list of the observations of the pure\ndata set is referred to as the `support`.\n\nStarting from a random support, the pure data set is found by the\nc_step procedure introduced by Rousseeuw and Van Driessen in\n[Rouseeuw1999]_.\n",
      "id": "sklearn.covariance.robust_covariance.select_candidates",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.robust_covariance.select_candidates",
      "parameters": [
        {
          "description": "Data (sub)set in which we look for the n_support purest observations. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "The number of samples the pure data set must contain. ",
          "name": "n_support",
          "type": "int"
        },
        {
          "description": "Number of best candidates results to return. ",
          "name": "select",
          "type": "int"
        },
        {
          "description": "Number of different initial sets of observations from which to run the algorithm. Instead of giving a number of trials to perform, one can provide a list of initial estimates that will be used to iteratively run c_step procedures. In this case: - n_trials[0]: array-like, shape (n_trials, n_features) is the list of `n_trials` initial location estimates - n_trials[1]: array-like, shape (n_trials, n_features, n_features) is the list of `n_trials` initial covariances estimates ",
          "name": "n_trials",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations for the c_step procedure. (2 is enough to be close to the final solution. \"Never\" exceeds 20). ",
          "name": "n_iter",
          "type": "int"
        },
        {
          "description": "The random generator used. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "type": "integer"
        },
        {
          "description": "The function which will be used to compute the covariance. Must return shape (n_features, n_features) ",
          "name": "cov_computation_method",
          "type": "callable"
        },
        {
          "description": "Control the output verbosity.  See Also --------- c_step ",
          "name": "verbose",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "The `select` location estimates computed from the `select` best supports found in the data set (`X`).  best_covariances : array-like, shape (select, n_features, n_features) The `select` covariance estimates computed from the `select` best supports found in the data set (`X`).  best_supports : array-like, shape (select, n_samples) The `select` best supports found in the data set (`X`).  References ---------- .. [Rouseeuw1999] A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS  '",
        "name": "best_locations",
        "shape": "select, n_features",
        "type": "array-like"
      },
      "tags": [
        "covariance",
        "robust_covariance"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.io.matlab.mio.loadmat",
      "description": "\"\nLoad MATLAB file\n",
      "id": "scipy.io.matlab.mio.loadmat",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.io.matlab.mio.loadmat",
      "parameters": [
        {
          "description": "Name of the mat file (do not need .mat extension if appendmat==True) Can also pass open file-like object.",
          "name": "file_name",
          "type": "str"
        },
        {
          "description": "Dictionary in which to insert matfile variables.",
          "name": "m_dict",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "True to append the .mat extension to the end of the given filename, if not already present.",
          "name": "appendmat",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "None by default, implying byte order guessed from mat file. Otherwise can be one of ('native', '=', 'little', '<', 'BIG', '>').",
          "name": "byte_order",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "If True, return arrays in same dtype as would be loaded into MATLAB (instead of the dtype with which they are saved).",
          "name": "mat_dtype",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to squeeze unit matrix dimensions or not.",
          "name": "squeeze_me",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to convert char arrays to string arrays.",
          "name": "chars_as_strings",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Returns matrices as would be loaded by MATLAB (implies squeeze_me=False, chars_as_strings=False, mat_dtype=True, struct_as_record=True).",
          "name": "matlab_compatible",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to load MATLAB structs as numpy record arrays, or as old-style numpy arrays with dtype=object.  Setting this flag to False replicates the behavior of scipy version 0.7.x (returning numpy object arrays).  The default setting is True, because it allows easier round-trip load and save of MATLAB files.",
          "name": "struct_as_record",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether the length of compressed sequences in the MATLAB file should be checked, to ensure that they are not longer than we expect. It is advisable to enable this (the default) because overlong compressed sequences in MATLAB files generally indicate that the files have experienced some sort of corruption.",
          "name": "verify_compressed_data_integrity",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "If None (the default) - read all variables in file. Otherwise `variable_names` should be a sequence of strings, giving names of the matlab variables to read from the file.  The reader will skip any variable with a name not in this sequence, possibly saving some read processing. ",
          "name": "variable_names",
          "type": ""
        }
      ],
      "returns": {
        "description": "dictionary with variable names as keys, and loaded matrices as values  Notes ----- v4 (Level 1.0), v6 and v7 to 7.2 matfiles are supported.  You will need an HDF5 python library to read matlab 7.3 format mat files.  Because scipy does not supply one, we do not implement the HDF5 / 7.3 interface here.  \"",
        "name": "mat_dict",
        "type": "dict"
      },
      "tags": [
        "io",
        "matlab",
        "mio"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.extmath.randomized_svd",
      "description": "\"Computes a truncated randomized SVD\n",
      "id": "sklearn.utils.extmath.randomized_svd",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.extmath.randomized_svd",
      "parameters": [
        {
          "description": "Matrix to decompose  n_components: int Number of singular values and vectors to extract.  n_oversamples: int (default is 10) Additional number of random vectors to sample the range of M so as to ensure proper conditioning. The total number of random vectors used to find the range of M is n_components + n_oversamples. Smaller number can improve speed but can negatively impact the quality of approximation of singular vectors and singular values.  n_iter: int or 'auto' (default is 'auto') Number of power iterations. It can be used to deal with very noisy problems. When 'auto', it is set to 4, unless `n_components` is small (< .1 * min(X.shape)) `n_iter` in which case is set to 7. This improves precision with few components.  .. versionchanged:: 0.18  power_iteration_normalizer: 'auto' (default), 'QR', 'LU', 'none' Whether the power iterations are normalized with step-by-step QR factorization (the slowest but most accurate), 'none' (the fastest but numerically unstable when `n_iter` is large, e.g. typically 5 or larger), or 'LU' factorization (numerically stable but can lose slightly in accuracy). The 'auto' mode applies no normalization if `n_iter`<=2 and switches to LU otherwise.  .. versionadded:: 0.18  transpose: True, False or 'auto' (default) Whether the algorithm should be applied to M.T instead of M. The result should approximately be the same. The 'auto' mode will trigger the transposition if M.shape[1] > M.shape[0] since this implementation of randomized SVD tend to be a little faster in that case.  .. versionchanged:: 0.18  flip_sign: boolean, (True by default) The output of a singular value decomposition is only unique up to a permutation of the signs of the singular vectors. If `flip_sign` is set to `True`, the sign ambiguity is resolved by making the largest loadings for each component in the left singular vectors positive.  random_state: RandomState or an int seed (0 by default) A random number generator instance to make behavior  Notes ----- This algorithm finds a (usually very good) approximate truncated singular value decomposition using randomization to speed up the computations. It is particularly fast on large matrices on which you wish to extract only a small number of components. In order to obtain further speed up, `n_iter` can be set <=2 (at the cost of loss of precision).  References ---------- * Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 http://arxiv.org/abs/arXiv:0909.4061  * A randomized algorithm for the decomposition of matrices Per-Gunnar Martinsson, Vladimir Rokhlin and Mark Tygert  * An implementation of a randomized algorithm for principal component analysis A. Szlam et al. 2014 \"",
          "name": "M",
          "type": "ndarray"
        }
      ],
      "tags": [
        "utils",
        "extmath"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.utils.validation.check_array",
      "description": "'Input validation on an array, list, sparse matrix or similar.\n\nBy default, the input is converted to an at least 2D numpy array.\nIf the dtype of the array is object, attempt converting to float,\nraising on failure.\n",
      "id": "sklearn.utils.validation.check_array",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.utils.validation.check_array",
      "parameters": [
        {
          "description": "Input object to check / convert. ",
          "name": "array",
          "type": "object"
        },
        {
          "description": "String[s] representing allowed sparse matrix formats, such as \\'csc\\', \\'csr\\', etc.  None means that sparse matrix input will raise an error. If the input is sparse but not in the allowed format, it will be converted to the first listed format. ",
          "name": "accept_sparse",
          "type": "string"
        },
        {
          "description": "Data type of result. If None, the dtype of the input is preserved. If \"numeric\", dtype is preserved unless array.dtype is object. If dtype is a list of types, conversion on the first type is only performed if the dtype of the input is not in the list. ",
          "name": "dtype",
          "type": "string"
        },
        {
          "description": "Whether an array will be forced to be fortran or c-style. When order is None (default), then if copy=False, nothing is ensured about the memory layout of the output array; otherwise (copy=True) the memory layout of the returned array is kept as close as possible to the original array. ",
          "name": "order",
          "type": ""
        },
        {
          "description": "Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion. ",
          "name": "copy",
          "type": "boolean"
        },
        {
          "description": "Whether to raise an error on np.inf and np.nan in X.  ensure_2d : boolean (default=True) Whether to make X at least 2d. ",
          "name": "force_all_finite",
          "type": "boolean"
        },
        {
          "description": "Whether to allow X.ndim > 2. ",
          "name": "allow_nd",
          "type": "boolean"
        },
        {
          "description": "Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check. ",
          "name": "ensure_min_samples",
          "type": "int"
        },
        {
          "description": "Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and ``ensure_2d`` is True. Setting to 0 disables this check. ",
          "name": "ensure_min_features",
          "type": "int"
        },
        {
          "description": "Raise DataConversionWarning if the dtype of the input data structure does not match the requested dtype, causing a memory copy. ",
          "name": "warn_on_dtype",
          "type": "boolean"
        },
        {
          "description": "If passed, include the name of the estimator in warning messages. ",
          "name": "estimator",
          "type": "str"
        }
      ],
      "returns": {
        "description": "The converted and validated X. '",
        "name": "X_converted",
        "type": "object"
      },
      "tags": [
        "utils",
        "validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.precision_score",
      "description": "\"Compute the precision\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe best value is 1 and the worst value is 0.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.classification.precision_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.precision_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order.  .. versionchanged:: 0.17 parameter *labels* improved for multiclass problem. ",
          "name": "labels",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only. ",
          "name": "pos_label",
          "type": "str"
        },
        {
          "description": "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``'binary'``: Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary. ``'micro'``: Calculate metrics globally by counting the total true positives, false negatives and false positives. ``'macro'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``'weighted'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. ``'samples'``: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`). ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "Precision of the positive class in binary classification or weighted average of the precision of each class for the multiclass task.  Examples --------  >>> from sklearn.metrics import precision_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> precision_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS 0.22... >>> precision_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS 0.33... >>> precision_score(y_true, y_pred, average='weighted') ... # doctest: +ELLIPSIS 0.22... >>> precision_score(y_true, y_pred, average=None)  # doctest: +ELLIPSIS array([ 0.66...,  0.        ,  0.        ])  \"",
        "name": "precision",
        "shape": "n_unique_labels",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.feature_selection.mutual_info_.mutual_info_regression",
      "description": "'Estimate mutual information for a continuous target variable.\n\nMutual information (MI) [1]_ between two random variables is a non-negative\nvalue, which measures the dependency between the variables. It is equal\nto zero if and only if two random variables are independent, and higher\nvalues mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation\nfrom k-nearest neighbors distances as described in [2]_ and [3]_. Both\nmethods are based on the idea originally proposed in [4]_.\n\nIt can be used for univariate features selection, read more in the\n:ref:`User Guide <univariate_feature_selection>`.\n",
      "id": "sklearn.feature_selection.mutual_info_.mutual_info_regression",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.feature_selection.mutual_info_.mutual_info_regression",
      "parameters": [
        {
          "description": "Feature matrix. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Target vector. ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If \\'auto\\', it is assigned to False for dense `X` and to True for sparse `X`. ",
          "name": "discrete_features",
          "type": "\\'auto\\', bool, array_like"
        },
        {
          "description": "Number of neighbors to use for MI estimation for continuous variables, see [2]_ and [3]_. Higher values reduce variance of the estimation, but could introduce a bias. ",
          "name": "n_neighbors",
          "type": "int"
        },
        {
          "description": "Whether to make a copy of the given data. If set to False, the initial data will be overwritten. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Estimated mutual information between each feature and the target.  Notes ----- 1. The term \"discrete features\" is used instead of naming them \"categorical\", because it describes the essence more accurately. For example, pixel intensities of an image are discrete features (but hardly categorical) and you will get better results if mark them as such. Also note, that treating a continuous variable as discrete and vice versa will usually give incorrect results, so be attentive about that. 2. True mutual information can\\'t be negative. If its estimate turns out to be negative, it is replaced by zero.  References ---------- .. [1] `Mutual Information <https://en.wikipedia.org/wiki/Mutual_information>`_ on Wikipedia. .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual information\". Phys. Rev. E 69, 2004. .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous Data Sets\". PLoS ONE 9(2), 2014. .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16 '",
        "name": "mi",
        "shape": "n_features,",
        "type": "ndarray"
      },
      "tags": [
        "feature_selection",
        "mutual_info_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp.eigh",
      "description": "'\nSolve an ordinary or generalized eigenvalue problem for a complex\nHermitian or real symmetric matrix.\n\nFind eigenvalues w and optionally eigenvectors v of matrix `a`, where\n`b` is positive definite::\n\na v[:,i] = w[i] b v[:,i]\nv[i,:].conj() a v[:,i] = w[i]\nv[i,:].conj() b v[:,i] = 1\n",
      "id": "scipy.linalg.decomp.eigh",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp.eigh",
      "parameters": [
        {
          "description": "A complex Hermitian or real symmetric matrix whose eigenvalues and eigenvectors will be computed.",
          "name": "a",
          "type": ""
        },
        {
          "description": "A complex Hermitian or real symmetric definite positive matrix in. If omitted, identity matrix is assumed.",
          "name": "b",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Whether the pertinent array data is taken from the lower or upper triangle of `a`. (Default: lower)",
          "name": "lower",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to calculate only eigenvalues and no eigenvectors. (Default: both are calculated)",
          "name": "eigvals_only",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Use divide and conquer algorithm (faster but expensive in memory, only for generalized eigenvalue problem and if eigvals=None)",
          "name": "turbo",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Indexes of the smallest and largest (in ascending order) eigenvalues and corresponding eigenvectors to be returned: 0 <= lo <= hi <= M-1. If omitted, all eigenvalues and eigenvectors are returned.",
          "name": "eigvals",
          "optional": "true",
          "type": "tuple"
        },
        {
          "description": "Specifies the problem type to be solved:  type = 1: a   v[:,i] = w[i] b v[:,i]  type = 2: a b v[:,i] = w[i]   v[:,i]  type = 3: b a v[:,i] = w[i]   v[:,i]",
          "name": "type",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to overwrite data in `a` (may improve performance)",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to overwrite data in `b` (may improve performance)",
          "name": "overwrite_b",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrices contain only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The N (1<=N<=M) selected eigenvalues, in ascending order, each repeated according to its multiplicity. v : (M, N) complex ndarray (if eigvals_only == False)  The normalized selected eigenvector corresponding to the eigenvalue w[i] is the column v[:,i].  Normalization:  type 1 and 3: v.conj() a      v  = w  type 2: inv(v).conj() a  inv(v) = w  type = 1 or 2: v.conj() b      v  = I  type = 3: v.conj() inv(b) v  = I  Raises ------ LinAlgError : If eigenvalue computation does not converge, an error occurred, or b matrix is not definite positive. Note that if input matrices are not symmetric or hermitian, no error is reported but results will be wrong.  See Also -------- eig : eigenvalues and right eigenvectors for non-symmetric arrays  '",
        "name": "w",
        "type": ""
      },
      "tags": [
        "linalg",
        "decomp"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.dbscan_.dbscan",
      "description": "'Perform DBSCAN clustering from vector array or distance matrix.\n\nRead more in the :ref:`User Guide <dbscan>`.\n",
      "id": "sklearn.cluster.dbscan_.dbscan",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.dbscan_.dbscan",
      "parameters": [
        {
          "description": "A feature array, or array of distances between samples if ``metric=\\'precomputed\\'``. ",
          "name": "X",
          "shape": "n_samples, n_samples",
          "type": "array"
        },
        {
          "description": "The maximum distance between two samples for them to be considered as in the same neighborhood. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. ",
          "name": "min_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. If metric is \"precomputed\", X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only \"nonzero\" elements may be considered neighbors for DBSCAN. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. ",
          "name": "algorithm",
          "optional": "true",
          "type": "\\'auto\\', \\'ball_tree\\', \\'kd_tree\\', \\'brute\\'"
        },
        {
          "description": "Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The power of the Minkowski metric to be used to calculate distance between points. ",
          "name": "p",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Weight of each sample, such that a sample with a weight of at least ``min_samples`` is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Indices of core samples.  labels : array [n_samples] Cluster labels for each point.  Noisy samples are given the label -1.  Notes ----- See examples/cluster/plot_dbscan.py for an example.  This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n).  Sparse neighborhoods can be precomputed using :func:`NearestNeighbors.radius_neighbors_graph <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with ``mode=\\'distance\\'``.  References ---------- Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\". In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996 '",
        "name": "core_samples",
        "type": "array"
      },
      "tags": [
        "cluster",
        "dbscan_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.pairwise_kernels",
      "description": "'Compute the kernel between arrays X and optional array Y.\n\nThis method takes either a vector array or a kernel matrix, and returns\na kernel matrix. If the input is a vector array, the kernels are\ncomputed. If the input is a kernel matrix, it is returned instead.\n\nThis method provides a safe way to take a kernel matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\nkernel between the arrays from both X and Y.\n\nValid values for metric are::\n[\\'rbf\\', \\'sigmoid\\', \\'polynomial\\', \\'poly\\', \\'linear\\', \\'cosine\\']\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.pairwise_kernels",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.pairwise_kernels",
      "parameters": [
        {
          "description": "Array of pairwise kernels between samples, or a feature array.  Y : array [n_samples_b, n_features] A second feature array only if X has shape [n_samples_a, n_features]. ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Whether to filter invalid parameters or not.  `**kwds` : optional keyword parameters Any further parameters are passed directly to the kernel function. ",
          "name": "filter_params",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y.  Notes ----- If metric is \\'precomputed\\', Y is ignored and X is returned.  '",
        "name": "K",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.coordinate_descent.enet_path",
      "description": "\"Compute elastic net path with coordinate descent\n\nThe elastic net optimization function varies for mono and multi-outputs.\n\nFor mono-output tasks it is::\n\n1 / (2 * n_samples) * ||y - Xw||^2_2\n+ alpha * l1_ratio * ||w||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n\nFor multi-output tasks it is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^Fro_2\n+ alpha * l1_ratio * ||W||_21\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <elastic_net>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.enet_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.coordinate_descent.enet_path",
      "parameters": [
        {
          "description": "Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be sparse. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Target values  l1_ratio : float, optional float between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso ",
          "name": "y",
          "shape": "n_samples,",
          "type": "ndarray"
        },
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3`` ",
          "name": "eps",
          "type": "float"
        },
        {
          "description": "Number of alphas along the regularization path ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "List of alphas where to compute the models. If None alphas are set automatically ",
          "name": "alphas",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  Xy : array-like, optional Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The initial values of the coefficients. ",
          "name": "coef_init",
          "shape": "n_features, ",
          "type": "array"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "keyword arguments passed to the coordinate descent solver. ",
          "name": "params",
          "type": "kwargs"
        },
        {
          "description": "whether to return the number of iterations or not. ",
          "name": "return_n_iter",
          "type": "bool"
        },
        {
          "description": "If set to True, forces coefficients to be positive. ",
          "name": "positive",
          "type": "bool"
        },
        {
          "description": "Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False. ",
          "name": "check_input",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The alphas along the path where models are computed.  coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) Coefficients along the path.  dual_gaps : array, shape (n_alphas,) The dual gaps at the end of the optimization for each alpha.  n_iters : array-like, shape (n_alphas,) The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when ``return_n_iter`` is set to True).  Notes ----- See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.  See also -------- MultiTaskElasticNet MultiTaskElasticNetCV ElasticNet ElasticNetCV \"",
        "name": "alphas",
        "shape": "n_alphas,",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.linalg.decomp_qr.qr",
      "description": "\"\nCompute QR decomposition of a matrix.\n\nCalculate the decomposition ``A = Q R`` where Q is unitary/orthogonal\nand R upper triangular.\n",
      "id": "scipy.linalg.decomp_qr.qr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.linalg.decomp_qr.qr",
      "parameters": [
        {
          "description": "Matrix to be decomposed",
          "name": "a",
          "type": ""
        },
        {
          "description": "Whether data in a is overwritten (may improve performance)",
          "name": "overwrite_a",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Work array size, lwork >= a.shape[1]. If None or -1, an optimal size is computed.",
          "name": "lwork",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Determines what information is to be returned: either both Q and R ('full', default), only R ('r') or both Q and R but computed in economy-size ('economic', see Notes). The final option 'raw' (added in Scipy 0.11) makes the function return two matrices (Q, TAU) in the internal format used by LAPACK.",
          "name": "mode",
          "optional": "true",
          "type": "'full', 'r', 'economic', 'raw'"
        },
        {
          "description": "Whether or not factorization should include pivoting for rank-revealing qr decomposition. If pivoting, compute the decomposition ``A P = Q R`` as above, but where P is chosen such that the diagonal of R is non-increasing.",
          "name": "pivoting",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to check that the input matrix contains only finite numbers. Disabling may give a performance gain, but may result in problems (crashes, non-termination) if the inputs do contain infinities or NaNs. ",
          "name": "check_finite",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Of shape (M, M), or (M, K) for ``mode='economic'``.  Not returned if ``mode='r'``. R : float or complex ndarray Of shape (M, N), or (K, N) for ``mode='economic'``.  ``K = min(M, N)``. P : int ndarray Of shape (N,) for ``pivoting=True``. Not returned if ``pivoting=False``.  Raises ------ LinAlgError Raised if decomposition fails  Notes ----- This is an interface to the LAPACK routines dgeqrf, zgeqrf, dorgqr, zungqr, dgeqp3, and zgeqp3.  If ``mode=economic``, the shapes of Q and R are (M, K) and (K, N) instead of (M,M) and (M,N), with ``K=min(M,N)``.  Examples -------- >>> from scipy import random, linalg, dot, diag, all, allclose >>> a = random.randn(9, 6)  >>> q, r = linalg.qr(a) >>> allclose(a, np.dot(q, r)) True >>> q.shape, r.shape ((9, 9), (9, 6))  >>> r2 = linalg.qr(a, mode='r') >>> allclose(r, r2) True  >>> q3, r3 = linalg.qr(a, mode='economic') >>> q3.shape, r3.shape ((9, 6), (6, 6))  >>> q4, r4, p4 = linalg.qr(a, pivoting=True) >>> d = abs(diag(r4)) >>> all(d[1:] <= d[:-1]) True >>> allclose(a[:, p4], dot(q4, r4)) True >>> q4.shape, r4.shape, p4.shape ((9, 9), (9, 6), (6,))  >>> q5, r5, p5 = linalg.qr(a, mode='economic', pivoting=True) >>> q5.shape, r5.shape, p5.shape ((9, 6), (6, 6), (6,))  \"",
        "name": "Q",
        "type": "float"
      },
      "tags": [
        "linalg",
        "decomp_qr"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.omp.orthogonal_mp_gram",
      "description": "'Gram Orthogonal Matching Pursuit (OMP)\n\nSolves n_targets Orthogonal Matching Pursuit problems using only\nthe Gram matrix X.T * X and the product X.T * y.\n\nRead more in the :ref:`User Guide <omp>`.\n",
      "id": "sklearn.linear_model.omp.orthogonal_mp_gram",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.omp.orthogonal_mp_gram",
      "parameters": [
        {
          "description": "Gram matrix of the input data: X.T * X  Xy : array, shape (n_features,) or (n_features, n_targets) Input targets multiplied by X: X.T * y ",
          "name": "Gram",
          "shape": "n_features, n_features",
          "type": "array"
        },
        {
          "description": "Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. ",
          "name": "n_nonzero_coefs",
          "type": "int"
        },
        {
          "description": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Squared L2 norms of the lines of y. Required if tol is not None.  copy_Gram : bool, optional Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway.  copy_Xy : bool, optional Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten. ",
          "name": "norms_squared",
          "shape": "n_targets,",
          "type": "array-like"
        },
        {
          "description": "Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation. ",
          "name": "return_path",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Coefficients of the OMP solution. If `return_path=True`, this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features.  n_iters : array-like or int Number of active features across every target. Returned only if `return_n_iter` is set to True.  See also -------- OrthogonalMatchingPursuit orthogonal_mp lars_path decomposition.sparse_encode  Notes ----- Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)  This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf  '",
        "name": "coef",
        "shape": "n_features,",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "omp"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.unsupervised.silhouette_score",
      "description": "'Compute the mean Silhouette Coefficient of all samples.\n\nThe Silhouette Coefficient is calculated using the mean intra-cluster\ndistance (``a``) and the mean nearest-cluster distance (``b``) for each\nsample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\nb)``.  To clarify, ``b`` is the distance between a sample and the nearest\ncluster that the sample is not a part of.\nNote that Silhouette Coefficent is only defined if number of labels\nis 2 <= n_labels <= n_samples - 1.\n\nThis function returns the mean Silhouette Coefficient over all samples.\nTo obtain the values for each sample, use :func:`silhouette_samples`.\n\nThe best value is 1 and the worst value is -1. Values near 0 indicate\noverlapping clusters. Negative values generally indicate that a sample has\nbeen assigned to the wrong cluster, as a different cluster is more similar.\n\nRead more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "id": "sklearn.metrics.cluster.unsupervised.silhouette_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.unsupervised.silhouette_score",
      "parameters": [
        {
          "description": "Array of pairwise distances between samples, or a feature array. ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "Predicted labels for each sample. ",
          "name": "labels",
          "shape": "n_samples",
          "type": "array"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by :func:`metrics.pairwise.pairwise_distances <sklearn.metrics.pairwise.pairwise_distances>`. If X is the distance array itself, use ``metric=\"precomputed\"``. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If ``sample_size is None``, no sampling is used. ",
          "name": "sample_size",
          "type": "int"
        },
        {
          "description": "The generator used to randomly select a subset of samples if ``sample_size is not None``. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.  `**kwds` : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Mean Silhouette Coefficient for all samples.  References ----------  .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis\". Computational and Applied Mathematics 20: 53-65. <http://www.sciencedirect.com/science/article/pii/0377042787901257>`_  .. [2] `Wikipedia entry on the Silhouette Coefficient <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_  '",
        "name": "silhouette",
        "type": "float"
      },
      "tags": [
        "metrics",
        "cluster",
        "unsupervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.cluster.supervised.adjusted_mutual_info_score",
      "description": "\"Adjusted Mutual Information between two clusterings.\n\nAdjusted Mutual Information (AMI) is an adjustment of the Mutual\nInformation (MI) score to account for chance. It accounts for the fact that\nthe MI is generally higher for two clusterings with a larger number of\nclusters, regardless of whether there is actually more information shared.\nFor two clusterings :math:`U` and :math:`V`, the AMI is given as::\n\nAMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]\n\nThis metric is independent of the absolute values of the labels:\na permutation of the class or cluster label values won't change the\nscore value in any way.\n\nThis metric is furthermore symmetric: switching ``label_true`` with\n``label_pred`` will return the same score value. This can be useful to\nmeasure the agreement of two independent label assignments strategies\non the same dataset when the real ground truth is not known.\n\nBe mindful that this function is an order of magnitude slower than other\nmetrics, such as the Adjusted Rand Index.\n\nRead more in the :ref:`User Guide <mutual_info_score>`.\n",
      "id": "sklearn.metrics.cluster.supervised.adjusted_mutual_info_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.cluster.supervised.adjusted_mutual_info_score",
      "parameters": [
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_true",
          "shape": "n_samples",
          "type": "int"
        },
        {
          "description": "A clustering of the data into disjoint subsets. ",
          "name": "labels_pred",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "returns": {
        "description": "The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative.  See also -------- adjusted_rand_score: Adjusted Rand Index mutual_information_score: Mutual Information (not adjusted for chance)  Examples --------  Perfect labelings are both homogeneous and complete, hence have score 1.0::  >>> from sklearn.metrics.cluster import adjusted_mutual_info_score >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1]) 1.0 >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0]) 1.0  If classes members are completely split across different clusters, the assignment is totally in-complete, hence the AMI is null::  >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3]) 0.0  References ---------- .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance, JMLR <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_  .. [2] `Wikipedia entry for the Adjusted Mutual Information <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_  \"",
        "name": "ami: float(upperlimited by 1.0)"
      },
      "tags": [
        "metrics",
        "cluster",
        "supervised"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.covariance.graph_lasso_.graph_lasso",
      "description": "\"l1-penalized covariance estimator\n\nRead more in the :ref:`User Guide <sparse_inverse_covariance>`.\n",
      "id": "sklearn.covariance.graph_lasso_.graph_lasso",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.covariance.graph_lasso_.graph_lasso",
      "parameters": [
        {
          "description": "Empirical covariance from which to compute the covariance estimate. ",
          "name": "emp_cov",
          "shape": "n_features, n_features",
          "type": ""
        },
        {
          "description": "The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. ",
          "name": "alpha",
          "type": "positive"
        },
        {
          "description": "The initial guess for the covariance. ",
          "name": "cov_init",
          "optional": "true",
          "type": ""
        },
        {
          "description": "The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable. ",
          "name": "mode",
          "type": "'cd', 'lars'"
        },
        {
          "description": "The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. ",
          "name": "tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode='cd'. ",
          "name": "enet_tol",
          "optional": "true",
          "type": "positive"
        },
        {
          "description": "The maximum number of iterations. ",
          "name": "max_iter",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "If verbose is True, the objective function and dual gap are printed at each iteration. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If return_costs is True, the objective function and dual gap at each iteration are returned. ",
          "name": "return_costs",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The estimated covariance matrix.  precision : 2D ndarray, shape (n_features, n_features) The estimated (sparse) precision matrix.  costs : list of (objective, dual_gap) pairs The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True.  n_iter : int Number of iterations. Returned only if `return_n_iter` is set to True.  See Also -------- GraphLasso, GraphLassoCV  Notes ----- The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R `glasso` package.  One possible difference with the `glasso` R package is that the diagonal coefficients are not penalized.  \"",
        "name": "covariance",
        "shape": "n_features, n_features",
        "type": ""
      },
      "tags": [
        "covariance",
        "graph_lasso_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.lfw.fetch_lfw_people",
      "description": "\"Loader for the Labeled Faces in the Wild (LFW) people dataset\n\nThis dataset is a collection of JPEG pictures of famous people\ncollected on the internet, all details are available on the\nofficial website:\n\nhttp://vis-www.cs.umass.edu/lfw/\n\nEach picture is centered on a single face. Each pixel of each channel\n(color in RGB) is encoded by a float in range 0.0 - 1.0.\n\nThe task is called Face Recognition (or Identification): given the\npicture of a face, find the name of the person given a training set\n(gallery).\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 74.\n",
      "id": "sklearn.datasets.lfw.fetch_lfw_people",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.lfw.fetch_lfw_people",
      "parameters": [
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in '~/scikit_learn_data' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Download and use the funneled variant of the dataset. ",
          "name": "funneled",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Ratio used to resize the each face picture. ",
          "name": "resize",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The extracted dataset will only retain pictures of people that have at least `min_faces_per_person` different pictures. ",
          "name": "min_faces_per_person",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False. ",
          "name": "color",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Provide a custom 2D slice (height, width) to extract the 'interesting' part of the jpeg files and avoid use statistical correlation from the background ",
          "name": "slice_",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": " dataset.data : numpy array of shape (13233, 2914) Each row corresponds to a ravelled face image of original size 62 x 47 pixels. Changing the ``slice_`` or resize parameters will change the shape of the output.  dataset.images : numpy array of shape (13233, 62, 47) Each row is a face image corresponding to one of the 5749 people in the dataset. Changing the ``slice_`` or resize parameters will change the shape of the output.  dataset.target : numpy array of shape (13233,) Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs.  dataset.DESCR : string Description of the Labeled Faces in the Wild (LFW) dataset. \"",
        "name": "dataset",
        "type": "dict-like"
      },
      "tags": [
        "datasets",
        "lfw"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.hierarchical.linkage_tree",
      "description": "'Linkage agglomerative clustering based on a Feature matrix.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "id": "sklearn.cluster.hierarchical.linkage_tree",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.hierarchical.linkage_tree",
      "parameters": [
        {
          "description": "feature matrix representing n_samples samples to be clustered ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. The matrix is assumed to be symmetric and only the upper triangular half is used. Default is None, i.e, the Ward algorithm is unstructured. ",
          "name": "connectivity",
          "optional": "true",
          "type": "sparse"
        },
        {
          "description": "Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the \\'children\\' output is of limited use, and the \\'parents\\' output should rather be used. This option is valid only when specifying a connectivity matrix. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Which linkage criteria to use. The linkage criterion determines which distance to use between sets of observation. - average uses the average of the distances of each observation of the two sets - complete or maximum linkage uses the maximum distances between all observations of the two sets. ",
          "name": "linkage",
          "optional": "true",
          "type": "\"average\", \"complete\""
        },
        {
          "description": "which metric to use. Can be \"euclidean\", \"manhattan\", or any distance know to paired distance (see metric.pairwise) ",
          "name": "affinity",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "whether or not to return the distances between the clusters. ",
          "name": "return_distance",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i - n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i`  n_components : int The number of connected components in the graph.  n_leaves : int The number of leaves in the tree.  parents : 1D array, shape (n_nodes, ) or None The parent of each node. Only returned when a connectivity matrix is specified, elsewhere \\'None\\' is returned.  distances : ndarray, shape (n_nodes-1,) Returned when return_distance is set to True.  distances[i] refers to the distance between children[i][0] and children[i][1] when they are merged.  See also -------- ward_tree : hierarchical clustering with ward linkage '",
        "name": "children",
        "shape": "n_nodes-1, 2",
        "type": ""
      },
      "tags": [
        "cluster",
        "hierarchical"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.spectral.spectral_clustering",
      "description": "\"Apply clustering to a projection to the normalized laplacian.\n\nIn practice Spectral Clustering is very useful when the structure of\nthe individual clusters is highly non-convex or more generally when\na measure of the center and spread of the cluster is not a suitable\ndescription of the complete cluster. For instance when clusters are\nnested circles on the 2D plan.\n\nIf affinity is the adjacency matrix of a graph, this method can be\nused to find normalized graph cuts.\n\nRead more in the :ref:`User Guide <spectral_clustering>`.\n",
      "id": "sklearn.cluster.spectral.spectral_clustering",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.spectral.spectral_clustering",
      "parameters": [
        {
          "description": "The affinity matrix describing the relationship of the samples to embed. **Must be symmetric**.  Possible examples: - adjacency matrix of a graph, - heat kernel of the pairwise distance matrix of the samples, - symmetric k-nearest neighbours connectivity matrix of the samples. ",
          "name": "affinity",
          "type": "array-like"
        },
        {
          "description": "Number of clusters to extract. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of eigen vectors to use for the spectral embedding ",
          "name": "n_components",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities ",
          "name": "eigen_solver",
          "type": "None, 'arpack', 'lobpcg', or 'amg'"
        },
        {
          "description": "A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == 'amg' and by the K-Means initialization. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. ",
          "name": "eigen_tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The strategy to use to assign labels in the embedding space.  There are two ways to assign labels after the laplacian embedding.  k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the 'Multiclass spectral clustering' paper referenced below for more details on the discretization approach. ",
          "name": "assign_labels",
          "type": "'kmeans', 'discretize'"
        }
      ],
      "returns": {
        "description": "The labels of the clusters.  References ----------  - Normalized cuts and image segmentation, 2000 Jianbo Shi, Jitendra Malik http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324  - A Tutorial on Spectral Clustering, 2007 Ulrike von Luxburg http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323  - Multiclass spectral clustering, 2003 Stella X. Yu, Jianbo Shi http://www1.icsi.berkeley.edu/~stellayu/publication/doc/2003kwayICCV.pdf  Notes ------ The graph should contain only one connect component, elsewhere the results make little sense.  This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering. \"",
        "name": "labels",
        "type": "array"
      },
      "tags": [
        "cluster",
        "spectral"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.hierarchical.ward_tree",
      "description": "\"Ward clustering based on a Feature matrix.\n\nRecursively merges the pair of clusters that minimally increases\nwithin-cluster variance.\n\nThe inertia matrix uses a Heapq-based representation.\n\nThis is the structured version, that takes into account some topological\nstructure between samples.\n\nRead more in the :ref:`User Guide <hierarchical_clustering>`.\n",
      "id": "sklearn.cluster.hierarchical.ward_tree",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.hierarchical.ward_tree",
      "parameters": [
        {
          "description": "feature matrix  representing n_samples samples to be clustered ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. The matrix is assumed to be symmetric and only the upper triangular half is used. Default is None, i.e, the Ward algorithm is unstructured. ",
          "name": "connectivity",
          "optional": "true",
          "type": "sparse"
        },
        {
          "description": "Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the 'children' output is of limited use, and the 'parents' output should rather be used. This option is valid only when specifying a connectivity matrix.  return_distance: bool (optional) If True, return the distance between the clusters. ",
          "name": "n_clusters",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The children of each non-leaf node. Values less than `n_samples` correspond to leaves of the tree which are the original samples. A node `i` greater than or equal to `n_samples` is a non-leaf node and has children `children_[i - n_samples]`. Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node `n_samples + i`  n_components : int The number of connected components in the graph.  n_leaves : int The number of leaves in the tree  parents : 1D array, shape (n_nodes, ) or None The parent of each node. Only returned when a connectivity matrix is specified, elsewhere 'None' is returned.  distances : 1D array, shape (n_nodes-1, ) Only returned if return_distance is set to True (for compatibility). The distances between the centers of the nodes. `distances[i]` corresponds to a weighted euclidean distance between the nodes `children[i, 1]` and `children[i, 2]`. If the nodes refer to leaves of the tree, then `distances[i]` is their unweighted euclidean distance. Distances are updated in the following way (from scipy.hierarchy.linkage):  The new entry :math:`d(u,v)` is computed as follows,  .. math::  d(u,v) = \\\\sqrt{\\\\frac{|v|+|s|} {T}d(v,s)^2 + \\\\frac{|v|+|t|} {T}d(v,t)^2 - \\\\frac{|v|} {T}d(s,t)^2}  where :math:`u` is the newly joined cluster consisting of clusters :math:`s` and :math:`t`, :math:`v` is an unused cluster in the forest, :math:`T=|v|+|s|+|t|`, and :math:`|*|` is the cardinality of its argument. This is also known as the incremental algorithm. \"",
        "name": "children",
        "shape": "n_nodes-1, 2",
        "type": ""
      },
      "tags": [
        "cluster",
        "hierarchical"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.pairwise_distances_argmin_min",
      "description": "\"Compute minimum distances between one point and a set of points.\n\nThis function computes for each row in X, the index of the row of Y which\nis closest (according to the specified distance). The minimal distances are\nalso returned.\n\nThis is mostly equivalent to calling:\n\n(pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\npairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n\nbut uses much less memory, and is faster for large arrays.\n",
      "id": "sklearn.metrics.pairwise.pairwise_distances_argmin_min",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.pairwise_distances_argmin_min",
      "parameters": [
        {
          "description": "Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features) ",
          "name": "X, Y",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "To reduce memory consumption over the naive solution, data are processed in batches, comprising batch_size rows of X and batch_size rows of Y. The default value is quite conservative, but can be changed for fine-tuning. The larger the number, the larger the memory usage. ",
          "name": "batch_size",
          "type": "integer"
        },
        {
          "description": "metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.  If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.  Distance matrices are not supported.  Valid values for metric are:  - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']  - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']  See the documentation for scipy.spatial.distance for details on these metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Keyword arguments to pass to specified metric function. ",
          "name": "metric_kwargs",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Axis along which the argmin and distances are to be computed. ",
          "name": "axis",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Y[argmin[i], :] is the row in Y that is closest to X[i, :].  distances : numpy.ndarray distances[i] is the distance between the i-th row in X and the argmin[i]-th row in Y.  See also -------- sklearn.metrics.pairwise_distances sklearn.metrics.pairwise_distances_argmin \"",
        "name": "argmin",
        "type": "numpy"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.learning_curve.validation_curve",
      "description": "'Validation curve.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.validation_curve` instead.\n\nDetermine training and test scores for varying parameter values.\n\nCompute scores for an estimator with different values of a specified\nparameter. This is similar to grid search with one parameter. However, this\nwill also compute training scores and is merely a utility for plotting the\nresults.\n\nRead more in the :ref:`User Guide <validation_curve>`.\n",
      "id": "sklearn.learning_curve.validation_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.learning_curve.validation_curve",
      "parameters": [
        {
          "description": "An object of that type which is cloned for each validation.  X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
          "name": "y",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Name of the parameter that will be varied. ",
          "name": "param_name",
          "type": "string"
        },
        {
          "description": "The values of the parameter that will be evaluated. ",
          "name": "param_range",
          "shape": "n_values,",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Number of jobs to run in parallel (default 1). ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like \\'2*n_jobs\\'. ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Scores on training sets.  test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set.  Notes ----- See :ref:`examples/model_selection/plot_validation_curve.py <sphx_glr_auto_examples_model_selection_plot_validation_curve.py>` '",
        "name": "train_scores",
        "shape": "n_ticks, n_cv_folds",
        "type": "array"
      },
      "tags": [
        "learning_curve"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.dict_learning.sparse_encode",
      "description": "\"Sparse coding\n\nEach row of the result is the solution to a sparse coding problem.\nThe goal is to find a sparse array `code` such that::\n\nX ~= code * dictionary\n\nRead more in the :ref:`User Guide <SparseCoder>`.\n",
      "id": "sklearn.decomposition.dict_learning.sparse_encode",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.dict_learning.sparse_encode",
      "parameters": [
        {
          "description": "Data matrix  dictionary: array of shape (n_components, n_features) The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output.  gram: array, shape=(n_components, n_components) Precomputed Gram matrix, dictionary * dictionary'  cov: array, shape=(n_components, n_samples) Precomputed covariance, dictionary' * X  algorithm: {'lasso_lars', 'lasso_cd', 'lars', 'omp', 'threshold'} lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X'  n_nonzero_coefs: int, 0.1 * n_features by default Number of nonzero coefficients to target in each column of the solution. This is only used by `algorithm='lars'` and `algorithm='omp'` and is overridden by `alpha` in the `omp` case.  alpha: float, 1. by default If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is the penalty applied to the L1 norm. If `algorithm='threshold'`, `alpha` is the absolute value of the threshold below which coefficients will be squashed to zero. If `algorithm='omp'`, `alpha` is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides `n_nonzero_coefs`.  init: array of shape (n_samples, n_components) Initialization value of the sparse codes. Only used if `algorithm='lasso_cd'`.  max_iter: int, 1000 by default Maximum number of iterations to perform if `algorithm='lasso_cd'`.  copy_cov: boolean, optional Whether to copy the precomputed covariance matrix; if False, it may be overwritten.  n_jobs: int, optional Number of parallel jobs to run.  check_input: boolean, optional If False, the input arrays X and dictionary will not be checked. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array"
        },
        {
          "description": "Controls the verbosity; the higher, the more messages. Defaults to 0. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The sparse codes  See also -------- sklearn.linear_model.lars_path sklearn.linear_model.orthogonal_mp sklearn.linear_model.Lasso SparseCoder \"",
        "name": "code: array of shape (n_samples, n_components)"
      },
      "tags": [
        "decomposition",
        "dict_learning"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.stats.mstats_basic.mquantiles",
      "description": "'\nComputes empirical quantiles for a data array.\n\nSamples quantile are defined by ``Q(p) = (1-gamma)*x[j] + gamma*x[j+1]``,\nwhere ``x[j]`` is the j-th order statistic, and gamma is a function of\n``j = floor(n*p + m)``, ``m = alphap + p*(1 - alphap - betap)`` and\n``g = n*p + m - j``.\n\nReinterpreting the above equations to compare to **R** lead to the\nequation: ``p(k) = (k - alphap)/(n + 1 - alphap - betap)``\n\nTypical values of (alphap,betap) are:\n- (0,1)    : ``p(k) = k/n`` : linear interpolation of cdf\n(**R** type 4)\n- (.5,.5)  : ``p(k) = (k - 1/2.)/n`` : piecewise linear function\n(**R** type 5)\n- (0,0)    : ``p(k) = k/(n+1)`` :\n(**R** type 6)\n- (1,1)    : ``p(k) = (k-1)/(n-1)``: p(k) = mode[F(x[k])].\n(**R** type 7, **R** default)\n- (1/3,1/3): ``p(k) = (k-1/3)/(n+1/3)``: Then p(k) ~ median[F(x[k])].\nThe resulting quantile estimates are approximately median-unbiased\nregardless of the distribution of x.\n(**R** type 8)\n- (3/8,3/8): ``p(k) = (k-3/8)/(n+1/4)``: Blom.\nThe resulting quantile estimates are approximately unbiased\nif x is normally distributed\n(**R** type 9)\n- (.4,.4)  : approximately quantile unbiased (Cunnane)\n- (.35,.35): APL, used with PWM\n",
      "id": "scipy.stats.mstats_basic.mquantiles",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.stats.mstats_basic.mquantiles",
      "parameters": [
        {
          "description": "Input data, as a sequence or array of dimension at most 2.",
          "name": "a",
          "type": "array"
        },
        {
          "description": "List of quantiles to compute.",
          "name": "prob",
          "optional": "true",
          "type": "array"
        },
        {
          "description": "Plotting positions parameter, default is 0.4.",
          "name": "alphap",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Plotting positions parameter, default is 0.4.",
          "name": "betap",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Axis along which to perform the trimming. If None (default), the input array is first flattened.",
          "name": "axis",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Tuple of (lower, upper) values. Values of `a` outside this open interval are ignored. ",
          "name": "limit",
          "optional": "true",
          "type": "tuple"
        }
      ],
      "returns": {
        "description": "An array containing the calculated quantiles.  Notes ----- This formulation is very similar to **R** except the calculation of ``m`` from ``alphap`` and ``betap``, where in **R** ``m`` is defined with each type.  References ---------- .. [1] *R* statistical software: http://www.r-project.org/ .. [2] *R* ``quantile`` function: http://stat.ethz.ch/R-manual/R-devel/library/stats/html/quantile.html  Examples -------- >>> from scipy.stats.mstats import mquantiles >>> a = np.array([6., 47., 49., 15., 42., 41., 7., 39., 43., 40., 36.]) >>> mquantiles(a) array([ 19.2,  40. ,  42.8])  Using a 2D array, specifying axis and limit.  >>> data = np.array([[   6.,    7.,    1.], ...                  [  47.,   15.,    2.], ...                  [  49.,   36.,    3.], ...                  [  15.,   39.,    4.], ...                  [  42.,   40., -999.], ...                  [  41.,   41., -999.], ...                  [   7., -999., -999.], ...                  [  39., -999., -999.], ...                  [  43., -999., -999.], ...                  [  40., -999., -999.], ...                  [  36., -999., -999.]]) >>> print(mquantiles(data, axis=0, limit=(0, 50))) [[ 19.2   14.6    1.45] [ 40.    37.5    2.5 ] [ 42.8   40.05   3.55]]  >>> data[:, 2] = -999. >>> print(mquantiles(data, axis=0, limit=(0, 50))) [[19.200000000000003 14.6 --] [40.0 37.5 --] [42.800000000000004 40.05 --]]  '",
        "name": "mquantiles",
        "type": ""
      },
      "tags": [
        "stats",
        "mstats_basic"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.unsupervised",
      "common_name": "Nearest Neighbors",
      "description": "\"Unsupervised learner for implementing neighbor searches.\n\nRead more in the :ref:`User Guide <unsupervised_neighbors>`.\n",
      "id": "sklearn.neighbors.unsupervised.NearestNeighbors",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"Fit the model using X as training data\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric='precomputed'. \"",
              "name": "X",
              "type": "array-like, sparse matrix, BallTree, KDTree"
            }
          ]
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Finds the K-neighbors of a point.\n\nReturns indices of and distances to the neighbors of each point.\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.kneighbors",
          "name": "kneighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors to get (default is the value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the lengths to points, only present if return_distance=True  ind : array Indices of the nearest points in the population matrix.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1,1,1]  >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=1) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> print(neigh.kneighbors([[1., 1., 1.]])) # doctest: +ELLIPSIS (array([[ 0.5]]), array([[2]]...))  As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:  >>> X = [[0., 1., 0.], [1., 0., 1.]] >>> neigh.kneighbors(X, return_distance=False) # doctest: +ELLIPSIS array([[1], [2]]...)  \"",
            "name": "dist",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of k-Neighbors for points in X\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.kneighbors_graph",
          "name": "kneighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "shape": "n_query, n_features",
              "type": "array-like"
            },
            {
              "description": "Number of neighbors for each sample. (default is value passed to the constructor). ",
              "name": "n_neighbors",
              "type": "int"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(n_neighbors=2) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.kneighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  1.], [ 1.,  0.,  1.]])  See also -------- NearestNeighbors.radius_neighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples_fit",
            "type": "sparse"
          }
        },
        {
          "description": "\"Finds the neighbors within a given radius of a point or points.\n\nReturn the indices and distances of each point from the dataset\nlying in a ball with size ``radius`` around the points of the query\narray. Points lying on the boundary are included in the results.\n\nThe result points are *not* necessarily sorted by distance to their\nquery point.\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.radius_neighbors",
          "name": "radius_neighbors",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "type": "array-like"
            },
            {
              "description": "Limiting distance of neighbors to return. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "If False, distances will not be returned ",
              "name": "return_distance",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the ``metric`` constructor parameter.  ind : array, shape (n_samples,) of arrays An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size ``radius`` around the query points.  Examples -------- In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask who's the closest point to [1, 1, 1]:  >>> import numpy as np >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.6) >>> neigh.fit(samples) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> rng = neigh.radius_neighbors([[1., 1., 1.]]) >>> print(np.asarray(rng[0][0])) # doctest: +ELLIPSIS [ 1.5  0.5] >>> print(np.asarray(rng[1][0])) # doctest: +ELLIPSIS [1 2]  The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.  Notes ----- Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, `radius_neighbors` returns arrays of objects, where each object is a 1D array of indices or distances. \"",
            "name": "dist",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Computes the (weighted) graph of Neighbors for points in X\n\nNeighborhoods are restricted the points at a distance lower than\nradius.\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.radius_neighbors_graph",
          "name": "radius_neighbors_graph",
          "parameters": [
            {
              "description": "The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor. ",
              "name": "X",
              "optional": "true",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Radius of neighborhoods. (default is the value passed to the constructor). ",
              "name": "radius",
              "type": "float"
            },
            {
              "description": "Type of returned matrix: 'connectivity' will return the connectivity matrix with ones and zeros, in 'distance' the edges are Euclidean distance between points. ",
              "name": "mode",
              "optional": "true",
              "type": "'connectivity', 'distance'"
            }
          ],
          "returns": {
            "description": "A[i, j] is assigned the weight of edge that connects i to j.  Examples -------- >>> X = [[0], [3], [1]] >>> from sklearn.neighbors import NearestNeighbors >>> neigh = NearestNeighbors(radius=1.5) >>> neigh.fit(X) # doctest: +ELLIPSIS NearestNeighbors(algorithm='auto', leaf_size=30, ...) >>> A = neigh.radius_neighbors_graph(X) >>> A.toarray() array([[ 1.,  0.,  1.], [ 0.,  1.,  0.], [ 1.,  0.,  1.]])  See also -------- kneighbors_graph \"",
            "name": "A",
            "shape": "n_samples, n_samples",
            "type": "sparse"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.neighbors.unsupervised.NearestNeighbors.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.neighbors.unsupervised.NearestNeighbors",
      "parameters": [
        {
          "description": "Number of neighbors to use by default for :meth:`k_neighbors` queries. ",
          "name": "n_neighbors",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Range of parameter space to use by default for :meth:`radius_neighbors` queries. ",
          "name": "radius",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Algorithm used to compute the nearest neighbors:  - 'ball_tree' will use :class:`BallTree` - 'kd_tree' will use :class:`KDtree` - 'brute' will use a brute-force search. - 'auto' will attempt to decide the most appropriate algorithm based on the values passed to :meth:`fit` method.  Note: fitting on sparse input will override the setting of this parameter, using brute force. ",
          "name": "algorithm",
          "optional": "true",
          "type": "'auto', 'ball_tree', 'kd_tree', 'brute'"
        },
        {
          "description": "Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem.  p: integer, optional (default = 2) Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. ",
          "name": "leaf_size",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used.  If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipy's metrics, but is less efficient than passing the metric name as a string.  Distance matrices are not supported.  Valid values for metric are:  - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2', 'manhattan']  - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev', 'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']  See the documentation for scipy.spatial.distance for details on these metrics. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "Additional keyword arguments for the metric function. ",
          "name": "metric_params",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "The number of parallel jobs to run for neighbors search. If ``-1``, then the number of jobs is set to the number of CPU cores. Affects only :meth:`k_neighbors` and :meth:`kneighbors_graph` methods. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/neighbors/unsupervised.pyc:9",
      "tags": [
        "neighbors",
        "unsupervised"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem). ",
          "name": "n_classes_",
          "type": "int"
        },
        {
          "description": "The number of features when ``fit`` is performed. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs when ``fit`` is performed. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. ",
          "name": "oob_decision_function_",
          "shape": "n_samples, n_classes",
          "type": "array"
        }
      ],
      "category": "ensemble.forest",
      "common_name": "Random Forest Classifier",
      "description": "'A random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree\nclassifiers on various sub-samples of the dataset and use averaging to\nimprove the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if\n`bootstrap=True` (default).\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.ensemble.forest.RandomForestClassifier",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class for X.\n\nThe predicted class of an input sample is a vote by the trees in\nthe forest, weighted by their probability estimates. That is,\nthe predicted class is the one with highest mean probability\nestimate across the trees.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the trees in the\nforest.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample are computed as\nthe mean predicted class probabilities of the trees in the forest. The\nclass probability of a single tree is the fraction of samples of the same\nclass in a leaf.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.RandomForestClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.RandomForestClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "10",
          "description": "The number of trees in the forest. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"gini\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. Note: this parameter is tree-specific. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"auto\"",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\"). - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether bootstrap samples are used when building trees. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether to use out-of-bag samples to estimate the generalization accuracy. ",
          "name": "oob_score",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "\"balanced_subsample\" or None, optional (default=None) Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are computed based on the bootstrap sample for every tree grown.  For multi-output, the weights of each column of y will be multiplied.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ",
          "name": "class_weight",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:744",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. ``oob_improvement_[0]`` is the improvement in loss of the first stage over the ``init`` estimator. ",
          "name": "oob_improvement_",
          "shape": "n_estimators",
          "type": "array"
        },
        {
          "description": "The i-th score ``train_score_[i]`` is the deviance (= loss) of the model at iteration ``i`` on the in-bag sample. If ``subsample == 1`` this is the deviance on the training data. ",
          "name": "train_score_",
          "shape": "n_estimators",
          "type": "array"
        },
        {
          "description": "The concrete ``LossFunction`` object.  `init` : BaseEstimator The estimator that provides the initial predictions. Set via the ``init`` argument or ``loss.init_estimator``. ",
          "name": "loss_",
          "type": ""
        },
        {
          "description": "The collection of fitted sub-estimators.  See also -------- DecisionTreeRegressor, RandomForestRegressor ",
          "name": "estimators_",
          "shape": "n_estimators, 1",
          "type": "ndarray"
        }
      ],
      "category": "ensemble.gradient_boosting",
      "common_name": "Gradient Boosting Regressor",
      "description": "'Gradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion;\nit allows for the optimization of arbitrary differentiable loss functions.\nIn each stage a regression tree is fit on the negative gradient of the\ngiven loss function.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nCompute the decision function of ``X``.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification produce an array of shape [n_samples]. '",
            "name": "score",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Fit the gradient boosting model.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers in classification, real numbers in regression) For classification, labels must correspond to classes. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of ``_fit_stages`` as keyword arguments ``callable(i, self, locals())``. If the callable returns ``True`` the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting. ",
              "name": "monitor",
              "optional": "true",
              "type": "callable"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression target for X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED:  and will be removed in 0.19\n\nCompute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.staged_decision_function",
          "name": "staged_decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification are special cases with ``k == 1``, otherwise ``k==n_classes``. '",
            "name": "score",
            "shape": "n_samples, k",
            "type": "generator"
          }
        },
        {
          "description": "'Predict regression target at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.staged_predict",
          "name": "staged_predict",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted value of the input samples. '",
            "name": "y",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.GradientBoostingRegressor",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "\\'ls\\'",
          "description": "loss function to be optimized. \\'ls\\' refers to least squares regression. \\'lad\\' (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. \\'huber\\' is a combination of the two. \\'quantile\\' allows quantile regression (use `alpha` to specify the quantile). ",
          "name": "loss",
          "optional": "true",
          "type": "\\'ls\\', \\'lad\\', \\'huber\\', \\'quantile\\'"
        },
        {
          "default": "0.1",
          "description": "learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. ",
          "name": "n_estimators",
          "type": "int"
        },
        {
          "default": "3",
          "description": "maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"friedman_mse\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with improvement score by Friedman, \"mse\" for mean squared error, and \"mae\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases.  .. versionadded:: 0.18 ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "1.0",
          "description": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. ",
          "name": "subsample",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The alpha-quantile of the huber loss function and the quantile loss function. Only if ``loss=\\'huber\\'`` or ``loss=\\'quantile\\'``. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "default": "None",
          "description": "An estimator object that is used to compute the initial predictions. ``init`` has to provide ``fit`` and ``predict``. If None it uses ``loss.init_estimator``. ",
          "name": "init",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.  .. versionadded:: 0.17 optional parameter *presort*. ",
          "name": "presort",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:1635",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The number of features. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Prediction computed with out-of-bag estimate on the training set. ",
          "name": "oob_prediction_",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "category": "ensemble.forest",
      "common_name": "Extra Trees Regressor",
      "description": "'An extra-trees regressor.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and use averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.ensemble.forest.ExtraTreesRegressor",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the trees in the forest.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.ExtraTreesRegressor",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "10",
          "description": "The number of trees in the forest. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"mse\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"mse\" for the mean squared error, which is equal to variance reduction as feature selection criterion, and \"mae\" for the mean absolute error.  .. versionadded:: 0.18 Mean Absolute Error (MAE) criterion. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"auto\"",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether bootstrap samples are used when building trees. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "Whether to use out-of-bag samples to estimate the R^2 on unseen data. ",
          "name": "oob_score",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:1337",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "Weights assigned to the features. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Constants in decision function. ",
          "name": "intercept_",
          "shape": "1,",
          "type": "array"
        }
      ],
      "category": "linear_model.stochastic_gradient",
      "common_name": "SGD Classifier",
      "description": "'Linear classifiers (SVM, logistic regression, a.o.) with SGD training.\n\nThis estimator implements regularized linear models with stochastic\ngradient descent (SGD) learning: the gradient of the loss is estimated\neach sample at a time and the model is updated along the way with a\ndecreasing strength schedule (aka learning rate). SGD allows minibatch\n(online/out-of-core) learning, see the partial_fit method.\nFor best results using the default learning rate schedule, the data should\nhave zero mean and unit variance.\n\nThis implementation works with data represented as dense or sparse arrays\nof floating point values for the features. The model it fits can be\ncontrolled with the loss parameter; by default, it fits a linear support\nvector machine (SVM).\n\nThe regularizer is a penalty added to the loss function that shrinks model\nparameters towards the zero vector using either the squared euclidean norm\nL2 or the absolute norm L1 or a combination of both (Elastic Net). If the\nparameter update crosses the 0.0 value because of the regularizer, the\nupdate is truncated to 0.0 to allow for learning sparse models and achieve\nonline feature selection.\n\nRead more in the :ref:`User Guide <sgd>`.\n",
      "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "The initial coefficients to warm-start the optimization. ",
              "name": "coef_init",
              "shape": "n_classes, n_features",
              "type": "array"
            },
            {
              "description": "The initial intercept to warm-start the optimization. ",
              "name": "intercept_init",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Fit linear model with Stochastic Gradient Descent.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.partial_fit",
          "name": "partial_fit",
          "parameters": [
            {
              "description": "Subset of the training data ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Subset of the target values ",
              "name": "y",
              "shape": "n_samples,",
              "type": "numpy"
            },
            {
              "description": "Classes across all calls to partial_fit. Can be obtained by via `np.unique(y_all)`, where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesn't need to contain all labels in `classes`. ",
              "name": "classes",
              "shape": "n_classes,",
              "type": "array"
            },
            {
              "description": "Weights applied to individual samples. If not provided, uniform weights are assumed. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "\"",
            "name": "self",
            "type": "returns"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "None",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.set_params",
          "name": "set_params",
          "parameters": []
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.linear_model.stochastic_gradient.SGDClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.linear_model.stochastic_gradient.SGDClassifier",
      "parameters": [
        {
          "description": "The loss function to be used. Defaults to \\'hinge\\', which gives a linear SVM. The \\'log\\' loss gives logistic regression, a probabilistic classifier. \\'modified_huber\\' is another smooth loss that brings tolerance to outliers as well as probability estimates. \\'squared_hinge\\' is like hinge but is quadratically penalized. \\'perceptron\\' is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description. ",
          "name": "loss",
          "type": "str"
        },
        {
          "description": "The penalty (aka regularization term) to be used. Defaults to \\'l2\\' which is the standard regularizer for linear SVM models. \\'l1\\' and \\'elasticnet\\' might bring sparsity to the model (feature selection) not achievable with \\'l2\\'. ",
          "name": "penalty",
          "type": "str"
        },
        {
          "description": "Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to \\'optimal\\'.  l1_ratio : float The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15. ",
          "name": "alpha",
          "type": "float"
        },
        {
          "description": "Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "The number of passes over the training data (aka epochs). The number of iterations is set to 1 if using partial_fit. Defaults to 5. ",
          "name": "n_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether or not the training data should be shuffled after each epoch. Defaults to True. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "The verbosity level ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is \\'huber\\', \\'epsilon_insensitive\\', or \\'squared_epsilon_insensitive\\'. For \\'huber\\', determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. ",
          "name": "epsilon",
          "type": "float"
        },
        {
          "description": "The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means \\'all CPUs\\'. Defaults to 1. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "The learning rate schedule:  - \\'constant\\': eta = eta0 - \\'optimal\\': eta = 1.0 / (alpha * (t + t0)) [default] - \\'invscaling\\': eta = eta0 / pow(t, power_t)  where t0 is chosen by a heuristic proposed by Leon Bottou.  eta0 : double The initial learning rate for the \\'constant\\' or \\'invscaling\\' schedules. The default value is 0.0 as eta0 is not used by the default schedule \\'optimal\\'. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "The exponent for inverse scaling learning rate [default 0.5]. ",
          "name": "power_t",
          "type": "double"
        },
        {
          "description": "Preset for the class_weight fit parameter.  Weights associated with classes. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))`` ",
          "name": "class_weight",
          "type": "dict"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "When set to True, computes the averaged SGD weights and stores the result in the ``coef_`` attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So ``average=10`` will begin averaging after seeing 10 samples. ",
          "name": "average",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.pyc:548",
      "tags": [
        "linear_model",
        "stochastic_gradient"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "bayesian"
      ],
      "attributes": [
        {
          "description": "The weights of each mixture components. ",
          "name": "weights_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "The mean of each mixture component. ",
          "name": "means_",
          "shape": "n_components, n_features",
          "type": "array-like"
        },
        {
          "description": "The covariance of each mixture component. The shape depends on `covariance_type`::  (n_components,)                        if \\'spherical\\', (n_features, n_features)               if \\'tied\\', (n_components, n_features)             if \\'diag\\', (n_components, n_features, n_features) if \\'full\\' ",
          "name": "covariances_",
          "type": "array-like"
        },
        {
          "description": "The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on ``covariance_type``::  (n_components,)                        if \\'spherical\\', (n_features, n_features)               if \\'tied\\', (n_components, n_features)             if \\'diag\\', (n_components, n_features, n_features) if \\'full\\' ",
          "name": "precisions_",
          "type": "array-like"
        },
        {
          "description": "The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on ``covariance_type``::  (n_components,)                        if \\'spherical\\', (n_features, n_features)               if \\'tied\\', (n_components, n_features)             if \\'diag\\', (n_components, n_features, n_features) if \\'full\\' ",
          "name": "precisions_cholesky_",
          "type": "array-like"
        },
        {
          "description": "True when convergence was reached in fit(), False otherwise. ",
          "name": "converged_",
          "type": "bool"
        },
        {
          "description": "Number of step used by the best fit of inference to reach the convergence. ",
          "name": "n_iter_",
          "type": "int"
        },
        {
          "description": "Lower bound value on the likelihood (of the training data with respect to the model) of the best fit of inference. ",
          "name": "lower_bound_",
          "type": "float"
        },
        {
          "description": "The dirichlet concentration of each component on the weight distribution (Dirichlet). The type depends on ``weight_concentration_prior_type``::  (float, float) if \\'dirichlet_process\\' (Beta parameters), float          if \\'dirichlet_distribution\\' (Dirichlet parameters).  The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the simplex. ",
          "name": "weight_concentration_prior_",
          "type": "tuple"
        },
        {
          "description": "The dirichlet concentration of each component on the weight distribution (Dirichlet). ",
          "name": "weight_concentration_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around `mean_prior`. ",
          "name": "mean_precision_prior",
          "type": "float"
        },
        {
          "description": "The precision of each components on the mean distribution (Gaussian). ",
          "name": "mean_precision_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "The prior on the mean distribution (Gaussian). ",
          "name": "means_prior_",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "The prior of the number of degrees of freedom on the covariance distributions (Wishart). ",
          "name": "degrees_of_freedom_prior_",
          "type": "float"
        },
        {
          "description": "The number of degrees of freedom of each components in the model. ",
          "name": "degrees_of_freedom_",
          "shape": "n_components,",
          "type": "array-like"
        },
        {
          "description": "The prior on the covariance distribution (Wishart). The shape depends on `covariance_type`::  (n_features, n_features) if \\'full\\', (n_features, n_features) if \\'tied\\', (n_features)             if \\'diag\\', float                    if \\'spherical\\'  See Also -------- GaussianMixture : Finite Gaussian mixture fit with EM. ",
          "name": "covariance_prior_",
          "type": "float"
        }
      ],
      "category": "mixture.bayesian_mixture",
      "common_name": "Bayesian Gaussian Mixture",
      "description": "'Variational Bayesian estimation of a Gaussian mixture.\n\nThis class allows to infer an approximate posterior distribution over the\nparameters of a Gaussian mixture distribution. The effective number of\ncomponents can be inferred from the data.\n\nThis class implements two types of prior for the weights distribution: a\nfinite mixture model with Dirichlet distribution and an infinite mixture\nmodel with the Dirichlet Process. In practice Dirichlet Process inference\nalgorithm is approximated and uses a truncated distribution with a fixed\nmaximum number of components (called the Stick-breaking representation).\nThe number of components actually used almost always depends on the data.\n\n.. versionadded:: 0.18\n*BayesianGaussianMixture*.\n\nRead more in the :ref:`User Guide <bgmm>`.\n",
      "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Estimate model parameters with the EM algorithm.\n\nThe method fit the model `n_init` times and set the parameters with\nwhich the model has the largest likelihood or lower bound. Within each\ntrial, the method iterates between E-step and M-step for `max_iter`\ntimes until the change of likelihood or lower bound is less than\n`tol`, otherwise, a `ConvergenceWarning` is raised.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "'",
            "name": "self"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict the labels for the data samples in X using trained model.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Component labels. '",
            "name": "labels",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "'Predict posterior probability of data per each component.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each Gaussian (state) in the model. '",
            "name": "resp",
            "shape": "n_samples, n_components",
            "type": "array"
          }
        },
        {
          "description": "'Generate random samples from the fitted Gaussian distribution.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.sample",
          "name": "sample",
          "parameters": [
            {
              "description": "Number of samples to generate. Defaults to 1. ",
              "name": "n_samples",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "Randomly generated sample  y : array, shape (nsamples,) Component labels  '",
            "name": "X",
            "shape": "n_samples, n_features",
            "type": "array"
          }
        },
        {
          "description": "'Compute the per-sample average log-likelihood of the given data X.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.score",
          "name": "score",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_dimensions",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log likelihood of the Gaussian mixture given X. '",
            "name": "log_likelihood",
            "type": "float"
          }
        },
        {
          "description": "'Compute the weighted log probabilities for each sample.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.score_samples",
          "name": "score_samples",
          "parameters": [
            {
              "description": "List of n_features-dimensional data points. Each row corresponds to a single data point. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Log probabilities of each data point in X. '",
            "name": "log_prob",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        }
      ],
      "name": "sklearn.mixture.bayesian_mixture.BayesianGaussianMixture",
      "parameters": [
        {
          "description": "The number of mixture components. Depending on the data and the value of the `weight_concentration_prior` the model can decide to not use all the components by setting some component `weights_` to values very close to zero. The number of effective components is therefore smaller than n_components. ",
          "name": "n_components",
          "type": "int"
        },
        {
          "description": "String describing the type of covariance parameters to use. Must be one of::  \\'full\\' (each component has its own general covariance matrix), \\'tied\\' (all components share the same general covariance matrix), \\'diag\\' (each component has its own diagonal covariance matrix), \\'spherical\\' (each component has its own single variance). ",
          "name": "covariance_type",
          "type": "\\'full\\', \\'tied\\', \\'diag\\', \\'spherical\\'"
        },
        {
          "description": "The convergence threshold. EM iterations will stop when the lower bound average gain on the likelihood (of the training data with respect to the model) is below this threshold. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive. ",
          "name": "reg_covar",
          "type": "float"
        },
        {
          "description": "The number of EM iterations to perform. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "The number of initializations to perform. The result with the highest lower bound value on the likelihood is kept. ",
          "name": "n_init",
          "type": "int"
        },
        {
          "description": "The method used to initialize the weights, the means and the covariances. Must be one of::  \\'kmeans\\' : responsibilities are initialized using kmeans. \\'random\\' : responsibilities are initialized randomly. ",
          "name": "init_params",
          "type": "\\'kmeans\\', \\'random\\'"
        },
        {
          "description": "String describing the type of the weight concentration prior. Must be one of::  \\'dirichlet_process\\' (using the Stick-breaking representation), \\'dirichlet_distribution\\' (can favor more uniform weights). ",
          "name": "weight_concentration_prior_type",
          "type": "str"
        },
        {
          "description": "The dirichlet concentration of each component on the weight distribution (Dirichlet). The higher concentration puts more mass in the center and will lead to more components being active, while a lower concentration parameter will lead to more mass at the edge of the mixture weights simplex. The value of the parameter must be greater than 0. If it is None, it\\'s set to ``1. / n_components``. ",
          "name": "weight_concentration_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The precision prior on the mean distribution (Gaussian). Controls the extend to where means can be placed. Smaller values concentrate the means of each clusters around `mean_prior`. The value of the parameter must be greater than 0. If it is None, it\\'s set to 1. ",
          "name": "mean_precision_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The prior on the mean distribution (Gaussian). If it is None, it\\'s set to the mean of X. ",
          "name": "mean_prior",
          "optional": "true",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "The prior of the number of degrees of freedom on the covariance distributions (Wishart). If it is None, it\\'s set to `n_features`. ",
          "name": "degrees_of_freedom_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The prior on the covariance distribution (Wishart). If it is None, the emiprical covariance prior is initialized using the covariance of X. The shape depends on `covariance_type`::  (n_features, n_features) if \\'full\\', (n_features, n_features) if \\'tied\\', (n_features)             if \\'diag\\', float                    if \\'spherical\\'  random_state: RandomState or an int seed, defaults to None. A random number generator instance. ",
          "name": "covariance_prior",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "If \\'warm_start\\' is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several time on similar problems. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "description": "Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Number of iteration done before the next print. ",
          "name": "verbose_interval",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/mixture/bayesian_mixture.pyc:65",
      "tags": [
        "mixture",
        "bayesian_mixture"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "The feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature.  It is also known as the Gini importance [4]_. ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The inferred value of max_features. ",
          "name": "max_features_",
          "type": "int"
        },
        {
          "description": "The number of classes (for single output problems), or a list containing the number of classes for each output (for multi-output problems). ",
          "name": "n_classes_",
          "type": "int"
        },
        {
          "description": "The number of features when ``fit`` is performed. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs when ``fit`` is performed. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "The underlying Tree object.  See also -------- DecisionTreeRegressor  References ----------  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification and Regression Trees\", Wadsworth, Belmont, CA, 1984.  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical Learning\", Springer, 2009.  .. [4] L. Breiman, and A. Cutler, \"Random Forests\", http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm ",
          "name": "tree_",
          "type": ""
        }
      ],
      "category": "tree.tree",
      "common_name": "Decision Tree Classifier",
      "description": "'A decision tree classifier.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.tree.tree.DecisionTreeClassifier",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": false,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nReturns the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within ``[0; self.tree_.node_count)``, possibly with gaps in the numbering. \"",
            "name": "X_leaves",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Return the decision path in the tree\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  \"",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "\"Build a decision tree classifier from the training set (X, y).\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels) as integers or strings. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  X_idx_sorted : array-like, shape = [n_samples, n_features], optional The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don't use this parameter unless you know what to do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Returns self. \"",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "The predicted classes, or the predict values. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities of the input samples X.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "\"Predict class probabilities of the input samples X.\n\nThe predicted class probability is the fraction of samples of the same\nclass in a leaf.\n\ncheck_input : boolean, (default=True)\nAllow to bypass several input checking.\nDon't use this parameter unless you know what you do.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. \"",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.tree.tree.DecisionTreeClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.tree.tree.DecisionTreeClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "\"gini\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"best\"",
          "description": "The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split. ",
          "name": "splitter",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "None",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  For multi-output, the weights of each column of y will be multiplied.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training. ",
          "name": "presort",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc:508",
      "tags": [
        "tree",
        "tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.cluster.k_means_.k_means",
      "description": "'K-means clustering algorithm.\n\nRead more in the :ref:`User Guide <k_means>`.\n",
      "id": "sklearn.cluster.k_means_.k_means",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.cluster.k_means_.k_means",
      "parameters": [
        {
          "description": "The observations to cluster. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "The number of clusters to form as well as the number of centroids to generate. ",
          "name": "n_clusters",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations of the k-means algorithm to run. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. ",
          "name": "n_init",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Method for initialization, default to \\'k-means++\\':  \\'k-means++\\' : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details.  \\'random\\': generate k centroids from a Gaussian with mean and variance estimated from the data.  If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers.  If a callable is passed, it should take arguments X, k and and a random state and return an initialization. ",
          "name": "init",
          "optional": "true",
          "type": "\\'k-means++\\', \\'random\\', or ndarray, or a callable"
        },
        {
          "description": "K-means algorithm to use. The classical EM-style algorithm is \"full\". The \"elkan\" variation is more efficient by using the triangle inequality, but currently doesn\\'t support sparse data. \"auto\" chooses \"elkan\" for dense data and \"full\" for sparse data. ",
          "name": "algorithm",
          "type": ""
        },
        {
          "description": "Precompute distances (faster but takes more memory).  \\'auto\\' : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision.  True : always precompute distances  False : never precompute distances ",
          "name": "precompute_distances",
          "type": "\\'auto\\', True, False"
        },
        {
          "description": "The relative increment in the results before declaring convergence. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Verbosity mode. ",
          "name": "verbose",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator. ",
          "name": "random_state",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. ",
          "name": "copy_x",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. ",
          "name": "n_jobs",
          "type": "int"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Centroids found at the last iteration of k-means.  label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the i\\'th observation is closest to.  inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set).  best_n_iter: int Number of iterations corresponding to the best results. Returned only if `return_n_iter` is set to True.  '",
        "name": "centroid",
        "shape": "k, n_features",
        "type": "float"
      },
      "tags": [
        "cluster",
        "k_means_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.fastica_.fastica",
      "description": "'Perform Fast Independent Component Analysis.\n\nRead more in the :ref:`User Guide <ICA>`.\n",
      "id": "sklearn.decomposition.fastica_.fastica",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.fastica_.fastica",
      "parameters": [
        {
          "description": "Training vector, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of components to extract. If None no dimension reduction is performed. ",
          "name": "n_components",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Apply a parallel or deflational FASTICA algorithm. ",
          "name": "algorithm",
          "optional": "true",
          "type": "\\'parallel\\', \\'deflation\\'"
        },
        {
          "description": "If True perform an initial whitening of the data. If False, the data is assumed to have already been preprocessed: it should be centered, normed and white. Otherwise you will get incorrect results. In this case the parameter n_components will be ignored. ",
          "name": "whiten",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "The functional form of the G function used in the approximation to neg-entropy. Could be either \\'logcosh\\', \\'exp\\', or \\'cube\\'. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example:  def my_g(x): return x ** 3, 3 * x ** 2 ",
          "name": "fun",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "Arguments to send to the functional form. If empty or None and if fun=\\'logcosh\\', fun_args will take value {\\'alpha\\' : 1.0} ",
          "name": "fun_args",
          "optional": "true",
          "type": "dictionary"
        },
        {
          "description": "Maximum number of iterations to perform.  tol: float, optional A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array of normal r.v.\\'s is used. ",
          "name": "w_init",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Pseudo number generator state used for random sampling.  return_X_mean : bool, optional If True, X_mean is returned too. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True. ",
          "name": "compute_sources",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether or not to return the number of iterations. ",
          "name": "return_n_iter",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "If whiten is \\'True\\', K is the pre-whitening matrix that projects data onto the first n_components principal components. If whiten is \\'False\\', K is \\'None\\'.  W : array, shape (n_components, n_components) Estimated un-mixing matrix. The mixing matrix can be obtained by::  w = np.dot(W, K.T) A = w.T * (w * w.T).I  S : array, shape (n_samples, n_components) | None Estimated source matrix  X_mean : array, shape (n_features, ) The mean over features. Returned only if return_X_mean is True.  n_iter : int If the algorithm is \"deflation\", n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. This is returned only when return_n_iter is set to `True`.  Notes -----  The data matrix X is considered to be a linear combination of non-Gaussian (independent) components i.e. X = AS where columns of S contain the independent components and A is a linear mixing matrix. In short ICA attempts to `un-mix\\' the data by estimating an un-mixing matrix W where ``S = W K X.``  This implementation was originally made for data of shape [n_features, n_samples]. Now the input is transposed before the algorithm is applied. This makes it slightly faster for Fortran-ordered input.  Implemented using FastICA: `A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430`  '",
        "name": "K",
        "shape": "n_components, n_features",
        "type": "array"
      },
      "tags": [
        "decomposition",
        "fastica_"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.coordinate_descent.lasso_path",
      "description": "\"Compute Lasso path with coordinate descent\n\nThe Lasso optimization function varies for mono and multi-outputs.\n\nFor mono-output tasks it is::\n\n(1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1\n\nFor multi-output tasks it is::\n\n(1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21\n\nWhere::\n\n||W||_21 = \\\\sum_i \\\\sqrt{\\\\sum_j w_{ij}^2}\n\ni.e. the sum of norm of each row.\n\nRead more in the :ref:`User Guide <lasso>`.\n",
      "id": "sklearn.linear_model.coordinate_descent.lasso_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.coordinate_descent.lasso_path",
      "parameters": [
        {
          "description": "Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be sparse. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like, sparse matrix"
        },
        {
          "description": "Target values ",
          "name": "y",
          "shape": "n_samples,",
          "type": "ndarray"
        },
        {
          "description": "Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = 1e-3`` ",
          "name": "eps",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Number of alphas along the regularization path ",
          "name": "n_alphas",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "List of alphas where to compute the models. If ``None`` alphas are set automatically ",
          "name": "alphas",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "Whether to use a precomputed Gram matrix to speed up calculations. If set to ``'auto'`` let us decide. The Gram matrix can also be passed as argument.  Xy : array-like, optional Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.  copy_X : boolean, optional, default True If ``True``, X will be copied; else, it may be overwritten. ",
          "name": "precompute",
          "type": ""
        },
        {
          "description": "The initial values of the coefficients. ",
          "name": "coef_init",
          "shape": "n_features, ",
          "type": "array"
        },
        {
          "description": "Amount of verbosity. ",
          "name": "verbose",
          "type": "bool"
        },
        {
          "description": "keyword arguments passed to the coordinate descent solver. ",
          "name": "params",
          "type": "kwargs"
        },
        {
          "description": "If set to True, forces coefficients to be positive. ",
          "name": "positive",
          "type": "bool"
        },
        {
          "description": "whether to return the number of iterations or not. ",
          "name": "return_n_iter",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The alphas along the path where models are computed.  coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) Coefficients along the path.  dual_gaps : array, shape (n_alphas,) The dual gaps at the end of the optimization for each alpha.  n_iters : array-like, shape (n_alphas,) The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha.  Notes ----- See examples/linear_model/plot_lasso_coordinate_descent_path.py for an example.  To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array.  Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path  Examples ---------  Comparing lasso_path and lars_path with interpolation:  >>> X = np.array([[1, 2, 3.1], [2.3, 5.4, 4.3]]).T >>> y = np.array([1, 2, 3.1]) >>> # Use lasso_path to compute a coefficient path >>> _, coef_path, _ = lasso_path(X, y, alphas=[5., 1., .5]) >>> print(coef_path) [[ 0.          0.          0.46874778] [ 0.2159048   0.4425765   0.23689075]]  >>> # Now use lars_path and 1D linear interpolation to compute the >>> # same path >>> from sklearn.linear_model import lars_path >>> alphas, active, coef_path_lars = lars_path(X, y, method='lasso') >>> from scipy import interpolate >>> coef_path_continuous = interpolate.interp1d(alphas[::-1], ...                                             coef_path_lars[:, ::-1]) >>> print(coef_path_continuous([5., 1., .5])) [[ 0.          0.          0.46915237] [ 0.2159048   0.4425765   0.23668876]]   See also -------- lars_path Lasso LassoLars LassoCV LassoLarsCV sklearn.decomposition.sparse_encode \"",
        "name": "alphas",
        "shape": "n_alphas,",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "coordinate_descent"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.fbeta_score",
      "description": "\"Compute the F-beta score\n\nThe F-beta score is the weighted harmonic mean of precision and recall,\nreaching its optimal value at 1 and its worst value at 0.\n\nThe `beta` parameter determines the weight of precision in the combined\nscore. ``beta < 1`` lends more weight to precision, while ``beta > 1``\nfavors recall (``beta -> 0`` considers only precision, ``beta -> inf``\nonly recall).\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.classification.fbeta_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.fbeta_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier.  beta: float Weight of precision in harmonic mean. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order.  .. versionchanged:: 0.17 parameter *labels* improved for multiclass problem. ",
          "name": "labels",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only. ",
          "name": "pos_label",
          "type": "str"
        },
        {
          "description": "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``'binary'``: Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary. ``'micro'``: Calculate metrics globally by counting the total true positives, false negatives and false positives. ``'macro'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``'weighted'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. ``'samples'``: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`). ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.  References ---------- .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 327-328.  .. [2] `Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_  Examples -------- >>> from sklearn.metrics import fbeta_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5) ... # doctest: +ELLIPSIS 0.23... >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5) ... # doctest: +ELLIPSIS 0.33... >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5) ... # doctest: +ELLIPSIS 0.23... >>> fbeta_score(y_true, y_pred, average=None, beta=0.5) ... # doctest: +ELLIPSIS array([ 0.71...,  0.        ,  0.        ])  \"",
        "name": "fbeta_score",
        "shape": "n_unique_labels",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.f1_score",
      "description": "\"Compute the F1 score, also known as balanced F-score or F-measure\n\nThe F1 score can be interpreted as a weighted average of the precision and\nrecall, where an F1 score reaches its best value at 1 and worst score at 0.\nThe relative contribution of precision and recall to the F1 score are\nequal. The formula for the F1 score is::\n\nF1 = 2 * (precision * recall) / (precision + recall)\n\nIn the multi-class and multi-label case, this is the weighted average of\nthe F1 score of each class.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.classification.f1_score",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.f1_score",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order.  .. versionchanged:: 0.17 parameter *labels* improved for multiclass problem. ",
          "name": "labels",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only. ",
          "name": "pos_label",
          "type": "str"
        },
        {
          "description": "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``'binary'``: Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary. ``'micro'``: Calculate metrics globally by counting the total true positives, false negatives and false positives. ``'macro'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``'weighted'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. ``'samples'``: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`). ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task.  References ---------- .. [1] `Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_  Examples -------- >>> from sklearn.metrics import f1_score >>> y_true = [0, 1, 2, 0, 1, 2] >>> y_pred = [0, 2, 1, 0, 0, 1] >>> f1_score(y_true, y_pred, average='macro')  # doctest: +ELLIPSIS 0.26... >>> f1_score(y_true, y_pred, average='micro')  # doctest: +ELLIPSIS 0.33... >>> f1_score(y_true, y_pred, average='weighted')  # doctest: +ELLIPSIS 0.26... >>> f1_score(y_true, y_pred, average=None) array([ 0.8,  0. ,  0. ])   \"",
        "name": "f1_score",
        "shape": "n_unique_labels",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.lfw.fetch_lfw_pairs",
      "description": "'Loader for the Labeled Faces in the Wild (LFW) pairs dataset\n\nThis dataset is a collection of JPEG pictures of famous people\ncollected on the internet, all details are available on the\nofficial website:\n\nhttp://vis-www.cs.umass.edu/lfw/\n\nEach picture is centered on a single face. Each pixel of each channel\n(color in RGB) is encoded by a float in range 0.0 - 1.0.\n\nThe task is called Face Verification: given a pair of two pictures,\na binary classifier must predict whether the two images are from\nthe same person.\n\nIn the official `README.txt`_ this task is described as the\n\"Restricted\" task.  As I am not sure as to implement the\n\"Unrestricted\" variant correctly, I left it as unsupported for now.\n\n.. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n\nThe original images are 250 x 250 pixels, but the default slice and resize\narguments reduce them to 62 x 74.\n\nRead more in the :ref:`User Guide <labeled_faces_in_the_wild>`.\n",
      "id": "sklearn.datasets.lfw.fetch_lfw_pairs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.lfw.fetch_lfw_pairs",
      "parameters": [
        {
          "description": "Select the dataset to load: \\'train\\' for the development training set, \\'test\\' for the development test set, and \\'10_folds\\' for the official evaluation set that is meant to be used with a 10-folds cross validation. ",
          "name": "subset",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Specify another download and cache folder for the datasets. By default all scikit learn data is stored in \\'~/scikit_learn_data\\' subfolders. ",
          "name": "data_home",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "Download and use the funneled variant of the dataset. ",
          "name": "funneled",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Ratio used to resize the each face picture. ",
          "name": "resize",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False. ",
          "name": "color",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Provide a custom 2D slice (height, width) to extract the \\'interesting\\' part of the jpeg files and avoid use statistical correlation from the background ",
          "name": "slice_",
          "optional": "true",
          "type": "optional"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "optional": "true",
          "type": "optional"
        }
      ],
      "returns": {
        "description": " data : numpy array of shape (2200, 5828). Shape depends on ``subset``. Each row corresponds to 2 ravel\\'d face images of original size 62 x 47 pixels. Changing the ``slice_``, ``resize`` or ``subset`` parameters will change the shape of the output.  pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on ``subset``. Each row has 2 face images corresponding to same or different person from the dataset containing 5749 people. Changing the ``slice_``, ``resize`` or ``subset`` parameters will change the shape of the output.  target : numpy array of shape (2200,). Shape depends on ``subset``. Labels associated to each pair of images. The two label values being different persons or the same person.  DESCR : string Description of the Labeled Faces in the Wild (LFW) dataset.  '",
        "name": "The data is returned as a Bunch object with the following attributes:"
      },
      "tags": [
        "datasets",
        "lfw"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "ensemble"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The classes labels (single output problem), or a list of arrays of class labels (multi-output problem). ",
          "name": "classes_",
          "shape": "n_classes",
          "type": "array"
        },
        {
          "description": "The number of classes (single output problem), or a list containing the number of classes for each output (multi-output problem). ",
          "name": "n_classes_",
          "type": "int"
        },
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The number of features when ``fit`` is performed. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs when ``fit`` is performed. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Decision function computed with out-of-bag estimate on the training set. If n_estimators is small it might be possible that a data point was never left out during the bootstrap. In this case, `oob_decision_function_` might contain NaN. ",
          "name": "oob_decision_function_",
          "shape": "n_samples, n_classes",
          "type": "array"
        }
      ],
      "category": "ensemble.forest",
      "common_name": "Extra Trees Classifier",
      "description": "'An extra-trees classifier.\n\nThis class implements a meta estimator that fits a number of\nrandomized decision trees (a.k.a. extra-trees) on various sub-samples\nof the dataset and use averaging to improve the predictive accuracy\nand control over-fitting.\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.ensemble.forest.ExtraTreesClassifier",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class for X.\n\nThe predicted class of an input sample is a vote by the trees in\nthe forest, weighted by their probability estimates. That is,\nthe predicted class is the one with highest mean probability\nestimate across the trees.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted classes. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n\nThe predicted class log-probabilities of an input sample is computed as\nthe log of the mean predicted class probabilities of the trees in the\nforest.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n\nThe predicted class probabilities of an input sample are computed as\nthe mean predicted class probabilities of the trees in the forest. The\nclass probability of a single tree is the fraction of samples of the same\nclass in a leaf.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.ExtraTreesClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.ExtraTreesClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "10",
          "description": "The number of trees in the forest. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"gini\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"auto\"",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether bootstrap samples are used when building trees. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "Whether to use out-of-bag samples to estimate the generalization accuracy. ",
          "name": "oob_score",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "None",
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``  The \"balanced_subsample\" mode is the same as \"balanced\" except that weights are computed based on the bootstrap sample for every tree grown.  For multi-output, the weights of each column of y will be multiplied.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:1131",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The feature importances. The higher, the more important the feature. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance [4]_. ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The inferred value of max_features. ",
          "name": "max_features_",
          "type": "int"
        },
        {
          "description": "The number of features when ``fit`` is performed. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs when ``fit`` is performed. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "The underlying Tree object.  See also -------- DecisionTreeClassifier  References ----------  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification and Regression Trees\", Wadsworth, Belmont, CA, 1984.  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical Learning\", Springer, 2009.  .. [4] L. Breiman, and A. Cutler, \"Random Forests\", http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm ",
          "name": "tree_",
          "type": ""
        }
      ],
      "category": "tree.tree",
      "common_name": "Decision Tree Regressor",
      "description": "'A decision tree regressor.\n\nRead more in the :ref:`User Guide <tree>`.\n",
      "id": "sklearn.tree.tree.DecisionTreeRegressor",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "\"\nReturns the index of the leaf that each sample is predicted as.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within ``[0; self.tree_.node_count)``, possibly with gaps in the numbering. \"",
            "name": "X_leaves",
            "shape": "n_samples,",
            "type": "array"
          }
        },
        {
          "description": "\"Return the decision path in the tree\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  \"",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "\"Build a decision tree regressor from the training set (X, y).\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (real numbers). Use ``dtype=np.float64`` and ``order='C'`` for maximum efficiency. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do.  X_idx_sorted : array-like, shape = [n_samples, n_features], optional The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Don't use this parameter unless you know what to do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Returns self. \"",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "\"Predict class or regression value for X.\n\nFor a classification model, the predicted class for each sample in X is\nreturned. For a regression model, the predicted value based on X is\nreturned.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, it will be converted to ``dtype=np.float32`` and if a sparse matrix is provided to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Allow to bypass several input checking. Don't use this parameter unless you know what you do. ",
              "name": "check_input",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "The predicted classes, or the predict values. \"",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.tree.tree.DecisionTreeRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.tree.tree.DecisionTreeRegressor",
      "parameters": [
        {
          "default": "\"mse\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"mse\" for the mean squared error, which is equal to variance reduction as feature selection criterion, and \"mae\" for the mean absolute error.  .. versionadded:: 0.18 Mean Absolute Error (MAE) criterion. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"best\"",
          "description": "The strategy used to choose the split at each node. Supported strategies are \"best\" to choose the best split and \"random\" to choose the best random split. ",
          "name": "splitter",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "None",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. If the impurity of a node is below the threshold, the node is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "False",
          "description": "Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training. ",
          "name": "presort",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/tree/tree.pyc:819",
      "tags": [
        "tree",
        "tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "Coefficient of the features in the decision function. ",
          "name": "coef_",
          "shape": "n_classes, n_features",
          "type": "array"
        },
        {
          "description": "Intercept (a.k.a. bias) added to the decision function. If `fit_intercept` is set to False, the intercept is set to zero. ",
          "name": "intercept_",
          "shape": "n_classes,",
          "type": "array"
        },
        {
          "description": "Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given.  See also -------- SGDClassifier : incrementally trained logistic regression (when given the parameter ``loss=\"log\"``). sklearn.svm.LinearSVC : learns SVM models using the same algorithm. ",
          "name": "n_iter_",
          "shape": "n_classes,",
          "type": "array"
        }
      ],
      "category": "linear_model.logistic",
      "common_name": "Logistic Regression",
      "description": "'Logistic Regression (aka logit, MaxEnt) classifier.\n\nIn the multiclass case, the training algorithm uses the one-vs-rest (OvR)\nscheme if the \\'multi_class\\' option is set to \\'ovr\\', and uses the cross-\nentropy loss if the \\'multi_class\\' option is set to \\'multinomial\\'.\n(Currently the \\'multinomial\\' option is supported only by the \\'lbfgs\\',\n\\'sag\\' and \\'newton-cg\\' solvers.)\n\nThis class implements regularized logistic regression using the\n\\'liblinear\\' library, \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solvers. It can handle\nboth dense and sparse input. Use C-ordered arrays or CSR matrices\ncontaining 64-bit floats for optimal performance; any other input format\nwill be converted (and copied).\n\nThe \\'newton-cg\\', \\'sag\\', and \\'lbfgs\\' solvers support only L2 regularization\nwith primal formulation. The \\'liblinear\\' solver supports both L1 and L2\nregularization, with a dual formulation only for the L2 penalty.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n",
      "id": "sklearn.linear_model.logistic.LogisticRegression",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit the model according to the given training data.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target vector relative to X. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.  .. versionadded:: 0.17 *sample_weight* support to LogisticRegression. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Log of probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. '",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n\nFor a multi_class problem, if multi_class is set to be \"multinomial\"\nthe softmax function is used to find the predicted probability of\neach class.\nElse use a one-vs-rest approach, i.e calculate the probability\nof each class assuming it to be positive using the logistic function.\nand normalize these values across all the classes.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. '",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegression.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.linear_model.logistic.LogisticRegression",
      "parameters": [
        {
          "description": "Used to specify the norm used in the penalization. The \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solvers support only l2 penalties. ",
          "name": "penalty",
          "type": "str"
        },
        {
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.  C : float, default: 1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. ",
          "name": "dual",
          "type": "bool"
        },
        {
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "Useful only when the solver \\'liblinear\\' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. ",
          "name": "intercept_scaling",
          "type": "float"
        },
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17 *class_weight=\\'balanced\\'* instead of deprecated *class_weight=\\'auto\\'*. ",
          "name": "class_weight",
          "type": "dict"
        },
        {
          "description": "Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. Used only in solvers \\'sag\\' and \\'liblinear\\'. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "Algorithm to use in the optimization problem.  - For small datasets, \\'liblinear\\' is a good choice, whereas \\'sag\\' is faster for large ones. - For multiclass problems, only \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' handle multinomial loss; \\'liblinear\\' is limited to one-versus-rest schemes. - \\'newton-cg\\', \\'lbfgs\\' and \\'sag\\' only handle L2 penalty.  Note that \\'sag\\' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17 Stochastic Average Gradient descent solver. ",
          "name": "solver",
          "type": "\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\', \\'sag\\'"
        },
        {
          "description": "Tolerance for stopping criteria. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Multiclass option can be either \\'ovr\\' or \\'multinomial\\'. If the option chosen is \\'ovr\\', then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solver.  .. versionadded:: 0.18 Stochastic Average Gradient descent solver for \\'multinomial\\' case. ",
          "name": "multi_class",
          "type": "str"
        },
        {
          "description": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver.  .. versionadded:: 0.17 *warm_start* to support *lbfgs*, *newton-cg*, *sag* solvers. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "description": "Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc:947",
      "tags": [
        "linear_model",
        "logistic"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression"
      ],
      "attributes": [
        {
          "description": "The collection of fitted sub-estimators. ",
          "name": "estimators_",
          "type": "list"
        },
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The number of features when ``fit`` is performed. ",
          "name": "n_features_",
          "type": "int"
        },
        {
          "description": "The number of outputs when ``fit`` is performed. ",
          "name": "n_outputs_",
          "type": "int"
        },
        {
          "description": "Score of the training dataset obtained using an out-of-bag estimate. ",
          "name": "oob_score_",
          "type": "float"
        },
        {
          "description": "Prediction computed with out-of-bag estimate on the training set. ",
          "name": "oob_prediction_",
          "shape": "n_samples",
          "type": "array"
        }
      ],
      "category": "ensemble.forest",
      "common_name": "Random Forest Regressor",
      "description": "'A random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying\ndecision trees on various sub-samples of the dataset and use averaging\nto improve the predictive accuracy and control over-fitting.\nThe sub-sample size is always the same as the original\ninput sample size but the samples are drawn with replacement if\n`bootstrap=True` (default).\n\nRead more in the :ref:`User Guide <forest>`.\n",
      "handles_classification": false,
      "handles_multiclass": false,
      "handles_multilabel": false,
      "handles_regression": true,
      "id": "sklearn.ensemble.forest.RandomForestRegressor",
      "input_type": [
        "DENSE",
        "SPARSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the forest to X, return leaf indices.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators",
            "type": "array"
          }
        },
        {
          "description": "'Return the decision path in the forest\n\n.. versionadded:: 0.18\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.decision_path",
          "name": "decision_path",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  n_nodes_ptr : array of size (n_estimators + 1, ) The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  '",
            "name": "indicator",
            "shape": "n_samples, n_nodes",
            "type": "sparse"
          }
        },
        {
          "description": "'Build a forest of trees from the training set (X, y).\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "The training input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csc_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "The target values (class labels in classification, real numbers in regression). ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict regression target for X.\n\nThe predicted regression target of an input sample is computed as the\nmean predicted regression targets of the trees in the forest.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted into a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the coefficient of determination R^2 of the prediction.\n\nThe coefficient R^2 is defined as (1 - u/v), where u is the regression\nsum of squares ((y_true - y_pred) ** 2).sum() and v is the residual\nsum of squares ((y_true - y_true.mean()) ** 2).sum().\nBest possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always\npredicts the expected value of y, disregarding the input features,\nwould get a R^2 score of 0.0.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True values for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "R^2 of self.predict(X) wrt. y. '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.forest.RandomForestRegressor.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.forest.RandomForestRegressor",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "10",
          "description": "The number of trees in the forest. ",
          "name": "n_estimators",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"mse\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"mse\" for the mean squared error, which is equal to variance reduction as feature selection criterion, and \"mae\" for the mean absolute error.  .. versionadded:: 0.18 Mean Absolute Error (MAE) criterion. ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "\"auto\"",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=n_features`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Whether bootstrap samples are used when building trees. ",
          "name": "bootstrap",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "False",
          "description": "whether to use out-of-bag samples to estimate the R^2 on unseen data. ",
          "name": "oob_score",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "1",
          "description": "The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "Controls the verbosity of the tree building process. ",
          "name": "verbose",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "False",
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ",
          "name": "warm_start",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.pyc:952",
      "tags": [
        "ensemble",
        "forest"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.base.load_files",
      "description": "\"Load text files with categories as subfolder names.\n\nIndividual samples are assumed to be files stored a two levels folder\nstructure such as the following:\n\ncontainer_folder/\ncategory_1_folder/\nfile_1.txt\nfile_2.txt\n...\nfile_42.txt\ncategory_2_folder/\nfile_43.txt\nfile_44.txt\n...\n\nThe folder names are used as supervised signal label names. The\nindividual file names are not important.\n\nThis function does not try to extract features into a numpy array or\nscipy sparse matrix. In addition, if load_content is false it\ndoes not try to load the files in memory.\n\nTo use text files in a scikit-learn classification or clustering\nalgorithm, you will need to use the `sklearn.feature_extraction.text`\nmodule to build a feature extraction transformer that suits your\nproblem.\n\nIf you set load_content=True, you should also specify the encoding of\nthe text using the 'encoding' parameter. For many modern text files,\n'utf-8' will be the correct encoding. If you leave encoding equal to None,\nthen the content will be made of bytes instead of Unicode, and you will\nnot be able to use most functions in `sklearn.feature_extraction.text`.\n\nSimilar feature extractors should be built for other kind of unstructured\ndata input such as images, audio, video, ...\n\nRead more in the :ref:`User Guide <datasets>`.\n",
      "id": "sklearn.datasets.base.load_files",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.base.load_files",
      "parameters": [
        {
          "description": "Path to the main folder holding one subfolder per category  description: string or unicode, optional (default=None) A paragraph describing the characteristic of the dataset: its source, reference, etc. ",
          "name": "container_path",
          "type": "string"
        },
        {
          "default": "None",
          "description": "If None (default), load all the categories. If not None, list of category names to load (other categories ignored). ",
          "name": "categories",
          "optional": "true",
          "type": ""
        },
        {
          "default": "True",
          "description": "Whether to load or not the content of the different files. If true a 'data' attribute containing the text information is present in the data structure returned. If not, a filenames attribute gives the path to the files. ",
          "name": "load_content",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True.  decode_error: {'strict', 'ignore', 'replace'}, optional Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. Passed as keyword argument 'errors' to bytes.decode. ",
          "name": "encoding",
          "type": "string"
        },
        {
          "default": "True",
          "description": "Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent. ",
          "name": "shuffle",
          "optional": "true",
          "type": "bool"
        },
        {
          "default": "0",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: either data, the raw text data to learn, or 'filenames', the files holding it, 'target', the classification labels (integer index), 'target_names', the meaning of the labels, and 'DESCR', the full description of the dataset. \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "base"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.model_selection._validation.learning_curve",
      "description": "'Learning curve.\n\nDetermines cross-validated training and test scores for different training\nset sizes.\n\nA cross-validation generator splits the whole dataset k times in training\nand test data. Subsets of the training set with varying sizes will be used\nto train the estimator and a score for each training subset size and the\ntest set will be computed. Afterwards, the scores will be averaged over\nall k runs for each training subset size.\n\nRead more in the :ref:`User Guide <learning_curve>`.\n",
      "id": "sklearn.model_selection._validation.learning_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.model_selection._validation.learning_curve",
      "parameters": [
        {
          "description": "An object of that type which is cloned for each validation.  X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
          "name": "y",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Group labels for the samples used while splitting the dataset into train/test set. ",
          "name": "groups",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) ",
          "name": "train_sizes",
          "shape": "n_ticks,",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are: - None, to use the default 3-fold cross validation, - integer, to specify the number of folds in a `(Stratified)KFold`, - An object to be used as a cross-validation generator. - An iterable yielding train, test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`StratifiedKFold` is used. In all other cases, :class:`KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes. ",
          "name": "exploit_incremental_learning",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Number of jobs to run in parallel (default 1). ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like \\'2*n_jobs\\'. ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        }
      ],
      "returns": {
        "description": "Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed.  train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets.  test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set.  Notes ----- See :ref:`examples/model_selection/plot_learning_curve.py <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>` '",
        "name": "train_sizes_abs",
        "shape": "n_unique_ticks,",
        "type": "array"
      },
      "tags": [
        "model_selection",
        "_validation"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.pairwise.pairwise_distances",
      "description": "' Compute the distance matrix from a vector array X and optional Y.\n\nThis method takes either a vector array or a distance matrix, and returns\na distance matrix. If the input is a vector array, the distances are\ncomputed. If the input is a distances matrix, it is returned instead.\n\nThis method provides a safe way to take a distance matrix as input, while\npreserving compatibility with many other algorithms that take a vector\narray.\n\nIf Y is given (default is None), then the returned matrix is the pairwise\ndistance between the arrays from both X and Y.\n\nValid values for metric are:\n\n- From scikit-learn: [\\'cityblock\\', \\'cosine\\', \\'euclidean\\', \\'l1\\', \\'l2\\',\n\\'manhattan\\']. These metrics support sparse matrix inputs.\n\n- From scipy.spatial.distance: [\\'braycurtis\\', \\'canberra\\', \\'chebyshev\\',\n\\'correlation\\', \\'dice\\', \\'hamming\\', \\'jaccard\\', \\'kulsinski\\', \\'mahalanobis\\',\n\\'matching\\', \\'minkowski\\', \\'rogerstanimoto\\', \\'russellrao\\', \\'seuclidean\\',\n\\'sokalmichener\\', \\'sokalsneath\\', \\'sqeuclidean\\', \\'yule\\']\nSee the documentation for scipy.spatial.distance for details on these\nmetrics. These metrics do not support sparse matrix inputs.\n\nNote that in the case of \\'cityblock\\', \\'cosine\\' and \\'euclidean\\' (which are\nvalid scipy.spatial.distance metrics), the scikit-learn implementation\nwill be used, which is faster and has support for sparse matrices (except\nfor \\'cityblock\\'). For a verbose description of the metrics from\nscikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\nfunction.\n\nRead more in the :ref:`User Guide <metrics>`.\n",
      "id": "sklearn.metrics.pairwise.pairwise_distances",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.pairwise.pairwise_distances",
      "parameters": [
        {
          "description": "Array of pairwise distances between samples, or a feature array.  Y : array [n_samples_b, n_features], optional An optional second feature array. Only allowed if metric != \"precomputed\". ",
          "name": "X",
          "type": "array"
        },
        {
          "description": "The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is \"precomputed\", X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. ",
          "name": "metric",
          "type": "string"
        },
        {
          "description": "The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel.  If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.  `**kwds` : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples. ",
          "name": "n_jobs",
          "type": "int"
        }
      ],
      "returns": {
        "description": "A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y.  '",
        "name": "D",
        "type": "array"
      },
      "tags": [
        "metrics",
        "pairwise"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.samples_generator.make_classification",
      "description": "'Generate a random n-class classification problem.\n\nThis initially creates clusters of points normally distributed (std=1)\nabout vertices of a `2 * class_sep`-sided hypercube, and assigns an equal\nnumber of clusters to each class. It introduces interdependence between\nthese features and adds various types of further noise to the data.\n\nPrior to shuffling, `X` stacks a number of these primary \"informative\"\nfeatures, \"redundant\" linear combinations of these, \"repeated\" duplicates\nof sampled features, and arbitrary noise for and remaining features.\n\nRead more in the :ref:`User Guide <sample_generators>`.\n",
      "id": "sklearn.datasets.samples_generator.make_classification",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.samples_generator.make_classification",
      "parameters": [
        {
          "default": "100",
          "description": "The number of samples. ",
          "name": "n_samples",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "20",
          "description": "The total number of features. These comprise `n_informative` informative features, `n_redundant` redundant features, `n_repeated` duplicated features and `n_features-n_informative-n_redundant- n_repeated` useless features drawn at random. ",
          "name": "n_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension `n_informative`. For each cluster, informative features are drawn independently from  N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube. ",
          "name": "n_informative",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of redundant features. These features are generated as random linear combinations of the informative features. ",
          "name": "n_redundant",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0",
          "description": "The number of duplicated features, drawn randomly from the informative and the redundant features. ",
          "name": "n_repeated",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of classes (or labels) of the classification problem. ",
          "name": "n_classes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "2",
          "description": "The number of clusters per class. ",
          "name": "n_clusters_per_class",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The proportions of samples assigned to each class. If None, then classes are balanced. Note that if `len(weights) == n_classes - 1`, then the last class weight is automatically inferred. More than `n_samples` samples may be returned if the sum of `weights` exceeds 1. ",
          "name": "weights",
          "type": "list"
        },
        {
          "default": "0.01",
          "description": "The fraction of samples whose class are randomly exchanged. ",
          "name": "flip_y",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "1.0",
          "description": "The factor multiplying the hypercube dimension. ",
          "name": "class_sep",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "True",
          "description": "If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope. ",
          "name": "hypercube",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "0.0",
          "description": "Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep]. ",
          "name": "shift",
          "optional": "true",
          "shape": "n_features",
          "type": "float"
        },
        {
          "default": "1.0",
          "description": "Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting. ",
          "name": "scale",
          "optional": "true",
          "shape": "n_features",
          "type": "float"
        },
        {
          "default": "True",
          "description": "Shuffle the samples and the features. ",
          "name": "shuffle",
          "optional": "true",
          "type": "boolean"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "The generated samples.  y : array of shape [n_samples] The integer labels for class membership of each sample.  Notes ----- The algorithm is adapted from Guyon [1] and was designed to generate the \"Madelon\" dataset.  References ---------- .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable selection benchmark\", 2003.  See also -------- make_blobs: simplified variant make_multilabel_classification: unrelated generator for multilabel tasks '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "array"
      },
      "tags": [
        "datasets",
        "samples_generator"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [
        {
          "description": "The number of nodes (internal nodes + leaves) in the tree. ",
          "name": "node_count",
          "type": "int"
        },
        {
          "description": "The current capacity (i.e., size) of the arrays, which is at least as great as `node_count`. ",
          "name": "capacity",
          "type": "int"
        },
        {
          "description": "The maximal depth of the tree. ",
          "name": "max_depth",
          "type": "int"
        },
        {
          "description": "children_left[i] holds the node id of the left child of node i. For leaves, children_left[i] == TREE_LEAF. Otherwise, children_left[i] > i. This child handles the case where X[:, feature[i]] <= threshold[i]. ",
          "name": "children_left",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "children_right[i] holds the node id of the right child of node i. For leaves, children_right[i] == TREE_LEAF. Otherwise, children_right[i] > i. This child handles the case where X[:, feature[i]] > threshold[i]. ",
          "name": "children_right",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "feature[i] holds the feature to split on, for the internal node i. ",
          "name": "feature",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "threshold[i] holds the threshold for the internal node i. ",
          "name": "threshold",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "Contains the constant prediction value of each node. ",
          "name": "value",
          "shape": "node_count, n_outputs, max_n_classes",
          "type": "array"
        },
        {
          "description": "impurity[i] holds the impurity (i.e., the value of the splitting criterion) at node i. ",
          "name": "impurity",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "n_node_samples[i] holds the number of training samples reaching node i. ",
          "name": "n_node_samples",
          "shape": "node_count",
          "type": "array"
        },
        {
          "description": "weighted_n_node_samples[i] holds the weighted number of training samples reaching node i.",
          "name": "weighted_n_node_samples",
          "shape": "node_count",
          "type": "array"
        }
      ],
      "category": "tree._tree",
      "common_name": "Tree",
      "description": "\"Array-based representation of a binary decision tree.\n\nThe binary tree is represented as a number of parallel arrays. The i-th\nelement of each array holds information about the node `i`. Node 0 is the\ntree's root. You can find a detailed description of all arrays in\n`_tree.pxd`. NOTE: Some of the arrays only apply to either leaves or split\nnodes, resp. In this case the values of nodes of the other type are\narbitrary!\n\nAttributes\n----------\nnode_count : int\nThe number of nodes (internal nodes + leaves) in the tree.\n\ncapacity : int\nThe current capacity (i.e., size) of the arrays, which is at least as\ngreat as `node_count`.\n\nmax_depth : int\nThe maximal depth of the tree.\n\nchildren_left : array of int, shape [node_count]\nchildren_left[i] holds the node id of the left child of node i.\nFor leaves, children_left[i] == TREE_LEAF. Otherwise,\nchildren_left[i] > i. This child handles the case where\nX[:, feature[i]] <= threshold[i].\n\nchildren_right : array of int, shape [node_count]\nchildren_right[i] holds the node id of the right child of node i.\nFor leaves, children_right[i] == TREE_LEAF. Otherwise,\nchildren_right[i] > i. This child handles the case where\nX[:, feature[i]] > threshold[i].\n\nfeature : array of int, shape [node_count]\nfeature[i] holds the feature to split on, for the internal node i.\n\nthreshold : array of double, shape [node_count]\nthreshold[i] holds the threshold for the internal node i.\n\nvalue : array of double, shape [node_count, n_outputs, max_n_classes]\nContains the constant prediction value of each node.\n\nimpurity : array of double, shape [node_count]\nimpurity[i] holds the impurity (i.e., the value of the splitting\ncriterion) at node i.\n\nn_node_samples : array of int, shape [node_count]\nn_node_samples[i] holds the number of training samples reaching node i.\n\nweighted_n_node_samples : array of int, shape [node_count]\nweighted_n_node_samples[i] holds the weighted number of training samples\nreaching node i.\n\"",
      "id": "sklearn.tree._tree.Tree",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.tree._tree.Tree",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "tree",
        "_tree"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.kddcup99.fetch_kddcup99",
      "description": "\"Load and return the kddcup 99 dataset (classification).\n\nThe KDD Cup '99 dataset was created by processing the tcpdump portions\nof the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\ncreated by MIT Lincoln Lab [1] . The artificial data was generated using\na closed network and hand-injected attacks to produce a large number of\ndifferent types of attack with normal activity in the background.\nAs the initial goal was to produce a large training set for supervised\nlearning algorithms, there is a large proportion (80.1%) of abnormal\ndata which is unrealistic in real world, and inappropriate for unsupervised\nanomaly detection which aims at detecting 'abnormal' data, ie\n\n1) qualitatively different from normal data.\n\n2) in large minority among the observations.\n\nWe thus transform the KDD Data set into two different data sets: SA and SF.\n\n- SA is obtained by simply selecting all the normal data, and a small\nproportion of abnormal data to gives an anomaly proportion of 1%.\n\n- SF is obtained as in [2]\nby simply picking up the data whose attribute logged_in is positive, thus\nfocusing on the intrusion attack, which gives a proportion of 0.3% of\nattack.\n\n- http and smtp are two subsets of SF corresponding with third feature\nequal to 'http' (resp. to 'smtp')\n\n\nGeneral KDD structure :\n\n================      ==========================================\nSamples total         4898431\nDimensionality        41\nFeatures              discrete (int) or continuous (float)\nTargets               str, 'normal.' or name of the anomaly type\n================      ==========================================\n\nSA structure :\n\n================      ==========================================\nSamples total         976158\nDimensionality        41\nFeatures              discrete (int) or continuous (float)\nTargets               str, 'normal.' or name of the anomaly type\n================      ==========================================\n\nSF structure :\n\n================      ==========================================\nSamples total         699691\nDimensionality        4\nFeatures              discrete (int) or continuous (float)\nTargets               str, 'normal.' or name of the anomaly type\n================      ==========================================\n\nhttp structure :\n\n================      ==========================================\nSamples total         619052\nDimensionality        3\nFeatures              discrete (int) or continuous (float)\nTargets               str, 'normal.' or name of the anomaly type\n================      ==========================================\n\nsmtp structure :\n\n================      ==========================================\nSamples total         95373\nDimensionality        3\nFeatures              discrete (int) or continuous (float)\nTargets               str, 'normal.' or name of the anomaly type\n================      ==========================================\n\n.. versionadded:: 0.18\n",
      "id": "sklearn.datasets.kddcup99.fetch_kddcup99",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.kddcup99.fetch_kddcup99",
      "parameters": [
        {
          "description": "To return the corresponding classical subsets of kddcup 99. If None, return the entire kddcup 99 dataset. ",
          "name": "subset",
          "type": ""
        },
        {
          "default": "None",
          "description": "Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Whether to shuffle dataset.  percent10 : bool, default=False Whether to load only 10 percent of the data. ",
          "name": "shuffle",
          "type": "bool"
        },
        {
          "description": "If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. ",
          "name": "download_if_missing",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Dictionary-like object, the interesting attributes are: 'data', the data to learn and 'target', the regression target for each sample.   References ---------- .. [1] Analysis and Results of the 1999 DARPA Off-Line Intrusion Detection Evaluation Richard Lippmann, Joshua W. Haines, David J. Fried, Jonathan Korba, Kumar Das  .. [2] A Geometric Framework for Unsupervised Anomaly Detection: Detecting Intrusions in Unlabeled Data (2002) by Eleazar Eskin, Andrew Arnold, Michael Prerau, Leonid Portnoy, Sal Stolfo  \"",
        "name": "data",
        "type": ""
      },
      "tags": [
        "datasets",
        "kddcup99"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.stats.stats.spearmanr",
      "description": "\"\nCalculates a Spearman rank-order correlation coefficient and the p-value\nto test for non-correlation.\n\nThe Spearman correlation is a nonparametric measure of the monotonicity\nof the relationship between two datasets. Unlike the Pearson correlation,\nthe Spearman correlation does not assume that both datasets are normally\ndistributed. Like other correlation coefficients, this one varies\nbetween -1 and +1 with 0 implying no correlation. Correlations of -1 or\n+1 imply an exact monotonic relationship. Positive correlations imply that\nas x increases, so does y. Negative correlations imply that as x\nincreases, y decreases.\n\nThe p-value roughly indicates the probability of an uncorrelated system\nproducing datasets that have a Spearman correlation at least as extreme\nas the one computed from these datasets. The p-values are not entirely\nreliable but are probably reasonable for datasets larger than 500 or so.\n",
      "id": "scipy.stats.stats.spearmanr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.stats.stats.spearmanr",
      "parameters": [
        {
          "description": "One or two 1-D or 2-D arrays containing multiple variables and observations. When these are 1-D, each represents a vector of observations of a single variable. For the behavior in the 2-D case, see under ``axis``, below. Both arrays need to have the same length in the ``axis`` dimension.",
          "name": "a, b",
          "optional": "true",
          "type": ""
        },
        {
          "description": "If axis=0 (default), then each column represents a variable, with observations in the rows. If axis=1, the relationship is transposed: each row represents a variable, while the columns contain observations. If axis=None, then both arrays will be raveled.",
          "name": "axis",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Defines how to handle when input contains nan. 'propagate' returns nan, 'raise' throws an error, 'omit' performs the calculations ignoring nan values. Default is 'propagate'. ",
          "name": "nan_policy",
          "optional": "true",
          "type": "'propagate', 'raise', 'omit'"
        }
      ],
      "returns": {
        "description": "Spearman correlation matrix or correlation coefficient (if only 2 variables are given as parameters. Correlation matrix is square with length equal to total number of variables (columns or rows) in a and b combined. pvalue : float The two-sided p-value for a hypothesis test whose null hypothesis is that two sets of data are uncorrelated, has same dimension as rho.  Notes ----- Changes in scipy 0.8.0: rewrite to add tie-handling, and axis.  References ----------  .. [1] Zwillinger, D. and Kokoska, S. (2000). CRC Standard Probability and Statistics Tables and Formulae. Chapman & Hall: New York. 2000. Section  14.7  Examples -------- >>> from scipy import stats >>> stats.spearmanr([1,2,3,4,5], [5,6,7,8,7]) (0.82078268166812329, 0.088587005313543798) >>> np.random.seed(1234321) >>> x2n = np.random.randn(100, 2) >>> y2n = np.random.randn(100, 2) >>> stats.spearmanr(x2n) (0.059969996999699973, 0.55338590803773591) >>> stats.spearmanr(x2n[:,0], x2n[:,1]) (0.059969996999699973, 0.55338590803773591) >>> rho, pval = stats.spearmanr(x2n, y2n) >>> rho array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626], [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653], [ 0.18569457,  0.110003  ,  1.        ,  0.03488749], [ 0.06258626,  0.02534653,  0.03488749,  1.        ]]) >>> pval array([[ 0.        ,  0.55338591,  0.06435364,  0.53617935], [ 0.55338591,  0.        ,  0.27592895,  0.80234077], [ 0.06435364,  0.27592895,  0.        ,  0.73039992], [ 0.53617935,  0.80234077,  0.73039992,  0.        ]]) >>> rho, pval = stats.spearmanr(x2n.T, y2n.T, axis=1) >>> rho array([[ 1.        ,  0.05997   ,  0.18569457,  0.06258626], [ 0.05997   ,  1.        ,  0.110003  ,  0.02534653], [ 0.18569457,  0.110003  ,  1.        ,  0.03488749], [ 0.06258626,  0.02534653,  0.03488749,  1.        ]]) >>> stats.spearmanr(x2n, y2n, axis=None) (0.10816770419260482, 0.1273562188027364) >>> stats.spearmanr(x2n.ravel(), y2n.ravel()) (0.10816770419260482, 0.1273562188027364)  >>> xint = np.random.randint(10, size=(100, 2)) >>> stats.spearmanr(xint) (0.052760927029710199, 0.60213045837062351)  \"",
        "name": "correlation",
        "type": "float"
      },
      "tags": [
        "stats",
        "stats"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "decision tree"
      ],
      "attributes": [],
      "category": "neighbors.dist_metrics",
      "common_name": "Distance Metric",
      "description": "'DistanceMetric class\n\nThis class provides a uniform interface to fast distance metric\nfunctions.  The various metrics can be accessed via the `get_metric`\nclass method and the metric string identifier (see below).\nFor example, to use the Euclidean distance:\n\n>>> dist = DistanceMetric.get_metric(\\'euclidean\\')\n>>> X = [[0, 1, 2],\n[3, 4, 5]])\n>>> dist.pairwise(X)\narray([[ 0.        ,  5.19615242],\n[ 5.19615242,  0.        ]])\n\nAvailable Metrics\nThe following lists the string metric identifiers and the associated\ndistance metric classes:\n\n**Metrics intended for real-valued vector spaces:**\n\n==============  ====================  ========  ===============================\nidentifier      class name            args      distance function\n--------------  --------------------  --------  -------------------------------\n\"euclidean\"     EuclideanDistance     -         ``sqrt(sum((x - y)^2))``\n\"manhattan\"     ManhattanDistance     -         ``sum(|x - y|)``\n\"chebyshev\"     ChebyshevDistance     -         ``max(|x - y|)``\n\"minkowski\"     MinkowskiDistance     p         ``sum(|x - y|^p)^(1/p)``\n\"wminkowski\"    WMinkowskiDistance    p, w      ``sum(w * |x - y|^p)^(1/p)``\n\"seuclidean\"    SEuclideanDistance    V         ``sqrt(sum((x - y)^2 / V))``\n\"mahalanobis\"   MahalanobisDistance   V or VI   ``sqrt((x - y)\\' V^-1 (x - y))``\n==============  ====================  ========  ===============================\n\n**Metrics intended for two-dimensional vector spaces:**  Note that the haversine\ndistance metric requires data in the form of [latitude, longitude] and both\ninputs and outputs are in units of radians.\n\n============  ==================  ========================================\nidentifier    class name          distance function\n------------  ------------------  ----------------------------------------\n\"haversine\"   HaversineDistance   2 arcsin(sqrt(sin^2(0.5*dx)\n+ cos(x1)cos(x2)sin^2(0.5*dy)))\n============  ==================  ========================================\n\n\n**Metrics intended for integer-valued vector spaces:**  Though intended\nfor integer-valued vectors, these are also valid metrics in the case of\nreal-valued vectors.\n\n=============  ====================  ========================================\nidentifier     class name            distance function\n-------------  --------------------  ----------------------------------------\n\"hamming\"      HammingDistance       ``N_unequal(x, y) / N_tot``\n\"canberra\"     CanberraDistance      ``sum(|x - y| / (|x| + |y|))``\n\"braycurtis\"   BrayCurtisDistance    ``sum(|x - y|) / (sum(|x|) + sum(|y|))``\n=============  ====================  ========================================\n\n**Metrics intended for boolean-valued vector spaces:**  Any nonzero entry\nis evaluated to \"True\".  In the listings below, the following\nabbreviations are used:\n\n- N  : number of dimensions\n- NTT : number of dims in which both values are True\n- NTF : number of dims in which the first value is True, second is False\n- NFT : number of dims in which the first value is False, second is True\n- NFF : number of dims in which both values are False\n- NNEQ : number of non-equal dimensions, NNEQ = NTF + NFT\n- NNZ : number of nonzero dimensions, NNZ = NTF + NFT + NTT\n\n=================  =======================  ===============================\nidentifier         class name               distance function\n-----------------  -----------------------  -------------------------------\n\"jaccard\"          JaccardDistance          NNEQ / NNZ\n\"matching\"         MatchingDistance         NNEQ / N\n\"dice\"             DiceDistance             NNEQ / (NTT + NNZ)\n\"kulsinski\"        KulsinskiDistance        (NNEQ + N - NTT) / (NNEQ + N)\n\"rogerstanimoto\"   RogersTanimotoDistance   2 * NNEQ / (N + NNEQ)\n\"russellrao\"       RussellRaoDistance       NNZ / N\n\"sokalmichener\"    SokalMichenerDistance    2 * NNEQ / (N + NNEQ)\n\"sokalsneath\"      SokalSneathDistance      NNEQ / (NNEQ + 0.5 * NTT)\n=================  =======================  ===============================\n\n**User-defined distance:**\n\n===========    ===============    =======\nidentifier     class name         args\n-----------    ---------------    -------\n\"pyfunc\"       PyFuncDistance     func\n===========    ===============    =======\n\nHere ``func`` is a function which takes two one-dimensional numpy\narrays, and returns a distance.  Note that in order to be used within\nthe BallTree, the distance must be a true metric:\ni.e. it must satisfy the following properties\n\n1) Non-negativity: d(x, y) >= 0\n2) Identity: d(x, y) = 0 if and only if x == y\n3) Symmetry: d(x, y) = d(y, x)\n4) Triangle Inequality: d(x, y) + d(y, z) >= d(x, z)\n\nBecause of the Python object overhead involved in calling the python\nfunction, this will be fairly slow, but it will have the same\nscaling as other distances.\n'",
      "id": "sklearn.neighbors.dist_metrics.DistanceMetric",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [],
      "name": "sklearn.neighbors.dist_metrics.DistanceMetric",
      "parameters": [],
      "source_code": ":",
      "tags": [
        "neighbors",
        "dist_metrics"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.ridge.ridge_regression",
      "description": "\"Solve the ridge equation by the method of normal equations.\n\nRead more in the :ref:`User Guide <ridge_regression>`.\n",
      "id": "sklearn.linear_model.ridge.ridge_regression",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.ridge.ridge_regression",
      "parameters": [
        {
          "description": "shape = [n_samples, n_features] Training data ",
          "name": "X",
          "type": "array-like, sparse matrix, LinearOperator"
        },
        {
          "description": "Target values ",
          "name": "y",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "shape = [n_targets] if array-like Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to ``C^-1`` in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. ",
          "name": "alpha",
          "type": "float, array-like"
        },
        {
          "description": "Maximum number of iterations for conjugate gradient solver. For 'sparse_cg' and 'lsqr' solvers, the default value is determined by scipy.sparse.linalg. For 'sag' solver, the default value is 1000. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Individual weights for each sample. If sample_weight is not None and solver='auto', the solver will be set to 'cholesky'.  .. versionadded:: 0.17 ",
          "name": "sample_weight",
          "shape": "n_samples",
          "type": "float"
        },
        {
          "description": "Solver to use in the computational routines:  - 'auto' chooses the solver automatically based on the type of data.  - 'svd' uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than 'cholesky'.  - 'cholesky' uses the standard scipy.linalg.solve function to obtain a closed-form solution via a Cholesky decomposition of dot(X.T, X)  - 'sparse_cg' uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than 'cholesky' for large-scale data (possibility to set `tol` and `max_iter`).  - 'lsqr' uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure.  - 'sag' uses a Stochastic Average Gradient descent. It also uses an iterative procedure, and is often faster than other solvers when both n_samples and n_features are large. Note that 'sag' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  All last four solvers support both dense and sparse data. However, only 'sag' supports sparse input when `fit_intercept` is True.  .. versionadded:: 0.17 Stochastic Average Gradient descent solver. ",
          "name": "solver",
          "type": "'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg'"
        },
        {
          "description": "Precision of the solution. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Verbosity level. Setting verbose > 0 will display additional information depending on the solver used. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. Used only in 'sag' solver. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If True, the method also returns `n_iter`, the actual number of iteration performed by the solver.  .. versionadded:: 0.17 ",
          "name": "return_n_iter",
          "type": "boolean"
        },
        {
          "description": "If True and if X is sparse, the method also returns the intercept, and the solver is automatically changed to 'sag'. This is only a temporary fix for fitting the intercept with sparse data. For dense data, use sklearn.linear_model._preprocess_data before your regression.  .. versionadded:: 0.17 ",
          "name": "return_intercept",
          "type": "boolean"
        }
      ],
      "returns": {
        "description": "Weight vector(s).  n_iter : int, optional The actual number of iteration performed by the solver. Only returned if `return_n_iter` is True.  intercept : float or array, shape = [n_targets] The intercept of the model. Only returned if `return_intercept` is True and if X is a scipy sparse array.  Notes ----- This function won't compute the intercept. \"",
        "name": "coef",
        "shape": "n_features",
        "type": "array"
      },
      "tags": [
        "linear_model",
        "ridge"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.datasets.svmlight_format.load_svmlight_file",
      "description": "'Load datasets in the svmlight / libsvm format into sparse CSR matrix\n\nThis format is a text-based format, with one sample per line. It does\nnot store zero valued features hence is suitable for sparse dataset.\n\nThe first element of each line can be used to store a target variable\nto predict.\n\nThis format is used as the default format for both svmlight and the\nlibsvm command line programs.\n\nParsing a text based source can be expensive. When working on\nrepeatedly on the same dataset, it is recommended to wrap this\nloader with joblib.Memory.cache to store a memmapped backup of the\nCSR results of the first call and benefit from the near instantaneous\nloading of memmapped structures for the subsequent calls.\n\nIn case the file contains a pairwise preference constraint (known\nas \"qid\" in the svmlight format) these are ignored unless the\nquery_id parameter is set to True. These pairwise preference\nconstraints can be used to constraint the combination of samples\nwhen using pairwise loss functions (as is the case in some\nlearning to rank problems) so that only pairs with the same\nquery_id value are considered.\n\nThis implementation is written in Cython and is reasonably fast.\nHowever, a faster API-compatible loader is also available at:\n\nhttps://github.com/mblondel/svmlight-loader\n",
      "id": "sklearn.datasets.svmlight_format.load_svmlight_file",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.datasets.svmlight_format.load_svmlight_file",
      "parameters": [
        {
          "description": "(Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will be uncompressed on the fly. If an integer is passed, it is assumed to be a file descriptor. A file-like or file descriptor will not be closed by this function. A file-like object must be opened in binary mode. ",
          "name": "f",
          "type": "str, file-like, int"
        },
        {
          "description": "The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. ",
          "name": "n_features",
          "type": "int"
        },
        {
          "description": "Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html) ",
          "name": "multilabel",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to \"auto\", a heuristic check is applied to determine this from the file contents. Both kinds of files occur \"in the wild\", but they are unfortunately not self-identifying. Using \"auto\" or True should always be safe. ",
          "name": "zero_based",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "If True, will return the query_id array for each file. ",
          "name": "query_id",
          "type": "boolean"
        },
        {
          "description": "Data type of dataset to be loaded. This will be the data type of the output numpy arrays ``X`` and ``y``. ",
          "name": "dtype",
          "type": "numpy"
        }
      ],
      "returns": {
        "description": " y : ndarray of shape (n_samples,), or, in the multilabel a list of tuples of length n_samples.  query_id : array of shape (n_samples,) query_id for each sample. Only returned when query_id is set to True.  See also -------- load_svmlight_files: similar function for loading multiple files in this format, enforcing the same number of features/columns on all of them.  Examples -------- To use joblib.Memory to cache the svmlight file::  from sklearn.externals.joblib import Memory from sklearn.datasets import load_svmlight_file mem = Memory(\"./mycache\")  @mem.cache def get_data(): data = load_svmlight_file(\"mysvmlightfile\") return data[0], data[1]  X, y = get_data() '",
        "name": "X",
        "shape": "n_samples, n_features",
        "type": "scipy"
      },
      "tags": [
        "datasets",
        "svmlight_format"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.learning_curve.learning_curve",
      "description": "'Learning curve.\n\n.. deprecated:: 0.18\nThis module will be removed in 0.20.\nUse :func:`sklearn.model_selection.learning_curve` instead.\n\nDetermines cross-validated training and test scores for different training\nset sizes.\n\nA cross-validation generator splits the whole dataset k times in training\nand test data. Subsets of the training set with varying sizes will be used\nto train the estimator and a score for each training subset size and the\ntest set will be computed. Afterwards, the scores will be averaged over\nall k runs for each training subset size.\n\nRead more in the :ref:`User Guide <learning_curves>`.\n",
      "id": "sklearn.learning_curve.learning_curve",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.learning_curve.learning_curve",
      "parameters": [
        {
          "description": "An object of that type which is cloned for each validation.  X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. ",
          "name": "estimator",
          "type": "object"
        },
        {
          "description": "Target relative to X for classification or regression; None for unsupervised learning. ",
          "name": "y",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        },
        {
          "description": "Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) ",
          "name": "train_sizes",
          "shape": "n_ticks,",
          "type": "array-like"
        },
        {
          "description": "Determines the cross-validation splitting strategy. Possible inputs for cv are:  - None, to use the default 3-fold cross-validation, - integer, to specify the number of folds. - An object to be used as a cross-validation generator. - An iterable yielding train/test splits.  For integer/None inputs, if the estimator is a classifier and ``y`` is either binary or multiclass, :class:`sklearn.model_selection.StratifiedKFold` is used. In all other cases, :class:`sklearn.model_selection.KFold` is used.  Refer :ref:`User Guide <cross_validation>` for the various cross-validation strategies that can be used here. ",
          "name": "cv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "A string (see model evaluation documentation) or a scorer callable object / function with signature ``scorer(estimator, X, y)``. ",
          "name": "scoring",
          "optional": "true",
          "type": "string"
        },
        {
          "description": "If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes. ",
          "name": "exploit_incremental_learning",
          "optional": "true",
          "type": "boolean"
        },
        {
          "description": "Number of jobs to run in parallel (default 1). ",
          "name": "n_jobs",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like \\'2*n_jobs\\'. ",
          "name": "pre_dispatch",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Controls the verbosity: the higher, the more messages. ",
          "name": "verbose",
          "optional": "true",
          "type": "integer"
        },
        {
          "description": "Value to assign to the score if an error occurs in estimator fitting. If set to \\'raise\\', the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. ",
          "name": "error_score",
          "type": ""
        }
      ],
      "returns": {
        "description": "Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed.  train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets.  test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set.  Notes ----- See :ref:`examples/model_selection/plot_learning_curve.py <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>` '",
        "name": "train_sizes_abs",
        "shape": "n_unique_ticks,",
        "type": "array"
      },
      "tags": [
        "learning_curve"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.decomposition.nmf.non_negative_factorization",
      "description": "'Compute Non-negative Matrix Factorization (NMF)\n\nFind two non-negative matrices (W, H) whose product approximates the non-\nnegative matrix X. This factorization can be used for example for\ndimensionality reduction, source separation or topic extraction.\n\nThe objective function is::\n\n0.5 * ||X - WH||_Fro^2\n+ alpha * l1_ratio * ||vec(W)||_1\n+ alpha * l1_ratio * ||vec(H)||_1\n+ 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n+ 0.5 * alpha * (1 - l1_ratio) * ||H||_Fro^2\n\nWhere::\n\n||A||_Fro^2 = \\\\sum_{i,j} A_{ij}^2 (Frobenius norm)\n||vec(A)||_1 = \\\\sum_{i,j} abs(A_{ij}) (Elementwise L1 norm)\n\nThe objective function is minimized with an alternating minimization of W\nand H. If H is given and update_H=False, it solves for W only.\n",
      "id": "sklearn.decomposition.nmf.non_negative_factorization",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.decomposition.nmf.non_negative_factorization",
      "parameters": [
        {
          "description": "Constant matrix.  W : array-like, shape (n_samples, n_components) If init=\\'custom\\', it is used as initial guess for the solution.  H : array-like, shape (n_components, n_features) If init=\\'custom\\', it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Number of components, if n_components is not set all features are kept. ",
          "name": "n_components",
          "type": "integer"
        },
        {
          "description": "Method used to initialize the procedure. Default: \\'nndsvd\\' if n_components < n_features, otherwise random. Valid options:  - \\'random\\': non-negative random matrices, scaled with: sqrt(X.mean() / n_components)  - \\'nndsvd\\': Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness)  - \\'nndsvda\\': NNDSVD with zeros filled with the average of X (better when sparsity is not desired)  - \\'nndsvdar\\': NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)  - \\'custom\\': use custom matrices W and H  update_H : boolean, default: True Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated. ",
          "name": "init",
          "type": ""
        },
        {
          "description": "Numerical solver to use: \\'pg\\' is a (deprecated) Projected Gradient solver. \\'cd\\' is a Coordinate Descent solver. ",
          "name": "solver",
          "type": ""
        },
        {
          "description": "Tolerance of the stopping condition. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations before timing out. ",
          "name": "max_iter",
          "type": "integer"
        },
        {
          "description": "Constant that multiplies the regularization terms.  l1_ratio : double, default: 0. The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2. ",
          "name": "alpha",
          "type": "double"
        },
        {
          "description": "Select whether the regularization affects the components (H), the transformation (W), both or none of them. ",
          "name": "regularization",
          "type": ""
        },
        {
          "description": "Random number generator seed control. ",
          "name": "random_state",
          "type": "integer"
        },
        {
          "description": "The verbosity level. ",
          "name": "verbose",
          "type": "integer"
        },
        {
          "description": "If true, randomize the order of coordinates in the CD solver. ",
          "name": "shuffle",
          "type": "boolean"
        },
        {
          "description": "Number of iterations in NLS subproblem. Used only in the deprecated \\'pg\\' solver. ",
          "name": "nls_max_iter",
          "type": "integer"
        },
        {
          "description": "Where to enforce sparsity in the model. Used only in the deprecated \\'pg\\' solver. ",
          "name": "sparseness",
          "type": ""
        },
        {
          "description": "Degree of sparseness, if sparseness is not None. Larger values mean more sparseness. Used only in the deprecated \\'pg\\' solver. ",
          "name": "beta",
          "type": "double"
        },
        {
          "description": "Degree of correctness to maintain, if sparsity is not None. Smaller values mean larger error. Used only in the deprecated \\'pg\\' solver. ",
          "name": "eta",
          "type": "double"
        }
      ],
      "returns": {
        "description": "Solution to the non-negative least squares problem.  H : array-like, shape (n_components, n_features) Solution to the non-negative least squares problem.  n_iter : int Actual number of iterations.  References ---------- C.-J. Lin. Projected gradient methods for non-negative matrix factorization. Neural Computation, 19(2007), 2756-2779. http://www.csie.ntu.edu.tw/~cjlin/nmf/  Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for large scale nonnegative matrix and tensor factorizations.\" IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009. '",
        "name": "W",
        "shape": "n_samples, n_components",
        "type": "array-like"
      },
      "tags": [
        "decomposition",
        "nmf"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regression",
        "classification"
      ],
      "attributes": [
        {
          "description": "The feature importances (the higher, the more important the feature). ",
          "name": "feature_importances_",
          "shape": "n_features",
          "type": "array"
        },
        {
          "description": "The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. ``oob_improvement_[0]`` is the improvement in loss of the first stage over the ``init`` estimator. ",
          "name": "oob_improvement_",
          "shape": "n_estimators",
          "type": "array"
        },
        {
          "description": "The i-th score ``train_score_[i]`` is the deviance (= loss) of the model at iteration ``i`` on the in-bag sample. If ``subsample == 1`` this is the deviance on the training data. ",
          "name": "train_score_",
          "shape": "n_estimators",
          "type": "array"
        },
        {
          "description": "The concrete ``LossFunction`` object. ",
          "name": "loss_",
          "type": ""
        },
        {
          "description": "The estimator that provides the initial predictions. Set via the ``init`` argument or ``loss.init_estimator``. ",
          "name": "init",
          "type": ""
        },
        {
          "description": "The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary classification, otherwise n_classes.   See also -------- sklearn.tree.DecisionTreeClassifier, RandomForestClassifier AdaBoostClassifier ",
          "name": "estimators_",
          "shape": "n_estimators, ``loss_.K``",
          "type": "ndarray"
        }
      ],
      "category": "ensemble.gradient_boosting",
      "common_name": "Gradient Boosting Classifier",
      "description": "'Gradient Boosting for classification.\n\nGB builds an additive model in a\nforward stage-wise fashion; it allows for the optimization of\narbitrary differentiable loss functions. In each stage ``n_classes_``\nregression trees are fit on the negative gradient of the\nbinomial or multinomial deviance loss function. Binary classification\nis a special case where only a single regression tree is induced.\n\nRead more in the :ref:`User Guide <gradient_boosting>`.\n",
      "handles_classification": true,
      "handles_multiclass": true,
      "handles_multilabel": true,
      "handles_regression": false,
      "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier",
      "input_type": [
        "DENSE",
        "UNSIGNED_DATA"
      ],
      "is_class": true,
      "is_deterministic": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Apply trees in the ensemble to X, return leaf indices.\n\n.. versionadded:: 0.17\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.apply",
          "name": "apply",
          "parameters": [
            {
              "description": "The input samples. Internally, its dtype will be converted to ``dtype=np.float32``. If a sparse matrix is provided, it will be converted to a sparse ``csr_matrix``. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1. '",
            "name": "X_leaves",
            "shape": "n_samples, n_estimators, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Compute the decision function of ``X``.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification produce an array of shape [n_samples]. '",
            "name": "score",
            "shape": "n_samples, n_classes",
            "type": "array"
          }
        },
        {
          "description": "'Fit the gradient boosting model.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vectors, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "Target values (integers in classification, real numbers in regression) For classification, labels must correspond to classes. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node. ",
              "name": "sample_weight",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of ``_fit_stages`` as keyword arguments ``callable(i, self, locals())``. If the callable returns ``True`` the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting. ",
              "name": "monitor",
              "optional": "true",
              "type": "callable"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class for X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted values. '",
            "name": "y: array of shape = [\"n_samples]"
          }
        },
        {
          "description": "'Predict class log-probabilities for X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "The input samples.  Raises ------ AttributeError If the ``loss`` does not support probabilities. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Predict class probabilities for X.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "The input samples.  Raises ------ AttributeError If the ``loss`` does not support probabilities. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The class probabilities of the input samples. The order of the classes corresponds to that in the attribute `classes_`. '",
            "name": "p",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Compute decision function of ``X`` for each iteration.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_decision_function",
          "name": "staged_decision_function",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The decision function of the input samples. The order of the classes corresponds to that in the attribute `classes_`. Regression and binary classification are special cases with ``k == 1``, otherwise ``k==n_classes``. '",
            "name": "score",
            "shape": "n_samples, k",
            "type": "generator"
          }
        },
        {
          "description": "'Predict class at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_predict",
          "name": "staged_predict",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted value of the input samples. '",
            "name": "y",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'Predict class probabilities at each stage for X.\n\nThis method allows monitoring (i.e. determine error on testing set)\nafter each stage.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.staged_predict_proba",
          "name": "staged_predict_proba",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "The predicted value of the input samples. '",
            "name": "y",
            "shape": "n_samples",
            "type": "generator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.ensemble.gradient_boosting.GradientBoostingClassifier",
      "output_type": [
        "PREDICTIONS"
      ],
      "parameters": [
        {
          "default": "\\'deviance\\'",
          "description": "loss function to be optimized. \\'deviance\\' refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss \\'exponential\\' gradient boosting recovers the AdaBoost algorithm. ",
          "name": "loss",
          "optional": "true",
          "type": "\\'deviance\\', \\'exponential\\'"
        },
        {
          "default": "0.1",
          "description": "learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between learning_rate and n_estimators. ",
          "name": "learning_rate",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. ",
          "name": "n_estimators",
          "type": "int"
        },
        {
          "default": "3",
          "description": "maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. ",
          "name": "max_depth",
          "optional": "true",
          "type": "integer"
        },
        {
          "default": "\"friedman_mse\"",
          "description": "The function to measure the quality of a split. Supported criteria are \"friedman_mse\" for the mean squared error with improvement score by Friedman, \"mse\" for mean squared error, and \"mae\" for the mean absolute error. The default value of \"friedman_mse\" is generally the best as it can provide a better approximation in some cases.  .. versionadded:: 0.18 ",
          "name": "criterion",
          "optional": "true",
          "type": "string"
        },
        {
          "default": "2",
          "description": "The minimum number of samples required to split an internal node:  - If int, then consider `min_samples_split` as the minimum number. - If float, then `min_samples_split` is a percentage and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_split",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1",
          "description": "The minimum number of samples required to be at a leaf node:  - If int, then consider `min_samples_leaf` as the minimum number. - If float, then `min_samples_leaf` is a percentage and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.  .. versionchanged:: 0.18 Added float values for percentages. ",
          "name": "min_samples_leaf",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "0.",
          "description": "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. ",
          "name": "min_weight_fraction_leaf",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "1.0",
          "description": "The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. `subsample` interacts with the parameter `n_estimators`. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias. ",
          "name": "subsample",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "The number of features to consider when looking for the best split:  - If int, then consider `max_features` features at each split. - If float, then `max_features` is a percentage and `int(max_features * n_features)` features are considered at each split. - If \"auto\", then `max_features=sqrt(n_features)`. - If \"sqrt\", then `max_features=sqrt(n_features)`. - If \"log2\", then `max_features=log2(n_features)`. - If None, then `max_features=n_features`.  Choosing `max_features < n_features` leads to a reduction of variance and an increase in bias.  Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than ``max_features`` features. ",
          "name": "max_features",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "None",
          "description": "Grow trees with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. ",
          "name": "max_leaf_nodes",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "1e-7",
          "description": "Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.  .. versionadded:: 0.18 ",
          "name": "min_impurity_split",
          "optional": "true",
          "type": "float"
        },
        {
          "default": "None",
          "description": "An estimator object that is used to compute the initial predictions. ``init`` has to provide ``fit`` and ``predict``. If None it uses ``loss.init_estimator``. ",
          "name": "init",
          "optional": "true",
          "type": ""
        },
        {
          "description": "Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "When set to ``True``, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. ",
          "name": "warm_start",
          "type": "bool"
        },
        {
          "default": "None",
          "description": "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by `np.random`. ",
          "name": "random_state",
          "optional": "true",
          "type": "int"
        },
        {
          "default": "\\'auto\\'",
          "description": "Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error.  .. versionadded:: 0.17 *presort* parameter. ",
          "name": "presort",
          "optional": "true",
          "type": "bool"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/ensemble/gradient_boosting.pyc:1265",
      "tags": [
        "ensemble",
        "gradient_boosting"
      ],
      "task_type": [
        "modeling",
        "feature extraction"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "regularization",
        "regression"
      ],
      "attributes": [
        {
          "description": "Coefficient of the features in the decision function.  `coef_` is of shape (1, n_features) when the given problem is binary. `coef_` is readonly property derived from `raw_coef_` that follows the internal memory layout of liblinear. ",
          "name": "coef_",
          "shape": "1, n_features",
          "type": "array"
        },
        {
          "description": "Intercept (a.k.a. bias) added to the decision function. It is available only when parameter intercept is set to True and is of shape(1,) when the problem is binary.  Cs_ : array Array of C i.e. inverse of regularization parameter values used for cross-validation. ",
          "name": "intercept_",
          "shape": "1,",
          "type": "array"
        },
        {
          "description": "dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the \\'multi_class\\' option is set to \\'multinomial\\', then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape ``(n_folds, len(Cs_), n_features)`` or ``(n_folds, len(Cs_), n_features + 1)`` depending on whether the intercept is fit or not. ",
          "name": "coefs_paths_",
          "shape": "n_folds, len(Cs_",
          "type": "array"
        },
        {
          "description": "dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the \\'multi_class\\' option given is \\'multinomial\\' then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape (n_folds, len(Cs))  C_ : array, shape (n_classes,) or (n_classes - 1,) Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the C\\'s that correspond to the best scores for each fold. ",
          "name": "scores_",
          "type": "dict"
        },
        {
          "description": "Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1.  See also -------- LogisticRegression ",
          "name": "n_iter_",
          "shape": "n_classes, n_folds, n_cs",
          "type": "array"
        }
      ],
      "category": "linear_model.logistic",
      "common_name": "Logistic Regression CV",
      "description": "'Logistic Regression CV (aka logit, MaxEnt) classifier.\n\nThis class implements logistic regression using liblinear, newton-cg, sag\nof lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2\nregularization with primal formulation. The liblinear solver supports both\nL1 and L2 regularization, with a dual formulation only for the L2 penalty.\n\nFor the grid of Cs values (that are set by default to be ten values in\na logarithmic scale between 1e-4 and 1e4), the best hyperparameter is\nselected by the cross-validator StratifiedKFold, but it can be changed\nusing the cv parameter. In the case of newton-cg and lbfgs solvers,\nwe warm start along the path i.e guess the initial coefficients of the\npresent fit to be the coefficients got after convergence in the previous\nfit, so it is supposed to be faster for high-dimensional dense data.\n\nFor a multiclass problem, the hyperparameters for each class are computed\nusing the best scores got by doing a one-vs-rest in parallel across all\nfolds and classes. Hence this is not the true multinomial loss.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n",
      "id": "sklearn.linear_model.logistic.LogisticRegressionCV",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'Predict confidence scores for samples.\n\nThe confidence score for a sample is the signed distance of that\nsample to the hyperplane.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.decision_function",
          "name": "decision_function",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. '",
            "name": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
          }
        },
        {
          "description": "'Convert coefficient matrix to dense array format.\n\nConverts the ``coef_`` member (back) to a numpy.ndarray. This is the\ndefault format of ``coef_`` and is required for fitting, so calling\nthis method is only required on models that have previously been\nsparsified; otherwise, it is a no-op.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.densify",
          "name": "densify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'Fit the model according to the given training data.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.fit",
          "name": "fit",
          "parameters": [
            {
              "description": "Training vector, where n_samples is the number of samples and n_features is the number of features. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            },
            {
              "description": "Target vector relative to X. ",
              "name": "y",
              "shape": "n_samples,",
              "type": "array-like"
            },
            {
              "description": "Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples,",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns self. '",
            "name": "self",
            "type": "object"
          }
        },
        {
          "description": "'Fit to data, then transform it.\n\nFits transformer to X and y with optional parameters fit_params\nand returns a transformed version of X.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.fit_transform",
          "name": "fit_transform",
          "parameters": [
            {
              "description": "Training set. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "numpy"
            },
            {
              "description": "Target values. ",
              "name": "y",
              "shape": "n_samples",
              "type": "numpy"
            }
          ],
          "returns": {
            "description": "Transformed array.  '",
            "name": "X_new",
            "shape": "n_samples, n_features_new",
            "type": "numpy"
          }
        },
        {
          "description": "'Get parameters for this estimator.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.get_params",
          "name": "get_params",
          "parameters": [
            {
              "description": "If True, will return the parameters for this estimator and contained subobjects that are estimators. ",
              "name": "deep",
              "optional": "true",
              "type": "boolean"
            }
          ],
          "returns": {
            "description": "Parameter names mapped to their values. '",
            "name": "params",
            "type": "mapping"
          }
        },
        {
          "description": "'Predict class labels for samples in X.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.predict",
          "name": "predict",
          "parameters": [
            {
              "description": "Samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like, sparse matrix"
            }
          ],
          "returns": {
            "description": "Predicted class label per sample. '",
            "name": "C",
            "shape": "n_samples",
            "type": "array"
          }
        },
        {
          "description": "'Log of probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.predict_log_proba",
          "name": "predict_log_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. '",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Probability estimates.\n\nThe returned estimates for all classes are ordered by the\nlabel of classes.\n\nFor a multi_class problem, if multi_class is set to be \"multinomial\"\nthe softmax function is used to find the predicted probability of\neach class.\nElse use a one-vs-rest approach, i.e calculate the probability\nof each class assuming it to be positive using the logistic function.\nand normalize these values across all the classes.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.predict_proba",
          "name": "predict_proba",
          "parameters": [
            {
              "description": "",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Returns the probability of the sample for each class in the model, where classes are ordered as they are in ``self.classes_``. '",
            "name": "T",
            "shape": "n_samples, n_classes",
            "type": "array-like"
          }
        },
        {
          "description": "'Returns the mean accuracy on the given test data and labels.\n\nIn multi-label classification, this is the subset accuracy\nwhich is a harsh metric since you require for each sample that\neach label set be correctly predicted.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.score",
          "name": "score",
          "parameters": [
            {
              "description": "Test samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array-like"
            },
            {
              "description": "True labels for X. ",
              "name": "y",
              "shape": "n_samples",
              "type": "array-like"
            },
            {
              "description": "Sample weights. ",
              "name": "sample_weight",
              "optional": "true",
              "shape": "n_samples",
              "type": "array-like"
            }
          ],
          "returns": {
            "description": "Mean accuracy of self.predict(X) wrt. y.  '",
            "name": "score",
            "type": "float"
          }
        },
        {
          "description": "\"Set the parameters of this estimator.\n\nThe method works on simple estimators as well as on nested objects\n(such as pipelines). The latter have parameters of the form\n``<component>__<parameter>`` so that it's possible to update each\ncomponent of a nested object.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.set_params",
          "name": "set_params",
          "parameters": [],
          "returns": {
            "description": "\"",
            "name": "self"
          }
        },
        {
          "description": "'Convert coefficient matrix to sparse format.\n\nConverts the ``coef_`` member to a scipy.sparse matrix, which for\nL1-regularized models can be much more memory- and storage-efficient\nthan the usual numpy.ndarray representation.\n\nThe ``intercept_`` member is not converted.\n\nNotes\n-----\nFor non-sparse models, i.e. when there are not many zeros in ``coef_``,\nthis may actually *increase* memory usage, so use this method with\ncare. A rule of thumb is that the number of zero elements, which can\nbe computed with ``(coef_ == 0).sum()``, must be more than 50% for this\nto provide significant benefits.\n\nAfter calling this method, further fitting with the partial_fit\nmethod (if any) will not work until you call densify.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.sparsify",
          "name": "sparsify",
          "parameters": [],
          "returns": {
            "description": "'",
            "name": "self: estimator"
          }
        },
        {
          "description": "'DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.\n\nReduce X to its most important features.\n\nUses ``coef_`` or ``feature_importances_`` to determine the most\nimportant features.  For models with a ``coef_`` for each class, the\nabsolute sum over the classes is used.\n",
          "id": "sklearn.linear_model.logistic.LogisticRegressionCV.transform",
          "name": "transform",
          "parameters": [
            {
              "description": "The input samples. ",
              "name": "X",
              "shape": "n_samples, n_features",
              "type": "array"
            },
            {
              "default": "None",
              "description": "The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If \"median\" (resp. \"mean\"), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., \"1.25*mean\") may also be used. If None and if available, the object attribute ``threshold`` is used. Otherwise, \"mean\" is used by default. ",
              "name": "threshold",
              "optional": "true",
              "type": "string"
            }
          ],
          "returns": {
            "description": "The input samples with only the selected features. '",
            "name": "X_r",
            "shape": "n_samples, n_selected_features",
            "type": "array"
          }
        }
      ],
      "name": "sklearn.linear_model.logistic.LogisticRegressionCV",
      "parameters": [
        {
          "description": "Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization. ",
          "name": "Cs",
          "type": "list"
        },
        {
          "description": "Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified.  .. versionadded:: 0.17 class_weight == \\'balanced\\' ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module :mod:`sklearn.model_selection` module for the list of possible cross-validation objects. ",
          "name": "cv",
          "type": "integer"
        },
        {
          "description": "Used to specify the norm used in the penalization. The \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solvers support only l2 penalties. ",
          "name": "penalty",
          "type": "str"
        },
        {
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. ",
          "name": "dual",
          "type": "bool"
        },
        {
          "description": "Scoring function to use as cross-validation criteria. For a list of scoring functions that can be used, look at :mod:`sklearn.metrics`. The default scoring option used is accuracy_score. ",
          "name": "scoring",
          "type": "callabale"
        },
        {
          "description": "Algorithm to use in the optimization problem.  - For small datasets, \\'liblinear\\' is a good choice, whereas \\'sag\\' is faster for large ones. - For multiclass problems, only \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' handle multinomial loss; \\'liblinear\\' is limited to one-versus-rest schemes. - \\'newton-cg\\', \\'lbfgs\\' and \\'sag\\' only handle L2 penalty. - \\'liblinear\\' might be slower in LogisticRegressionCV because it does not handle warm-starting.  Note that \\'sag\\' fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing.  .. versionadded:: 0.17 Stochastic Average Gradient descent solver. ",
          "name": "solver",
          "type": "\\'newton-cg\\', \\'lbfgs\\', \\'liblinear\\', \\'sag\\'"
        },
        {
          "description": "Tolerance for stopping criteria. ",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Maximum number of iterations of the optimization algorithm. ",
          "name": "max_iter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used. ",
          "name": "n_jobs",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "For the \\'liblinear\\', \\'sag\\' and \\'lbfgs\\' solvers set verbose to any positive number for verbosity. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged. ",
          "name": "refit",
          "type": "bool"
        },
        {
          "description": "Multiclass option can be either \\'ovr\\' or \\'multinomial\\'. If the option chosen is \\'ovr\\', then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solver.  .. versionadded:: 0.18 Stochastic Average Gradient descent solver for \\'multinomial\\' case. ",
          "name": "multi_class",
          "type": "str"
        },
        {
          "description": "Useful only when the solver \\'liblinear\\' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. ",
          "name": "intercept_scaling",
          "type": "float"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. ",
          "name": "random_state",
          "type": "int"
        }
      ],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/linear_model/logistic.pyc:1309",
      "tags": [
        "linear_model",
        "logistic"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.eigen.arpack.arpack.eigs",
      "description": "\"\nFind k eigenvalues and eigenvectors of the square matrix A.\n\nSolves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem\nfor w[i] eigenvalues with corresponding eigenvectors x[i].\n\nIf M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the\ngeneralized eigenvalue problem for w[i] eigenvalues\nwith corresponding eigenvectors x[i]\n",
      "id": "scipy.sparse.linalg.eigen.arpack.arpack.eigs",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.eigen.arpack.arpack.eigs",
      "parameters": [
        {
          "description": "An array, sparse matrix, or LinearOperator representing the operation ``A * x``, where A is a real or complex square matrix.",
          "name": "A",
          "type": "ndarray"
        },
        {
          "description": "The number of eigenvalues and eigenvectors desired. `k` must be smaller than N. It is not possible to compute all eigenvectors of a matrix. M : ndarray, sparse matrix or LinearOperator, optional An array, sparse matrix, or LinearOperator representing the operation M*x for the generalized eigenvalue problem  A * x = w * M * x.  M must represent a real, symmetric matrix if A is real, and must represent a complex, hermitian matrix if A is complex. For best results, the data type of M should be the same as that of A. Additionally:  If `sigma` is None, M is positive definite  If sigma is specified, M is positive semi-definite  If sigma is None, eigs requires an operator to compute the solution of the linear equation ``M * x = b``.  This is done internally via a (sparse) LU decomposition for an explicit matrix M, or via an iterative solver for a general linear operator.  Alternatively, the user can supply the matrix or operator Minv, which gives ``x = Minv * b = M^-1 * b``.",
          "name": "k",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Find eigenvalues near sigma using shift-invert mode.  This requires an operator to compute the solution of the linear system ``[A - sigma * M] * x = b``, where M is the identity matrix if unspecified. This is computed internally via a (sparse) LU decomposition for explicit matrices A & M, or via an iterative solver if either A or M is a general linear operator. Alternatively, the user can supply the matrix or operator OPinv, which gives ``x = OPinv * b = [A - sigma * M]^-1 * b``. For a real matrix A, shift-invert can either be done in imaginary mode or real mode, specified by the parameter OPpart ('r' or 'i'). Note that when sigma is specified, the keyword 'which' (below) refers to the shifted eigenvalues ``w'[i]`` where:  If A is real and OPpart == 'r' (default), ``w'[i] = 1/2 * [1/(w[i]-sigma) + 1/(w[i]-conj(sigma))]``.  If A is real and OPpart == 'i', ``w'[i] = 1/2i * [1/(w[i]-sigma) - 1/(w[i]-conj(sigma))]``.  If A is complex, ``w'[i] = 1/(w[i]-sigma)``.  v0 : ndarray, optional Starting vector for iteration. Default: random",
          "name": "sigma",
          "optional": "true",
          "type": "real"
        },
        {
          "description": "The number of Lanczos vectors generated `ncv` must be greater than `k`; it is recommended that ``ncv > 2*k``. Default: ``min(n, 2*k + 1)``",
          "name": "ncv",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Which `k` eigenvectors and eigenvalues to find:  'LM' : largest magnitude  'SM' : smallest magnitude  'LR' : largest real part  'SR' : smallest real part  'LI' : largest imaginary part  'SI' : smallest imaginary part  When sigma != None, 'which' refers to the shifted eigenvalues w'[i] (see discussion in 'sigma', above).  ARPACK is generally better at finding large values than small values.  If small eigenvalues are desired, consider using shift-invert mode for better performance.",
          "name": "which",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "Maximum number of Arnoldi update iterations allowed Default: ``n*10``",
          "name": "maxiter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Relative accuracy for eigenvalues (stopping criterion) The default value of 0 implies machine precision.",
          "name": "tol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Return eigenvectors (True) in addition to eigenvalues Minv : ndarray, sparse matrix or LinearOperator, optional See notes in M, above. OPinv : ndarray, sparse matrix or LinearOperator, optional See notes in sigma, above. OPpart : {'r' or 'i'}, optional See notes in sigma, above ",
          "name": "return_eigenvectors",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "Array of k eigenvalues. v : ndarray An array of `k` eigenvectors. ``v[:, i]`` is the eigenvector corresponding to the eigenvalue w[i].  Raises ------ ArpackNoConvergence When the requested convergence is not obtained. The currently converged eigenvalues and eigenvectors can be found as ``eigenvalues`` and ``eigenvectors`` attributes of the exception object.  See Also -------- eigsh : eigenvalues and eigenvectors for symmetric matrix A svds : singular value decomposition for a matrix A  Notes ----- This function is a wrapper to the ARPACK [1]_ SNEUPD, DNEUPD, CNEUPD, ZNEUPD, functions which use the Implicitly Restarted Arnoldi Method to find the eigenvalues and eigenvectors [2]_.  References ---------- .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/ .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE: Solution of Large Scale Eigenvalue Problems by Implicitly Restarted Arnoldi Methods. SIAM, Philadelphia, PA, 1998.  Examples -------- Find 6 eigenvectors of the identity matrix:  >>> import scipy.sparse as sparse >>> id = np.eye(13) >>> vals, vecs = sparse.linalg.eigs(id, k=6) >>> vals array([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j]) >>> vecs.shape (13, 6)  \"",
        "name": "w",
        "type": "ndarray"
      },
      "tags": [
        "sparse",
        "linalg",
        "eigen",
        "arpack",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.spatial.distance.pdist",
      "description": "\"\nPairwise distances between observations in n-dimensional space.\n\nThe following are common calling conventions.\n\n1. ``Y = pdist(X, 'euclidean')``\n\nComputes the distance between m points using Euclidean distance\n(2-norm) as the distance metric between the points. The points\nare arranged as m n-dimensional row vectors in the matrix X.\n\n2. ``Y = pdist(X, 'minkowski', p)``\n\nComputes the distances using the Minkowski distance\n:math:`||u-v||_p` (p-norm) where :math:`p \\\\geq 1`.\n\n3. ``Y = pdist(X, 'cityblock')``\n\nComputes the city block or Manhattan distance between the\npoints.\n\n4. ``Y = pdist(X, 'seuclidean', V=None)``\n\nComputes the standardized Euclidean distance. The standardized\nEuclidean distance between two n-vectors ``u`` and ``v`` is\n\n.. math::\n\n\\\\sqrt{\\\\sum {(u_i-v_i)^2 / V[x_i]}}\n\n\nV is the variance vector; V[i] is the variance computed over all\nthe i'th components of the points.  If not passed, it is\nautomatically computed.\n\n5. ``Y = pdist(X, 'sqeuclidean')``\n\nComputes the squared Euclidean distance :math:`||u-v||_2^2` between\nthe vectors.\n\n6. ``Y = pdist(X, 'cosine')``\n\nComputes the cosine distance between vectors u and v,\n\n.. math::\n\n1 - \\\\frac{u \\\\cdot v}\n{{||u||}_2 {||v||}_2}\n\nwhere :math:`||*||_2` is the 2-norm of its argument ``*``, and\n:math:`u \\\\cdot v` is the dot product of ``u`` and ``v``.\n\n7. ``Y = pdist(X, 'correlation')``\n\nComputes the correlation distance between vectors u and v. This is\n\n.. math::\n\n1 - \\\\frac{(u - \\\\bar{u}) \\\\cdot (v - \\\\bar{v})}\n{{||(u - \\\\bar{u})||}_2 {||(v - \\\\bar{v})||}_2}\n\nwhere :math:`\\\\bar{v}` is the mean of the elements of vector v,\nand :math:`x \\\\cdot y` is the dot product of :math:`x` and :math:`y`.\n\n8. ``Y = pdist(X, 'hamming')``\n\nComputes the normalized Hamming distance, or the proportion of\nthose vector elements between two n-vectors ``u`` and ``v``\nwhich disagree. To save memory, the matrix ``X`` can be of type\nboolean.\n\n9. ``Y = pdist(X, 'jaccard')``\n\nComputes the Jaccard distance between the points. Given two\nvectors, ``u`` and ``v``, the Jaccard distance is the\nproportion of those elements ``u[i]`` and ``v[i]`` that\ndisagree where at least one of them is non-zero.\n\n10. ``Y = pdist(X, 'chebyshev')``\n\nComputes the Chebyshev distance between the points. The\nChebyshev distance between two n-vectors ``u`` and ``v`` is the\nmaximum norm-1 distance between their respective elements. More\nprecisely, the distance is given by\n\n.. math::\n\nd(u,v) = \\\\max_i {|u_i-v_i|}\n\n11. ``Y = pdist(X, 'canberra')``\n\nComputes the Canberra distance between the points. The\nCanberra distance between two points ``u`` and ``v`` is\n\n.. math::\n\nd(u,v) = \\\\sum_i \\\\frac{|u_i-v_i|}\n{|u_i|+|v_i|}\n\n\n12. ``Y = pdist(X, 'braycurtis')``\n\nComputes the Bray-Curtis distance between the points. The\nBray-Curtis distance between two points ``u`` and ``v`` is\n\n\n.. math::\n\nd(u,v) = \\\\frac{\\\\sum_i {u_i-v_i}}\n{\\\\sum_i {u_i+v_i}}\n\n13. ``Y = pdist(X, 'mahalanobis', VI=None)``\n\nComputes the Mahalanobis distance between the points. The\nMahalanobis distance between two points ``u`` and ``v`` is\n:math:`(u-v)(1/V)(u-v)^T` where :math:`(1/V)` (the ``VI``\nvariable) is the inverse covariance. If ``VI`` is not None,\n``VI`` will be used as the inverse covariance matrix.\n\n14. ``Y = pdist(X, 'yule')``\n\nComputes the Yule distance between each pair of boolean\nvectors. (see yule function documentation)\n\n15. ``Y = pdist(X, 'matching')``\n\nComputes the matching distance between each pair of boolean\nvectors. (see matching function documentation)\n\n16. ``Y = pdist(X, 'dice')``\n\nComputes the Dice distance between each pair of boolean\nvectors. (see dice function documentation)\n\n17. ``Y = pdist(X, 'kulsinski')``\n\nComputes the Kulsinski distance between each pair of\nboolean vectors. (see kulsinski function documentation)\n\n18. ``Y = pdist(X, 'rogerstanimoto')``\n\nComputes the Rogers-Tanimoto distance between each pair of\nboolean vectors. (see rogerstanimoto function documentation)\n\n19. ``Y = pdist(X, 'russellrao')``\n\nComputes the Russell-Rao distance between each pair of\nboolean vectors. (see russellrao function documentation)\n\n20. ``Y = pdist(X, 'sokalmichener')``\n\nComputes the Sokal-Michener distance between each pair of\nboolean vectors. (see sokalmichener function documentation)\n\n21. ``Y = pdist(X, 'sokalsneath')``\n\nComputes the Sokal-Sneath distance between each pair of\nboolean vectors. (see sokalsneath function documentation)\n\n22. ``Y = pdist(X, 'wminkowski')``\n\nComputes the weighted Minkowski distance between each pair of\nvectors. (see wminkowski function documentation)\n\n23. ``Y = pdist(X, f)``\n\nComputes the distance between all pairs of vectors in X\nusing the user supplied 2-arity function f. For example,\nEuclidean distance between the vectors could be computed\nas follows::\n\ndm = pdist(X, lambda u, v: np.sqrt(((u-v)**2).sum()))\n\nNote that you should avoid passing a reference to one of\nthe distance functions defined in this library. For example,::\n\ndm = pdist(X, sokalsneath)\n\nwould calculate the pair-wise distances between the vectors in\nX using the Python function sokalsneath. This would result in\nsokalsneath being called :math:`{n \\\\choose 2}` times, which\nis inefficient. Instead, the optimized C version is more\nefficient, and we call it using the following syntax.::\n\ndm = pdist(X, 'sokalsneath')\n",
      "id": "scipy.spatial.distance.pdist",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.spatial.distance.pdist",
      "parameters": [
        {
          "description": "An m by n array of m original observations in an n-dimensional space.",
          "name": "X",
          "type": "ndarray"
        },
        {
          "description": "The distance metric to use. The distance function can be 'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'.",
          "name": "metric",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "The weight vector (for weighted Minkowski).",
          "name": "w",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "The p-norm to apply (for Minkowski, weighted and unweighted) V : ndarray, optional The variance vector (for standardized Euclidean). VI : ndarray, optional The inverse of the covariance matrix (for Mahalanobis). ",
          "name": "p",
          "optional": "true",
          "type": "double"
        }
      ],
      "returns": {
        "description": "Returns a condensed distance matrix Y.  For each :math:`i` and :math:`j` (where :math:`i<j<n`), the metric ``dist(u=X[i], v=X[j])`` is computed and stored in entry ``ij``.  See Also -------- squareform : converts between condensed distance matrices and square distance matrices.  Notes ----- See ``squareform`` for information on how to calculate the index of this entry or to convert the condensed distance matrix to a redundant square matrix.  \"",
        "name": "Y",
        "type": "ndarray"
      },
      "tags": [
        "spatial",
        "distance"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.optimize.lbfgsb.fmin_l_bfgs_b",
      "description": "\"\nMinimize a function func using the L-BFGS-B algorithm.\n",
      "id": "scipy.optimize.lbfgsb.fmin_l_bfgs_b",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.optimize.lbfgsb.fmin_l_bfgs_b",
      "parameters": [
        {
          "description": "Function to minimise. x0 : ndarray Initial guess.",
          "name": "func",
          "type": "callable"
        },
        {
          "description": "The gradient of `func`.  If None, then `func` returns the function value and the gradient (``f, g = func(x, *args)``), unless `approx_grad` is True in which case `func` returns only ``f``.",
          "name": "fprime",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Arguments to pass to `func` and `fprime`.",
          "name": "args",
          "optional": "true",
          "type": "sequence"
        },
        {
          "description": "Whether to approximate the gradient numerically (in which case `func` returns only the function value).",
          "name": "approx_grad",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "``(min, max)`` pairs for each element in ``x``, defining the bounds on that parameter. Use None or +-inf for one of ``min`` or ``max`` when there is no bound in that direction.",
          "name": "bounds",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The maximum number of variable metric corrections used to define the limited memory matrix. (The limited memory BFGS method does not store the full hessian but uses this many terms in an approximation to it.)",
          "name": "m",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "The iteration stops when ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``, where ``eps`` is the machine precision, which is automatically generated by the code. Typical values for `factr` are: 1e12 for low accuracy; 1e7 for moderate accuracy; 10.0 for extremely high accuracy.",
          "name": "factr",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "The iteration will stop when ``max{|proj g_i | i = 1, ..., n} <= pgtol`` where ``pg_i`` is the i-th component of the projected gradient.",
          "name": "pgtol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Step size used when `approx_grad` is True, for numerically calculating the gradient",
          "name": "epsilon",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Controls the frequency of output. ``iprint < 0`` means no output; ``iprint = 0``    print only one line at the last iteration; ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations; ``iprint = 99``   print details of every iteration except n-vectors; ``iprint = 100``  print also the changes of active set and final x; ``iprint > 100``  print details of every iteration including x and g.",
          "name": "iprint",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "If zero, then no output.  If a positive number, then this over-rides `iprint` (i.e., `iprint` gets the value of `disp`).",
          "name": "disp",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of function evaluations.",
          "name": "maxfun",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Maximum number of iterations.",
          "name": "maxiter",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Called after each iteration, as ``callback(xk)``, where ``xk`` is the current parameter vector.",
          "name": "callback",
          "optional": "true",
          "type": "callable"
        },
        {
          "description": "Maximum number of line search steps (per iteration). Default is 20. ",
          "name": "maxls",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Estimated position of the minimum. f : float Value of `func` at the minimum. d : dict Information dictionary.  * d['warnflag'] is  - 0 if converged, - 1 if too many function evaluations or too many iterations, - 2 if stopped for another reason, given in d['task']  * d['grad'] is the gradient at the minimum (should be 0 ish) * d['funcalls'] is the number of function calls made. * d['nit'] is the number of iterations.  See also -------- minimize: Interface to minimization algorithms for multivariate functions. See the 'L-BFGS-B' `method` in particular.  Notes ----- License of L-BFGS-B (FORTRAN code):  The version included here (in fortran code) is 3.0 (released April 25, 2011).  It was written by Ciyou Zhu, Richard Byrd, and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following condition for use:  This software is freely available, but we expect that all publications describing work using this software, or all commercial products using it, quote at least one of the references given below. This software is released under the BSD License.  References ---------- * R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound Constrained Optimization, (1995), SIAM Journal on Scientific and Statistical Computing, 16, 5, pp. 1190-1208. * C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization (1997), ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560. * J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B, FORTRAN routines for large scale bound constrained optimization (2011), ACM Transactions on Mathematical Software, 38, 1.  \"",
        "name": "x",
        "type": "array"
      },
      "tags": [
        "optimize",
        "lbfgsb"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.eigen.arpack.arpack.eigsh",
      "description": "\"\nFind k eigenvalues and eigenvectors of the real symmetric square matrix\nor complex hermitian matrix A.\n\nSolves ``A * x[i] = w[i] * x[i]``, the standard eigenvalue problem for\nw[i] eigenvalues with corresponding eigenvectors x[i].\n\nIf M is specified, solves ``A * x[i] = w[i] * M * x[i]``, the\ngeneralized eigenvalue problem for w[i] eigenvalues\nwith corresponding eigenvectors x[i]\n",
      "id": "scipy.sparse.linalg.eigen.arpack.arpack.eigsh",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.eigen.arpack.arpack.eigsh",
      "parameters": [
        {
          "description": "the operation A * x, where A is a real symmetric matrix For buckling mode (see below) A must additionally be positive-definite",
          "name": "A",
          "type": ""
        },
        {
          "description": "The number of eigenvalues and eigenvectors desired. `k` must be smaller than N. It is not possible to compute all eigenvectors of a matrix. ",
          "name": "k",
          "optional": "true",
          "type": "int"
        }
      ],
      "returns": {
        "description": "Array of k eigenvalues v : array An array representing the `k` eigenvectors.  The column ``v[:, i]`` is the eigenvector corresponding to the eigenvalue ``w[i]``.  Other Parameters ---------------- M : An N x N matrix, array, sparse matrix, or linear operator representing the operation M * x for the generalized eigenvalue problem  A * x = w * M * x.  M must represent a real, symmetric matrix if A is real, and must represent a complex, hermitian matrix if A is complex. For best results, the data type of M should be the same as that of A. Additionally:  If sigma is None, M is symmetric positive definite  If sigma is specified, M is symmetric positive semi-definite  In buckling mode, M is symmetric indefinite.  If sigma is None, eigsh requires an operator to compute the solution of the linear equation ``M * x = b``. This is done internally via a (sparse) LU decomposition for an explicit matrix M, or via an iterative solver for a general linear operator.  Alternatively, the user can supply the matrix or operator Minv, which gives ``x = Minv * b = M^-1 * b``. sigma : real Find eigenvalues near sigma using shift-invert mode.  This requires an operator to compute the solution of the linear system `[A - sigma * M] x = b`, where M is the identity matrix if unspecified.  This is computed internally via a (sparse) LU decomposition for explicit matrices A & M, or via an iterative solver if either A or M is a general linear operator. Alternatively, the user can supply the matrix or operator OPinv, which gives ``x = OPinv * b = [A - sigma * M]^-1 * b``. Note that when sigma is specified, the keyword 'which' refers to the shifted eigenvalues ``w'[i]`` where:  if mode == 'normal', ``w'[i] = 1 / (w[i] - sigma)``.  if mode == 'cayley', ``w'[i] = (w[i] + sigma) / (w[i] - sigma)``.  if mode == 'buckling', ``w'[i] = w[i] / (w[i] - sigma)``.  (see further discussion in 'mode' below) v0 : ndarray, optional Starting vector for iteration. Default: random ncv : int, optional The number of Lanczos vectors generated ncv must be greater than k and smaller than n; it is recommended that ``ncv > 2*k``. Default: ``min(n, 2*k + 1)`` which : str ['LM' | 'SM' | 'LA' | 'SA' | 'BE'] If A is a complex hermitian matrix, 'BE' is invalid. Which `k` eigenvectors and eigenvalues to find:  'LM' : Largest (in magnitude) eigenvalues  'SM' : Smallest (in magnitude) eigenvalues  'LA' : Largest (algebraic) eigenvalues  'SA' : Smallest (algebraic) eigenvalues  'BE' : Half (k/2) from each end of the spectrum  When k is odd, return one more (k/2+1) from the high end. When sigma != None, 'which' refers to the shifted eigenvalues ``w'[i]`` (see discussion in 'sigma', above).  ARPACK is generally better at finding large values than small values.  If small eigenvalues are desired, consider using shift-invert mode for better performance. maxiter : int, optional Maximum number of Arnoldi update iterations allowed Default: ``n*10`` tol : float Relative accuracy for eigenvalues (stopping criterion). The default value of 0 implies machine precision. Minv : N x N matrix, array, sparse matrix, or LinearOperator See notes in M, above OPinv : N x N matrix, array, sparse matrix, or LinearOperator See notes in sigma, above. return_eigenvectors : bool Return eigenvectors (True) in addition to eigenvalues mode : string ['normal' | 'buckling' | 'cayley'] Specify strategy to use for shift-invert mode.  This argument applies only for real-valued A and sigma != None.  For shift-invert mode, ARPACK internally solves the eigenvalue problem ``OP * x'[i] = w'[i] * B * x'[i]`` and transforms the resulting Ritz vectors x'[i] and Ritz values w'[i] into the desired eigenvectors and eigenvalues of the problem ``A * x[i] = w[i] * M * x[i]``. The modes are as follows:  'normal' : OP = [A - sigma * M]^-1 * M, B = M, w'[i] = 1 / (w[i] - sigma)  'buckling' : OP = [A - sigma * M]^-1 * A, B = A, w'[i] = w[i] / (w[i] - sigma)  'cayley' : OP = [A - sigma * M]^-1 * [A + sigma * M], B = M, w'[i] = (w[i] + sigma) / (w[i] - sigma)  The choice of mode will affect which eigenvalues are selected by the keyword 'which', and can also impact the stability of convergence (see [2] for a discussion)  Raises ------ ArpackNoConvergence When the requested convergence is not obtained.  The currently converged eigenvalues and eigenvectors can be found as ``eigenvalues`` and ``eigenvectors`` attributes of the exception object.  See Also -------- eigs : eigenvalues and eigenvectors for a general (nonsymmetric) matrix A svds : singular value decomposition for a matrix A  Notes ----- This function is a wrapper to the ARPACK [1]_ SSEUPD and DSEUPD functions which use the Implicitly Restarted Lanczos Method to find the eigenvalues and eigenvectors [2]_.  References ---------- .. [1] ARPACK Software, http://www.caam.rice.edu/software/ARPACK/ .. [2] R. B. Lehoucq, D. C. Sorensen, and C. Yang,  ARPACK USERS GUIDE: Solution of Large Scale Eigenvalue Problems by Implicitly Restarted Arnoldi Methods. SIAM, Philadelphia, PA, 1998.  Examples -------- >>> import scipy.sparse as sparse >>> id = np.eye(13) >>> vals, vecs = sparse.linalg.eigsh(id, k=6) >>> vals array([ 1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j,  1.+0.j]) >>> vecs.shape (13, 6)  \"",
        "name": "w",
        "type": "array"
      },
      "tags": [
        "sparse",
        "linalg",
        "eigen",
        "arpack",
        "arpack"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.linear_model.logistic.logistic_regression_path",
      "description": "'Compute a Logistic Regression model for a list of regularization\nparameters.\n\nThis is an implementation that uses the result of the previous model\nto speed up computations along the set of solutions, making it faster\nthan sequentially calling LogisticRegression for the different parameters.\nNote that there will be no speedup with liblinear solver, since it does\nnot handle warm-starting.\n\nRead more in the :ref:`User Guide <logistic_regression>`.\n",
      "id": "sklearn.linear_model.logistic.logistic_regression_path",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.linear_model.logistic.logistic_regression_path",
      "parameters": [
        {
          "description": "Input data. ",
          "name": "X",
          "shape": "n_samples, n_features",
          "type": "array-like"
        },
        {
          "description": "Input data, target values.  Cs : int | array-like, shape (n_cs,) List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4. ",
          "name": "y",
          "shape": "n_samples,",
          "type": "array-like"
        },
        {
          "description": "The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary. ",
          "name": "pos_class",
          "type": "int"
        },
        {
          "description": "Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1). ",
          "name": "fit_intercept",
          "type": "bool"
        },
        {
          "description": "Maximum number of iterations for the solver. ",
          "name": "max_iter",
          "type": "int"
        },
        {
          "description": "Stopping criterion. For the newton-cg and lbfgs solvers, the iteration will stop when ``max{|g_i | i = 1, ..., n} <= tol`` where ``g_i`` is the i-th component of the gradient. ",
          "name": "tol",
          "type": "float"
        },
        {
          "description": "For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. ",
          "name": "verbose",
          "type": "int"
        },
        {
          "description": "Numerical solver to use. ",
          "name": "solver",
          "type": "\\'lbfgs\\', \\'newton-cg\\', \\'liblinear\\', \\'sag\\'"
        },
        {
          "description": "Initialization value for coefficients of logistic regression. Useless for liblinear solver. ",
          "name": "coef",
          "shape": "n_features,",
          "type": "array-like"
        },
        {
          "description": "Whether or not to produce a copy of the data. A copy is not required anymore. This parameter is deprecated and will be removed in 0.19. ",
          "name": "copy",
          "type": "bool"
        },
        {
          "description": "Weights associated with classes in the form ``{class_label: weight}``. If not given, all classes are supposed to have weight one.  The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.  Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. ",
          "name": "class_weight",
          "optional": "true",
          "type": "dict"
        },
        {
          "description": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. ",
          "name": "dual",
          "type": "bool"
        },
        {
          "description": "Used to specify the norm used in the penalization. The \\'newton-cg\\', \\'sag\\' and \\'lbfgs\\' solvers support only l2 penalties. ",
          "name": "penalty",
          "type": "str"
        },
        {
          "description": "Useful only when the solver \\'liblinear\\' is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a \"synthetic\" feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes ``intercept_scaling * synthetic_feature_weight``.  Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. ",
          "name": "intercept_scaling",
          "type": "float"
        },
        {
          "description": "Multiclass option can be either \\'ovr\\' or \\'multinomial\\'. If the option chosen is \\'ovr\\', then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the \\'lbfgs\\' and \\'newton-cg\\' solvers. ",
          "name": "multi_class",
          "type": "str"
        },
        {
          "description": "The seed of the pseudo random number generator to use when shuffling the data. Used only in solvers \\'sag\\' and \\'liblinear\\'. ",
          "name": "random_state",
          "type": "int"
        },
        {
          "description": "If False, the input arrays X and y will not be checked. ",
          "name": "check_input",
          "type": "bool"
        },
        {
          "description": "Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation. ",
          "name": "max_squared_sum",
          "type": "float"
        },
        {
          "description": "Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples,",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": "List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept.  Cs : ndarray Grid of Cs used for cross-validation.  n_iter : array, shape (n_cs,) Actual number of iteration for each Cs.  Notes ----- You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR which penalizes the intercept. '",
        "name": "coefs",
        "shape": "n_cs, n_features",
        "type": "ndarray"
      },
      "tags": [
        "linear_model",
        "logistic"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.eigen.lobpcg.lobpcg.lobpcg",
      "description": "'Locally Optimal Block Preconditioned Conjugate Gradient Method (LOBPCG)\n\nLOBPCG is a preconditioned eigensolver for large symmetric positive\ndefinite (SPD) generalized eigenproblems.\n",
      "id": "scipy.sparse.linalg.eigen.lobpcg.lobpcg.lobpcg",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.eigen.lobpcg.lobpcg.lobpcg",
      "parameters": [
        {
          "description": "The symmetric linear operator of the problem, usually a sparse matrix.  Often called the \"stiffness matrix\". X : array_like Initial approximation to the k eigenvectors. If A has shape=(n,n) then X should have shape shape=(n,k). B : {dense matrix, sparse matrix, LinearOperator}, optional the right hand side operator in a generalized eigenproblem. by default, B = Identity often called the \"mass matrix\" M : {dense matrix, sparse matrix, LinearOperator}, optional preconditioner to A; by default M = Identity M should approximate the inverse of A Y : array_like, optional n-by-sizeY matrix of constraints, sizeY < n The iterations will be performed in the B-orthogonal complement of the column-space of Y. Y must be full rank. ",
          "name": "A",
          "type": "sparse matrix, dense matrix, LinearOperator"
        }
      ],
      "returns": {
        "description": "Array of k eigenvalues v : array An array of k eigenvectors.  V has the same shape as X.  Other Parameters ---------------- tol : scalar, optional Solver tolerance (stopping criterion) by default: tol=n*sqrt(eps) maxiter : integer, optional maximum number of iterations by default: maxiter=min(n,20) largest : bool, optional when True, solve for the largest eigenvalues, otherwise the smallest verbosityLevel : integer, optional controls solver output.  default: verbosityLevel = 0. retLambdaHistory : boolean, optional whether to return eigenvalue history retResidualNormsHistory : boolean, optional whether to return history of residual norms  Examples --------  Solve A x = lambda B x with constraints and preconditioning.  >>> from scipy.sparse import spdiags, issparse >>> from scipy.sparse.linalg import lobpcg, LinearOperator >>> n = 100 >>> vals = [np.arange(n, dtype=np.float64) + 1] >>> A = spdiags(vals, 0, n, n) >>> A.toarray() array([[   1.,    0.,    0., ...,    0.,    0.,    0.], [   0.,    2.,    0., ...,    0.,    0.,    0.], [   0.,    0.,    3., ...,    0.,    0.,    0.], ..., [   0.,    0.,    0., ...,   98.,    0.,    0.], [   0.,    0.,    0., ...,    0.,   99.,    0.], [   0.,    0.,    0., ...,    0.,    0.,  100.]])  Constraints.  >>> Y = np.eye(n, 3)  Initial guess for eigenvectors, should have linearly independent columns. Column dimension = number of requested eigenvalues.  >>> X = np.random.rand(n, 3)  Preconditioner -- inverse of A (as an abstract linear operator).  >>> invA = spdiags([1./vals[0]], 0, n, n) >>> def precond( x ): ...     return invA  * x >>> M = LinearOperator(matvec=precond, shape=(n, n), dtype=float)  Here, ``invA`` could of course have been used directly as a preconditioner. Let us then solve the problem:  >>> eigs, vecs = lobpcg(A, X, Y=Y, M=M, tol=1e-4, maxiter=40, largest=False) >>> eigs array([ 4.,  5.,  6.])  Note that the vectors passed in Y are the eigenvectors of the 3 smallest eigenvalues. The results returned are orthogonal to those.  Notes ----- If both retLambdaHistory and retResidualNormsHistory are True, the return tuple has the following format (lambda, V, lambda history, residual norms history).  In the following ``n`` denotes the matrix size and ``m`` the number of required eigenvalues (smallest or largest).  The LOBPCG code internally solves eigenproblems of the size 3``m`` on every iteration by calling the \"standard\" dense eigensolver, so if ``m`` is not small enough compared to ``n``, it does not make sense to call the LOBPCG code, but rather one should use the \"standard\" eigensolver, e.g. numpy or scipy function in this case. If one calls the LOBPCG algorithm for 5``m``>``n``, it will most likely break internally, so the code tries to call the standard function instead.  It is not that n should be large for the LOBPCG to work, but rather the ratio ``n``/``m`` should be large. It you call the LOBPCG code with ``m``=1 and ``n``=10, it should work, though ``n`` is small. The method is intended for extremely large ``n``/``m``, see e.g., reference [28] in http://arxiv.org/abs/0705.2626  The convergence speed depends basically on two factors:  1.  How well relatively separated the seeking eigenvalues are from the rest of the eigenvalues. One can try to vary ``m`` to make this better.  2.  How well conditioned the problem is. This can be changed by using proper preconditioning. For example, a rod vibration test problem (under tests directory) is ill-conditioned for large ``n``, so convergence will be slow, unless efficient preconditioning is used. For this specific problem, a good simple preconditioner function would be a linear solve for A, which is easy to code since A is tridiagonal.  *Acknowledgements*  lobpcg.py code was written by Robert Cimrman. Many thanks belong to Andrew Knyazev, the author of the algorithm, for lots of advice and support.  References ---------- .. [1] A. V. Knyazev (2001), Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM Journal on Scientific Computing 23, no. 2, pp. 517-541. http://dx.doi.org/10.1137/S1064827500366124  .. [2] A. V. Knyazev, I. Lashuk, M. E. Argentati, and E. Ovchinnikov (2007), Block Locally Optimal Preconditioned Eigenvalue Xolvers (BLOPEX) in hypre and PETSc.  http://arxiv.org/abs/0705.2626  .. [3] A. V. Knyazev\\'s C and MATLAB implementations: http://www-math.cudenver.edu/~aknyazev/software/BLOPEX/  '",
        "name": "w",
        "type": "array"
      },
      "tags": [
        "sparse",
        "linalg",
        "eigen",
        "lobpcg",
        "lobpcg"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "sklearn.metrics.classification.precision_recall_fscore_support",
      "description": "\"Compute precision, recall, F-measure and support for each class\n\nThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\ntrue positives and ``fp`` the number of false positives. The precision is\nintuitively the ability of the classifier not to label as positive a sample\nthat is negative.\n\nThe recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\ntrue positives and ``fn`` the number of false negatives. The recall is\nintuitively the ability of the classifier to find all the positive samples.\n\nThe F-beta score can be interpreted as a weighted harmonic mean of\nthe precision and recall, where an F-beta score reaches its best\nvalue at 1 and worst score at 0.\n\nThe F-beta score weights recall more than precision by a factor of\n``beta``. ``beta == 1.0`` means recall and precision are equally important.\n\nThe support is the number of occurrences of each class in ``y_true``.\n\nIf ``pos_label is None`` and in binary classification, this function\nreturns the average precision, recall and F-measure if ``average``\nis one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n\nRead more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "id": "sklearn.metrics.classification.precision_recall_fscore_support",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "sklearn.metrics.classification.precision_recall_fscore_support",
      "parameters": [
        {
          "description": "Ground truth (correct) target values. ",
          "name": "y_true",
          "type": ""
        },
        {
          "description": "Estimated targets as returned by a classifier. ",
          "name": "y_pred",
          "type": ""
        },
        {
          "description": "The strength of recall versus precision in the F-score. ",
          "name": "beta",
          "type": "float"
        },
        {
          "description": "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order. ",
          "name": "labels",
          "optional": "true",
          "type": "list"
        },
        {
          "description": "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only. ",
          "name": "pos_label",
          "type": "str"
        },
        {
          "description": "If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:  ``'binary'``: Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary. ``'micro'``: Calculate metrics globally by counting the total true positives, false negatives and false positives. ``'macro'``: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. ``'weighted'``: Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall. ``'samples'``: Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`). ",
          "name": "average",
          "type": "string"
        },
        {
          "description": "This determines which warnings will be made in the case that this function is being used to return only one of its metrics. ",
          "name": "warn_for",
          "type": "tuple"
        },
        {
          "description": "Sample weights. ",
          "name": "sample_weight",
          "optional": "true",
          "shape": "n_samples",
          "type": "array-like"
        }
      ],
      "returns": {
        "description": " recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]  fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]  support : int (if average is not None) or array of int, shape =        [n_unique_labels] The number of occurrences of each label in ``y_true``.  References ---------- .. [1] `Wikipedia entry for the Precision and recall <https://en.wikipedia.org/wiki/Precision_and_recall>`_  .. [2] `Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_  .. [3] `Discriminative Methods for Multi-labeled Classification Advances in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu Godbole, Sunita Sarawagi <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`  Examples -------- >>> from sklearn.metrics import precision_recall_fscore_support >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig']) >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog']) >>> precision_recall_fscore_support(y_true, y_pred, average='macro') ... # doctest: +ELLIPSIS (0.22..., 0.33..., 0.26..., None) >>> precision_recall_fscore_support(y_true, y_pred, average='micro') ... # doctest: +ELLIPSIS (0.33..., 0.33..., 0.33..., None) >>> precision_recall_fscore_support(y_true, y_pred, average='weighted') ... # doctest: +ELLIPSIS (0.22..., 0.33..., 0.26..., None)  It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging: >>> precision_recall_fscore_support(y_true, y_pred, average=None, ... labels=['pig', 'dog', 'cat']) ... # doctest: +ELLIPSIS,+NORMALIZE_WHITESPACE (array([ 0. ,  0. ,  0.66...]), array([ 0.,  0.,  1.]), array([ 0. ,  0. ,  0.8]), array([2, 2, 2]))  \"",
        "name": "precision",
        "shape": "n_unique_labels",
        "type": "float"
      },
      "tags": [
        "metrics",
        "classification"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.sparse.linalg.isolve.lsqr.lsqr",
      "description": "'Find the least-squares solution to a large, sparse, linear system\nof equations.\n\nThe function solves ``Ax = b``  or  ``min ||b - Ax||^2`` or\n``min ||Ax - b||^2 + d^2 ||x||^2``.\n\nThe matrix A may be square or rectangular (over-determined or\nunder-determined), and may have any rank.\n\n::\n\n1. Unsymmetric equations --    solve  A*x = b\n\n2. Linear least squares  --    solve  A*x = b\nin the least-squares sense\n\n3. Damped least squares  --    solve  (   A    )*x = ( b )\n( damp*I )     ( 0 )\nin the least-squares sense\n",
      "id": "scipy.sparse.linalg.isolve.lsqr.lsqr",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.sparse.linalg.isolve.lsqr.lsqr",
      "parameters": [
        {
          "description": "Representation of an m-by-n matrix.  It is required that the linear operator can produce ``Ax`` and ``A^T x``.",
          "name": "A",
          "type": "sparse matrix, ndarray, LinearOperator"
        },
        {
          "description": "Right-hand side vector ``b``.",
          "name": "b",
          "shape": "m,",
          "type": "array"
        },
        {
          "description": "Damping coefficient.",
          "name": "damp",
          "type": "float"
        },
        {
          "description": "Stopping tolerances. If both are 1.0e-9 (say), the final residual norm should be accurate to about 9 digits.  (The final x will usually have fewer correct digits, depending on cond(A) and the size of damp.)",
          "name": "atol, btol",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Another stopping tolerance.  lsqr terminates if an estimate of ``cond(A)`` exceeds `conlim`.  For compatible systems ``Ax = b``, `conlim` could be as large as 1.0e+12 (say).  For least-squares problems, conlim should be less than 1.0e+8. Maximum precision can be obtained by setting ``atol = btol = conlim = zero``, but the number of iterations may then be excessive.",
          "name": "conlim",
          "optional": "true",
          "type": "float"
        },
        {
          "description": "Explicit limitation on number of iterations (for safety).",
          "name": "iter_lim",
          "optional": "true",
          "type": "int"
        },
        {
          "description": "Display an iteration log.",
          "name": "show",
          "optional": "true",
          "type": "bool"
        },
        {
          "description": "Whether to estimate diagonals of ``(A\\'A + damp^2*I)^{-1}``. ",
          "name": "calc_var",
          "optional": "true",
          "type": "bool"
        }
      ],
      "returns": {
        "description": "The final solution. istop : int Gives the reason for termination. 1 means x is an approximate solution to Ax = b. 2 means x approximately solves the least-squares problem. itn : int Iteration number upon termination. r1norm : float ``norm(r)``, where ``r = b - Ax``. r2norm : float ``sqrt( norm(r)^2  +  damp^2 * norm(x)^2 )``.  Equal to `r1norm` if ``damp == 0``. anorm : float Estimate of Frobenius norm of ``Abar = [[A]; [damp*I]]``. acond : float Estimate of ``cond(Abar)``. arnorm : float Estimate of ``norm(A\\'*r - damp^2*x)``. xnorm : float ``norm(x)`` var : ndarray of float If ``calc_var`` is True, estimates all diagonals of ``(A\\'A)^{-1}`` (if ``damp == 0``) or more generally ``(A\\'A + damp^2*I)^{-1}``.  This is well defined if A has full column rank or ``damp > 0``.  (Not sure what var means if ``rank(A) < n`` and ``damp = 0.``)  Notes ----- LSQR uses an iterative method to approximate the solution.  The number of iterations required to reach a certain accuracy depends strongly on the scaling of the problem.  Poor scaling of the rows or columns of A should therefore be avoided where possible.  For example, in problem 1 the solution is unaltered by row-scaling.  If a row of A is very small or large compared to the other rows of A, the corresponding row of ( A  b ) should be scaled up or down.  In problems 1 and 2, the solution x is easily recovered following column-scaling.  Unless better information is known, the nonzero columns of A should be scaled so that they all have the same Euclidean norm (e.g., 1.0).  In problem 3, there is no freedom to re-scale if damp is nonzero.  However, the value of damp should be assigned only after attention has been paid to the scaling of A.  The parameter damp is intended to help regularize ill-conditioned systems, by preventing the true solution from being very large.  Another aid to regularization is provided by the parameter acond, which may be used to terminate iterations before the computed solution becomes very large.  If some initial estimate ``x0`` is known and if ``damp == 0``, one could proceed as follows:  1. Compute a residual vector ``r0 = b - A*x0``. 2. Use LSQR to solve the system  ``A*dx = r0``. 3. Add the correction dx to obtain a final solution ``x = x0 + dx``.  This requires that ``x0`` be available before and after the call to LSQR.  To judge the benefits, suppose LSQR takes k1 iterations to solve A*x = b and k2 iterations to solve A*dx = r0. If x0 is \"good\", norm(r0) will be smaller than norm(b). If the same stopping tolerances atol and btol are used for each system, k1 and k2 will be similar, but the final solution x0 + dx should be more accurate.  The only way to reduce the total work is to use a larger stopping tolerance for the second system. If some value btol is suitable for A*x = b, the larger value btol*norm(b)/norm(r0)  should be suitable for A*dx = r0.  Preconditioning is another way to reduce the number of iterations. If it is possible to solve a related system ``M*x = b`` efficiently, where M approximates A in some helpful way (e.g. M - A has low rank or its elements are small relative to those of A), LSQR may converge more rapidly on the system ``A*M(inverse)*z = b``, after which x can be recovered by solving M*x = z.  If A is symmetric, LSQR should not be used!  Alternatives are the symmetric conjugate-gradient method (cg) and/or SYMMLQ.  SYMMLQ is an implementation of symmetric cg that applies to any symmetric A and will converge more rapidly than LSQR.  If A is positive definite, there are other implementations of symmetric cg that require slightly less work per iteration than SYMMLQ (but will take the same number of iterations).  References ---------- .. [1] C. C. Paige and M. A. Saunders (1982a). \"LSQR: An algorithm for sparse linear equations and sparse least squares\", ACM TOMS 8(1), 43-71. .. [2] C. C. Paige and M. A. Saunders (1982b). \"Algorithm 583.  LSQR: Sparse linear equations and least squares problems\", ACM TOMS 8(2), 195-209. .. [3] M. A. Saunders (1995).  \"Solution of sparse rectangular systems using LSQR and CRAIG\", BIT 35, 588-604.  '",
        "name": "x",
        "type": "ndarray"
      },
      "tags": [
        "sparse",
        "linalg",
        "isolve",
        "lsqr"
      ],
      "version": "0.18.1"
    },
    {
      "common_name": "scipy.spatial.distance.cdist",
      "description": "\"\nComputes distance between each pair of the two collections of inputs.\n\nThe following are common calling conventions:\n\n1. ``Y = cdist(XA, XB, 'euclidean')``\n\nComputes the distance between :math:`m` points using\nEuclidean distance (2-norm) as the distance metric between the\npoints. The points are arranged as :math:`m`\n:math:`n`-dimensional row vectors in the matrix X.\n\n2. ``Y = cdist(XA, XB, 'minkowski', p)``\n\nComputes the distances using the Minkowski distance\n:math:`||u-v||_p` (:math:`p`-norm) where :math:`p \\\\geq 1`.\n\n3. ``Y = cdist(XA, XB, 'cityblock')``\n\nComputes the city block or Manhattan distance between the\npoints.\n\n4. ``Y = cdist(XA, XB, 'seuclidean', V=None)``\n\nComputes the standardized Euclidean distance. The standardized\nEuclidean distance between two n-vectors ``u`` and ``v`` is\n\n.. math::\n\n\\\\sqrt{\\\\sum {(u_i-v_i)^2 / V[x_i]}}.\n\nV is the variance vector; V[i] is the variance computed over all\nthe i'th components of the points. If not passed, it is\nautomatically computed.\n\n5. ``Y = cdist(XA, XB, 'sqeuclidean')``\n\nComputes the squared Euclidean distance :math:`||u-v||_2^2` between\nthe vectors.\n\n6. ``Y = cdist(XA, XB, 'cosine')``\n\nComputes the cosine distance between vectors u and v,\n\n.. math::\n\n1 - \\\\frac{u \\\\cdot v}\n{{||u||}_2 {||v||}_2}\n\nwhere :math:`||*||_2` is the 2-norm of its argument ``*``, and\n:math:`u \\\\cdot v` is the dot product of :math:`u` and :math:`v`.\n\n7. ``Y = cdist(XA, XB, 'correlation')``\n\nComputes the correlation distance between vectors u and v. This is\n\n.. math::\n\n1 - \\\\frac{(u - \\\\bar{u}) \\\\cdot (v - \\\\bar{v})}\n{{||(u - \\\\bar{u})||}_2 {||(v - \\\\bar{v})||}_2}\n\nwhere :math:`\\\\bar{v}` is the mean of the elements of vector v,\nand :math:`x \\\\cdot y` is the dot product of :math:`x` and :math:`y`.\n\n\n8. ``Y = cdist(XA, XB, 'hamming')``\n\nComputes the normalized Hamming distance, or the proportion of\nthose vector elements between two n-vectors ``u`` and ``v``\nwhich disagree. To save memory, the matrix ``X`` can be of type\nboolean.\n\n9. ``Y = cdist(XA, XB, 'jaccard')``\n\nComputes the Jaccard distance between the points. Given two\nvectors, ``u`` and ``v``, the Jaccard distance is the\nproportion of those elements ``u[i]`` and ``v[i]`` that\ndisagree where at least one of them is non-zero.\n\n10. ``Y = cdist(XA, XB, 'chebyshev')``\n\nComputes the Chebyshev distance between the points. The\nChebyshev distance between two n-vectors ``u`` and ``v`` is the\nmaximum norm-1 distance between their respective elements. More\nprecisely, the distance is given by\n\n.. math::\n\nd(u,v) = \\\\max_i {|u_i-v_i|}.\n\n11. ``Y = cdist(XA, XB, 'canberra')``\n\nComputes the Canberra distance between the points. The\nCanberra distance between two points ``u`` and ``v`` is\n\n.. math::\n\nd(u,v) = \\\\sum_i \\\\frac{|u_i-v_i|}\n{|u_i|+|v_i|}.\n\n12. ``Y = cdist(XA, XB, 'braycurtis')``\n\nComputes the Bray-Curtis distance between the points. The\nBray-Curtis distance between two points ``u`` and ``v`` is\n\n\n.. math::\n\nd(u,v) = \\\\frac{\\\\sum_i (u_i-v_i)}\n{\\\\sum_i (u_i+v_i)}\n\n13. ``Y = cdist(XA, XB, 'mahalanobis', VI=None)``\n\nComputes the Mahalanobis distance between the points. The\nMahalanobis distance between two points ``u`` and ``v`` is\n:math:`(u-v)(1/V)(u-v)^T` where :math:`(1/V)` (the ``VI``\nvariable) is the inverse covariance. If ``VI`` is not None,\n``VI`` will be used as the inverse covariance matrix.\n\n14. ``Y = cdist(XA, XB, 'yule')``\n\nComputes the Yule distance between the boolean\nvectors. (see `yule` function documentation)\n\n15. ``Y = cdist(XA, XB, 'matching')``\n\nComputes the matching distance between the boolean\nvectors. (see `matching` function documentation)\n\n16. ``Y = cdist(XA, XB, 'dice')``\n\nComputes the Dice distance between the boolean vectors. (see\n`dice` function documentation)\n\n17. ``Y = cdist(XA, XB, 'kulsinski')``\n\nComputes the Kulsinski distance between the boolean\nvectors. (see `kulsinski` function documentation)\n\n18. ``Y = cdist(XA, XB, 'rogerstanimoto')``\n\nComputes the Rogers-Tanimoto distance between the boolean\nvectors. (see `rogerstanimoto` function documentation)\n\n19. ``Y = cdist(XA, XB, 'russellrao')``\n\nComputes the Russell-Rao distance between the boolean\nvectors. (see `russellrao` function documentation)\n\n20. ``Y = cdist(XA, XB, 'sokalmichener')``\n\nComputes the Sokal-Michener distance between the boolean\nvectors. (see `sokalmichener` function documentation)\n\n21. ``Y = cdist(XA, XB, 'sokalsneath')``\n\nComputes the Sokal-Sneath distance between the vectors. (see\n`sokalsneath` function documentation)\n\n\n22. ``Y = cdist(XA, XB, 'wminkowski')``\n\nComputes the weighted Minkowski distance between the\nvectors. (see `wminkowski` function documentation)\n\n23. ``Y = cdist(XA, XB, f)``\n\nComputes the distance between all pairs of vectors in X\nusing the user supplied 2-arity function f. For example,\nEuclidean distance between the vectors could be computed\nas follows::\n\ndm = cdist(XA, XB, lambda u, v: np.sqrt(((u-v)**2).sum()))\n\nNote that you should avoid passing a reference to one of\nthe distance functions defined in this library. For example,::\n\ndm = cdist(XA, XB, sokalsneath)\n\nwould calculate the pair-wise distances between the vectors in\nX using the Python function `sokalsneath`. This would result in\nsokalsneath being called :math:`{n \\\\choose 2}` times, which\nis inefficient. Instead, the optimized C version is more\nefficient, and we call it using the following syntax::\n\ndm = cdist(XA, XB, 'sokalsneath')\n",
      "id": "scipy.spatial.distance.cdist",
      "is_class": false,
      "language": "python",
      "library": "sklearn",
      "name": "scipy.spatial.distance.cdist",
      "parameters": [
        {
          "description": "An :math:`m_A` by :math:`n` array of :math:`m_A` original observations in an :math:`n`-dimensional space. Inputs are converted to float type. XB : ndarray An :math:`m_B` by :math:`n` array of :math:`m_B` original observations in an :math:`n`-dimensional space. Inputs are converted to float type.",
          "name": "XA",
          "type": "ndarray"
        },
        {
          "description": "The distance metric to use.  If a string, the distance function can be 'braycurtis', 'canberra', 'chebyshev', 'cityblock', 'correlation', 'cosine', 'dice', 'euclidean', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis', 'matching', 'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean', 'wminkowski', 'yule'.",
          "name": "metric",
          "optional": "true",
          "type": "str"
        },
        {
          "description": "The weight vector (for weighted Minkowski).",
          "name": "w",
          "optional": "true",
          "type": "ndarray"
        },
        {
          "description": "The p-norm to apply (for Minkowski, weighted and unweighted) V : ndarray, optional The variance vector (for standardized Euclidean). VI : ndarray, optional The inverse of the covariance matrix (for Mahalanobis). ",
          "name": "p",
          "optional": "true",
          "type": "scalar"
        }
      ],
      "returns": {
        "description": "A :math:`m_A` by :math:`m_B` distance matrix is returned. For each :math:`i` and :math:`j`, the metric ``dist(u=XA[i], v=XB[j])`` is computed and stored in the :math:`ij` th entry.  Raises ------ ValueError An exception is thrown if `XA` and `XB` do not have the same number of columns.  Examples -------- Find the Euclidean distances between four 2-D coordinates:  >>> from scipy.spatial import distance >>> coords = [(35.0456, -85.2672), ...           (35.1174, -89.9711), ...           (35.9728, -83.9422), ...           (36.1667, -86.7833)] >>> distance.cdist(coords, coords, 'euclidean') array([[ 0.    ,  4.7044,  1.6172,  1.8856], [ 4.7044,  0.    ,  6.0893,  3.3561], [ 1.6172,  6.0893,  0.    ,  2.8477], [ 1.8856,  3.3561,  2.8477,  0.    ]])   Find the Manhattan distance from a 3-D point to the corners of the unit cube:  >>> a = np.array([[0, 0, 0], ...               [0, 0, 1], ...               [0, 1, 0], ...               [0, 1, 1], ...               [1, 0, 0], ...               [1, 0, 1], ...               [1, 1, 0], ...               [1, 1, 1]]) >>> b = np.array([[ 0.1,  0.2,  0.4]]) >>> distance.cdist(a, b, 'cityblock') array([[ 0.7], [ 0.9], [ 1.3], [ 1.5], [ 1.5], [ 1.7], [ 2.1], [ 2.3]])  \"",
        "name": "Y",
        "type": "ndarray"
      },
      "tags": [
        "spatial",
        "distance"
      ],
      "version": "0.18.1"
    },
    {
      "attributes": [],
      "category": "utils.fixes",
      "common_name": "Masked Array",
      "description": "None",
      "id": "sklearn.utils.fixes.MaskedArray",
      "is_class": true,
      "language": "python",
      "library": "sklearn",
      "methods_available": [
        {
          "description": "'\nCheck if all of the elements of `a` are true.\n\nPerforms a :func:`logical_and` over the given axis and returns the result.\nMasked values are considered as True during computation.\nFor convenience, the output array is masked where ALL the values along the\ncurrent axis are masked: if the output would have been a scalar and that\nall the values are masked, then the output is `masked`.\n",
          "id": "sklearn.utils.fixes.MaskedArray.all",
          "name": "all",
          "parameters": [
            {
              "description": "Axis to perform the operation over. If None, perform over flattened array.",
              "name": "axis",
              "type": "None, integer"
            },
            {
              "description": "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output.  See Also --------",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            },
            {
              "description": " Examples -------- >>> np.ma.array([1,2,3]).all() True >>> a = np.ma.array([1,2,3], mask=True) >>> (a.all() is np.ma.masked) True  '",
              "name": "all",
              "type": "equivalent"
            }
          ]
        },
        {
          "description": "'\nCompute the anomalies (deviations from the arithmetic mean)\nalong the given axis.\n\nReturns an array of anomalies, with the same shape as the input and\nwhere the arithmetic mean is computed along the given axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.anom",
          "name": "anom",
          "parameters": [
            {
              "description": "Axis over which the anomalies are taken. The default is to use the mean of the flattened array as reference.",
              "name": "axis",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "Type to use in computing the variance. For arrays of integer type the default is float32; for arrays of float types it is the same as the array type.  See Also --------",
              "name": "dtype",
              "optional": "true",
              "type": "dtype"
            },
            {
              "description": " Examples -------- >>> a = np.ma.array([1,2,3]) >>> a.anom() masked_array(data = [-1.  0.  1.], mask = False, fill_value = 1e+20)  '",
              "name": "mean",
              "type": ""
            }
          ]
        },
        {
          "description": "'\nCheck if any of the elements of `a` are true.\n\nPerforms a logical_or over the given axis and returns the result.\nMasked values are considered as False during computation.\n",
          "id": "sklearn.utils.fixes.MaskedArray.any",
          "name": "any",
          "parameters": [
            {
              "description": "Axis to perform the operation over. If None, perform over flattened array and return a scalar.",
              "name": "axis",
              "type": "None, integer"
            },
            {
              "description": "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output.  See Also --------",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            },
            {
              "description": " '",
              "name": "any",
              "type": "equivalent"
            }
          ]
        },
        {
          "description": "'\nReturns array of indices of the maximum values along the given axis.\nMasked values are treated as if they had the value fill_value.\n",
          "id": "sklearn.utils.fixes.MaskedArray.argmax",
          "name": "argmax",
          "parameters": [
            {
              "description": "If None, the index is into the flattened array, otherwise along the specified axis",
              "name": "axis",
              "type": "None, integer"
            },
            {
              "description": "Value used to fill in the masked values.  If None, the output of maximum_fill_value(self._data) is used instead.",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            },
            {
              "description": "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output. ",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            }
          ],
          "returns": {
            "description": " Examples -------- >>> a = np.arange(6).reshape(2,3) >>> a.argmax() 5 >>> a.argmax(0) array([1, 1, 1]) >>> a.argmax(1) array([2, 2])  '",
            "name": "index_array",
            "type": "integer_array"
          }
        },
        {
          "description": "'\nReturn array of indices to the minimum values along the given axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.argmin",
          "name": "argmin",
          "parameters": [
            {
              "description": "If None, the index is into the flattened array, otherwise along the specified axis",
              "name": "axis",
              "type": "None, integer"
            },
            {
              "description": "Value used to fill in the masked values.  If None, the output of minimum_fill_value(self._data) is used instead.",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            },
            {
              "description": "Array into which the result can be placed. Its type is preserved and it must be of the right shape to hold the output. ",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            }
          ],
          "returns": {
            "description": "If multi-dimension input, returns a new ndarray of indices to the minimum values along the given axis.  Otherwise, returns a scalar of index to the minimum values along the given axis.  Examples -------- >>> x = np.ma.array(arange(4), mask=[1,1,0,0]) >>> x.shape = (2,2) >>> print(x) [[-- --] [2 3]] >>> print(x.argmin(axis=0, fill_value=-1)) [0 0] >>> print(x.argmin(axis=0, fill_value=9)) [1 1]  '",
            "name": "ndarray or scalar"
          }
        },
        {
          "description": "\"\nReturn an ndarray of indices that sort the array along the\nspecified axis.  Masked values are filled beforehand to\n`fill_value`.\n",
          "id": "sklearn.utils.fixes.MaskedArray.argsort",
          "name": "argsort",
          "parameters": [
            {
              "description": "Axis along which to sort.  The default is -1 (last axis). If None, the flattened array is used.",
              "name": "axis",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "Value used to fill the array before sorting. The default is the `fill_value` attribute of the input array.",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            },
            {
              "description": "Sorting algorithm.",
              "name": "kind",
              "optional": "true",
              "type": "'quicksort', 'mergesort', 'heapsort'"
            },
            {
              "description": "When `a` is an array with fields defined, this argument specifies which fields to compare first, second, etc.  Not all fields need be specified. ",
              "name": "order",
              "optional": "true",
              "type": "list"
            }
          ],
          "returns": {
            "description": "Array of indices that sort `a` along the specified axis. In other words, ``a[index_array]`` yields a sorted `a`.  See Also -------- sort : Describes sorting algorithms used. lexsort : Indirect stable sort with multiple keys. ndarray.sort : Inplace sort.  Notes ----- See `sort` for notes on the different sorting algorithms.  Examples -------- >>> a = np.ma.array([3,2,1], mask=[False, False, True]) >>> a masked_array(data = [3 2 --], mask = [False False  True], fill_value = 999999) >>> a.argsort() array([1, 0, 2])  \"",
            "name": "index_array",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nReturns a copy of the MaskedArray cast to given newtype.\n",
          "id": "sklearn.utils.fixes.MaskedArray.astype",
          "name": "astype",
          "parameters": [],
          "returns": {
            "description": "A copy of self cast to input newtype. The returned record shape matches self.shape.  Examples -------- >>> x = np.ma.array([[1,2,3.1],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1.0 -- 3.1] [-- 5.0 --] [7.0 -- 9.0]] >>> print(x.astype(int32)) [[1 -- 3] [-- 5 --] [7 -- 9]]  '",
            "name": "output",
            "type": ""
          }
        },
        {
          "description": "'a.clip(min=None, max=None, out=None)\n\nReturn an array whose values are limited to ``[min, max]``.\nOne of max or min must be given.\n\nRefer to `numpy.clip` for full documentation.\n\nSee Also\n--------\nnumpy.clip : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.clip",
          "name": "clip",
          "parameters": []
        },
        {
          "description": "'\nReturn `a` where condition is ``True``.\n\nIf condition is a `MaskedArray`, missing values are considered\nas ``False``.\n",
          "id": "sklearn.utils.fixes.MaskedArray.compress",
          "name": "compress",
          "parameters": [
            {
              "description": "Boolean 1-d array selecting which entries to return. If len(condition) is less than the size of a along the axis, then output is truncated to length of condition array.",
              "name": "condition",
              "type": "var"
            },
            {
              "description": "Axis along which the operation must be performed.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "None, ndarray"
            }
          ],
          "returns": {
            "description": "A :class:`MaskedArray` object.  Notes ----- Please note the difference with :meth:`compressed` ! The output of :meth:`compress` has a mask, the output of :meth:`compressed` does not.  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> x.compress([1, 0, 1]) masked_array(data = [1 3], mask = [False False], fill_value=999999)  >>> x.compress([1, 0, 1], axis=1) masked_array(data = [[1 3] [-- --] [7 9]], mask = [[False False] [ True  True] [False False]], fill_value=999999)  '",
            "name": "result",
            "type": ""
          }
        },
        {
          "description": "\"\nReturn all the non-masked data as a 1-D array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.compressed",
          "name": "compressed",
          "parameters": [],
          "returns": {
            "description": "A new `ndarray` holding the non-masked data is returned.  Notes ----- The result is **not** a MaskedArray!  Examples -------- >>> x = np.ma.array(np.arange(5), mask=[0]*2 + [1]*3) >>> x.compressed() array([0, 1]) >>> type(x.compressed()) <type 'numpy.ndarray'>  \"",
            "name": "data",
            "type": "ndarray"
          }
        },
        {
          "description": "\"a.copy(order='C')\n\nReturn a copy of the array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.copy",
          "name": "copy",
          "parameters": [
            {
              "description": "Controls the memory layout of the copy. 'C' means C-order, 'F' means F-order, 'A' means 'F' if `a` is Fortran contiguous, 'C' otherwise. 'K' means match the layout of `a` as closely as possible. (Note that this function and :func:numpy.copy are very similar, but have different default values for their order= arguments.)  See also -------- numpy.copy numpy.copyto  Examples -------- >>> x = np.array([[1,2,3],[4,5,6]], order='F')  >>> y = x.copy()  >>> x.fill(0)  >>> x array([[0, 0, 0], [0, 0, 0]])  >>> y array([[1, 2, 3], [4, 5, 6]])  >>> y.flags['C_CONTIGUOUS'] True\"",
              "name": "order",
              "optional": "true",
              "type": "'C', 'F', 'A', 'K'"
            }
          ]
        },
        {
          "description": "'\nCount the non-masked elements of the array along the given axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.count",
          "name": "count",
          "parameters": [
            {
              "description": "Axis along which to count the non-masked elements. If `axis` is `None`, all non-masked elements are counted. ",
              "name": "axis",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "If `axis` is `None`, an integer count is returned. When `axis` is not `None`, an array with shape determined by the lengths of the remaining axes, is returned.  See Also -------- count_masked : Count masked elements in array or along a given axis.  Examples -------- >>> import numpy.ma as ma >>> a = ma.arange(6).reshape((2, 3)) >>> a[1, :] = ma.masked >>> a masked_array(data = [[0 1 2] [-- -- --]], mask = [[False False False] [ True  True  True]], fill_value = 999999) >>> a.count() 3  When the `axis` keyword is specified an array of appropriate size is returned.  >>> a.count(axis=0) array([1, 1, 1]) >>> a.count(axis=1) array([3, 0])  '",
            "name": "result",
            "type": "int"
          }
        },
        {
          "description": "'\nReturn the cumulative product of the elements along the given axis.\nThe cumulative product is taken over the flattened array by\ndefault, otherwise over the specified axis.\n\nMasked values are set to 1 internally during the computation.\nHowever, their position is saved, and the result will be masked at\nthe same locations.\n",
          "id": "sklearn.utils.fixes.MaskedArray.cumprod",
          "name": "cumprod",
          "parameters": [
            {
              "description": "Axis along which the product is computed. The default (`axis` = None) is to compute over the flattened array.",
              "name": "axis",
              "optional": "true",
              "type": "None, -1, int"
            },
            {
              "description": "Determines the type of the returned array and of the accumulator where the elements are multiplied. If ``dtype`` has the value ``None`` and the type of ``a`` is an integer type of precision less than the default platform integer, then the default platform integer precision is used.  Otherwise, the dtype is the same as that of ``a``.",
              "name": "dtype",
              "optional": "true",
              "type": "None, dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "A new array holding the result is returned unless out is specified, in which case a reference to out is returned.  Notes ----- The mask is lost if `out` is not a valid MaskedArray !  Arithmetic is modular when using integer types, and no error is raised on overflow.  '",
            "name": "cumprod",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nReturn the cumulative sum of the elements along the given axis.\nThe cumulative sum is calculated over the flattened array by\ndefault, otherwise over the specified axis.\n\nMasked values are set to 0 internally during the computation.\nHowever, their position is saved, and the result will be masked at\nthe same locations.\n",
          "id": "sklearn.utils.fixes.MaskedArray.cumsum",
          "name": "cumsum",
          "parameters": [
            {
              "description": "Axis along which the sum is computed. The default (`axis` = None) is to compute over the flattened array. `axis` may be negative, in which case it counts from the   last to the first axis.",
              "name": "axis",
              "optional": "true",
              "type": "None, -1, int"
            },
            {
              "description": "Type of the returned array and of the accumulator in which the elements are summed.  If `dtype` is not specified, it defaults to the dtype of `a`, unless `a` has an integer dtype with a precision less than that of the default platform integer.  In that case, the default platform integer is used.",
              "name": "dtype",
              "optional": "true",
              "type": "None, dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "A new array holding the result is returned unless ``out`` is specified, in which case a reference to ``out`` is returned.  Notes ----- The mask is lost if `out` is not a valid :class:`MaskedArray` !  Arithmetic is modular when using integer types, and no error is raised on overflow.  Examples -------- >>> marr = np.ma.array(np.arange(10), mask=[0,0,0,1,1,1,0,0,0,0]) >>> print(marr.cumsum()) [0 1 3 -- -- -- 9 16 24 33]  '",
            "name": "cumsum",
            "type": "ndarray"
          }
        },
        {
          "description": "'a.diagonal(offset=0, axis1=0, axis2=1)\n\nReturn specified diagonals. In NumPy 1.9 the returned array is a\nread-only view instead of a copy as in previous NumPy versions.  In\na future version the read-only restriction will be removed.\n\nRefer to :func:`numpy.diagonal` for full documentation.\n\nSee Also\n--------\nnumpy.diagonal : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.diagonal",
          "name": "diagonal",
          "parameters": []
        },
        {
          "description": "'\na.dot(b, out=None)\n\nMasked dot product of two arrays. Note that `out` and `strict` are\nlocated in different positions than in `ma.dot`. In order to\nmaintain compatibility with the functional version, it is\nrecommended that the optional arguments be treated as keyword only.\nAt some point that may be mandatory.\n\n.. versionadded:: 1.10.0\n",
          "id": "sklearn.utils.fixes.MaskedArray.dot",
          "name": "dot",
          "parameters": [
            {
              "description": "Inputs array.",
              "name": "b",
              "type": "masked"
            },
            {
              "description": "Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for `ma.dot(a,b)`. This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.",
              "name": "out",
              "optional": "true",
              "type": "masked"
            },
            {
              "description": "Whether masked data are propagated (True) or set to 0 (False) for the computation. Default is False.  Propagating the mask means that if a masked value appears in a row or column, the whole row or column is considered masked.  .. versionadded:: 1.10.2  See Also -------- numpy.ma.dot : equivalent function  '",
              "name": "strict",
              "optional": "true",
              "type": "bool"
            }
          ]
        },
        {
          "description": "\"\nReturn a copy of self, with masked values filled with a given value.\n**However**, if there are no masked values to fill, self will be\nreturned instead as an ndarray.\n",
          "id": "sklearn.utils.fixes.MaskedArray.filled",
          "name": "filled",
          "parameters": [
            {
              "description": "The value to use for invalid entries (None by default). If None, the `fill_value` attribute of the array is used instead. ",
              "name": "fill_value",
              "optional": "true",
              "type": "scalar"
            }
          ],
          "returns": {
            "description": "A copy of ``self`` with invalid entries replaced by *fill_value* (be it the function argument or the attribute of ``self``), or ``self`` itself as an ndarray if there are no invalid entries to be replaced.  Notes ----- The result is **not** a MaskedArray!  Examples -------- >>> x = np.ma.array([1,2,3,4,5], mask=[0,0,1,0,1], fill_value=-999) >>> x.filled() array([1, 2, -999, 4, -999]) >>> type(x.filled()) <type 'numpy.ndarray'>  Subclassing is preserved. This means that if the data part of the masked array is a matrix, `filled` returns a matrix:  >>> x = np.ma.array(np.matrix([[1, 2], [3, 4]]), mask=[[0, 1], [1, 0]]) >>> x.filled() matrix([[     1, 999999], [999999,      4]])  \"",
            "name": "filled_array",
            "type": "ndarray"
          }
        },
        {
          "description": "\"a.flatten(order='C')\n\nReturn a copy of the array collapsed into one dimension.\n",
          "id": "sklearn.utils.fixes.MaskedArray.flatten",
          "name": "flatten",
          "parameters": [
            {
              "description": "'C' means to flatten in row-major (C-style) order. 'F' means to flatten in column-major (Fortran- style) order. 'A' means to flatten in column-major order if `a` is Fortran *contiguous* in memory, row-major order otherwise. 'K' means to flatten `a` in the order the elements occur in memory. The default is 'C'. ",
              "name": "order",
              "optional": "true",
              "type": "'C', 'F', 'A', 'K'"
            }
          ],
          "returns": {
            "description": "A copy of the input array, flattened to one dimension.  See Also -------- ravel : Return a flattened array. flat : A 1-D flat iterator over the array.  Examples -------- >>> a = np.array([[1,2], [3,4]]) >>> a.flatten() array([1, 2, 3, 4]) >>> a.flatten('F') array([1, 3, 2, 4])\"",
            "name": "y",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nReturn the filling value of the masked array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.get_fill_value",
          "name": "get_fill_value",
          "parameters": [],
          "returns": {
            "description": "The filling value.  Examples -------- >>> for dt in [np.int32, np.int64, np.float64, np.complex128]: ...     np.ma.array([0, 1], dtype=dt).get_fill_value() ... 999999 999999 1e+20 (1e+20+0j)  >>> x = np.ma.array([0, 1.], fill_value=-np.inf) >>> x.get_fill_value() -inf  '",
            "name": "fill_value",
            "type": "scalar"
          }
        },
        {
          "description": "'\nReturn the imaginary part of the masked array.\n\nThe returned array is a view on the imaginary part of the `MaskedArray`\nwhose `get_imag` method is called.\n",
          "id": "sklearn.utils.fixes.MaskedArray.get_imag",
          "name": "get_imag",
          "parameters": [],
          "returns": {
            "description": "The imaginary part of the masked array.  See Also -------- get_real, real, imag  Examples -------- >>> x = np.ma.array([1+1.j, -2j, 3.45+1.6j], mask=[False, True, False]) >>> x.get_imag() masked_array(data = [1.0 -- 1.6], mask = [False  True False], fill_value = 1e+20)  '",
            "name": "result",
            "type": ""
          }
        },
        {
          "description": "'\nReturn the real part of the masked array.\n\nThe returned array is a view on the real part of the `MaskedArray`\nwhose `get_real` method is called.\n",
          "id": "sklearn.utils.fixes.MaskedArray.get_real",
          "name": "get_real",
          "parameters": [],
          "returns": {
            "description": "The real part of the masked array.  See Also -------- get_imag, real, imag  Examples -------- >>> x = np.ma.array([1+1.j, -2j, 3.45+1.6j], mask=[False, True, False]) >>> x.get_real() masked_array(data = [1.0 -- 3.45], mask = [False  True False], fill_value = 1e+20)  '",
            "name": "result",
            "type": ""
          }
        },
        {
          "description": "'\nForce the mask to hard.\n\nWhether the mask of a masked array is hard or soft is determined by\nits `hardmask` property. `harden_mask` sets `hardmask` to True.\n\nSee Also\n--------\nhardmask\n\n'",
          "id": "sklearn.utils.fixes.MaskedArray.harden_mask",
          "name": "harden_mask",
          "parameters": []
        },
        {
          "description": "'\nReturn the addresses of the data and mask areas.\n",
          "id": "sklearn.utils.fixes.MaskedArray.ids",
          "name": "ids",
          "parameters": []
        },
        {
          "description": "'\nReturn a boolean indicating whether the data is contiguous.\n",
          "id": "sklearn.utils.fixes.MaskedArray.iscontiguous",
          "name": "iscontiguous",
          "parameters": []
        },
        {
          "description": "'\nReturn the maximum along a given axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.max",
          "name": "max",
          "parameters": [
            {
              "description": "Axis along which to operate.  By default, ``axis`` is None and the flattened input is used.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Alternative output array in which to place the result.  Must be of the same shape and buffer length as the expected output.",
              "name": "out",
              "optional": "true",
              "type": "array"
            },
            {
              "description": "Value used to fill in the masked values. If None, use the output of maximum_fill_value(). ",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            }
          ],
          "returns": {
            "description": "New array holding the result. If ``out`` was specified, ``out`` is returned.  See Also -------- maximum_fill_value Returns the maximum filling value for a given datatype.  '",
            "name": "amax",
            "type": "array"
          }
        },
        {
          "description": "'\nReturns the average of the array elements.\n\nMasked entries are ignored.\nThe average is taken over the flattened array by default, otherwise over\nthe specified axis. Refer to `numpy.mean` for the full documentation.\n",
          "id": "sklearn.utils.fixes.MaskedArray.mean",
          "name": "mean",
          "parameters": [
            {
              "description": "Array containing numbers whose mean is desired. If `a` is not an array, a conversion is attempted.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis along which the means are computed. The default is to compute the mean of the flattened array.",
              "name": "axis",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "Type to use in computing the mean. For integer inputs, the default is float64; for floating point, inputs it is the same as the input dtype.",
              "name": "dtype",
              "optional": "true",
              "type": "dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            }
          ],
          "returns": {
            "description": "If `out=None`, returns a new array containing the mean values, otherwise a reference to the output array is returned.  See Also -------- numpy.ma.mean : Equivalent function. numpy.mean : Equivalent function on non-masked arrays. numpy.ma.average: Weighted average.  Examples -------- >>> a = np.ma.array([1,2,3], mask=[False, False, True]) >>> a masked_array(data = [1 2 --], mask = [False False  True], fill_value = 999999) >>> a.mean() 1.5  '",
            "name": "mean",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nReturn the minimum along a given axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.min",
          "name": "min",
          "parameters": [
            {
              "description": "Axis along which to operate.  By default, ``axis`` is None and the flattened input is used.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Alternative output array in which to place the result.  Must be of the same shape and buffer length as the expected output.",
              "name": "out",
              "optional": "true",
              "type": "array"
            },
            {
              "description": "Value used to fill in the masked values. If None, use the output of `minimum_fill_value`. ",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            }
          ],
          "returns": {
            "description": "New array holding the result. If ``out`` was specified, ``out`` is returned.  See Also -------- minimum_fill_value Returns the minimum filling value for a given datatype.  '",
            "name": "amin",
            "type": "array"
          }
        },
        {
          "description": "'\nReturn the array minimum along the specified axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.mini",
          "name": "mini",
          "parameters": [
            {
              "description": "The axis along which to find the minima. Default is None, in which case the minimum value in the whole array is returned. ",
              "name": "axis",
              "optional": "true",
              "type": "int"
            }
          ],
          "returns": {
            "description": "If `axis` is None, the result is a scalar. Otherwise, if `axis` is given and the array is at least 2-D, the result is a masked array with dimension one smaller than the array on which `mini` is called.  Examples -------- >>> x = np.ma.array(np.arange(6), mask=[0 ,1, 0, 0, 0 ,1]).reshape(3, 2) >>> print(x) [[0 --] [2 3] [4 --]] >>> x.mini() 0 >>> x.mini(axis=0) masked_array(data = [0 3], mask = [False False], fill_value = 999999) >>> print(x.mini(axis=1)) [0 2 4]  '",
            "name": "min",
            "type": "scalar"
          }
        },
        {
          "description": "'\nReturn the indices of unmasked elements that are not zero.\n\nReturns a tuple of arrays, one for each dimension, containing the\nindices of the non-zero elements in that dimension. The corresponding\nnon-zero values can be obtained with::\n\na[a.nonzero()]\n\nTo group the indices by element, rather than dimension, use\ninstead::\n\nnp.transpose(a.nonzero())\n\nThe result of this is always a 2d array, with a row for each non-zero\nelement.\n",
          "id": "sklearn.utils.fixes.MaskedArray.nonzero",
          "name": "nonzero",
          "parameters": [],
          "returns": {
            "description": "Indices of elements that are non-zero.  See Also -------- numpy.nonzero : Function operating on ndarrays. flatnonzero : Return indices that are non-zero in the flattened version of the input array. ndarray.nonzero : Equivalent ndarray method. count_nonzero : Counts the number of non-zero elements in the input array.  Examples -------- >>> import numpy.ma as ma >>> x = ma.array(np.eye(3)) >>> x masked_array(data = [[ 1.  0.  0.] [ 0.  1.  0.] [ 0.  0.  1.]], mask = False, fill_value=1e+20) >>> x.nonzero() (array([0, 1, 2]), array([0, 1, 2]))  Masked elements are ignored.  >>> x[1, 1] = ma.masked >>> x masked_array(data = [[1.0 0.0 0.0] [0.0 -- 0.0] [0.0 0.0 1.0]], mask = [[False False False] [False  True False] [False False False]], fill_value=1e+20) >>> x.nonzero() (array([0, 2]), array([0, 2]))  Indices can also be grouped by element.  >>> np.transpose(x.nonzero()) array([[0, 0], [2, 2]])  A common use for ``nonzero`` is to find the indices of an array, where a condition is True.  Given an array `a`, the condition `a` > 3 is a boolean array and since False is interpreted as 0, ma.nonzero(a > 3) yields the indices of the `a` where the condition is true.  >>> a = ma.array([[1,2,3],[4,5,6],[7,8,9]]) >>> a > 3 masked_array(data = [[False False False] [ True  True  True] [ True  True  True]], mask = False, fill_value=999999) >>> ma.nonzero(a > 3) (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))  The ``nonzero`` method of the condition array can also be called.  >>> (a > 3).nonzero() (array([1, 1, 1, 2, 2, 2]), array([0, 1, 2, 0, 1, 2]))  '",
            "name": "tuple_of_arrays",
            "type": "tuple"
          }
        },
        {
          "description": "'\nReturn the product of the array elements over the given axis.\nMasked elements are set to 1 internally for computation.\n",
          "id": "sklearn.utils.fixes.MaskedArray.prod",
          "name": "prod",
          "parameters": [
            {
              "description": "Axis over which the product is taken. If None is used, then the product is over all the array elements.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Determines the type of the returned array and of the accumulator where the elements are multiplied. If ``dtype`` has the value ``None`` and the type of a is an integer type of precision less than the default platform integer, then the default platform integer precision is used.  Otherwise, the dtype is the same as that of a.",
              "name": "dtype",
              "optional": "true",
              "type": "None, dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            }
          ],
          "returns": {
            "description": "Returns an array whose shape is the same as a with the specified axis removed. Returns a 0d array when a is 1d or axis=None. Returns a reference to the specified output array if specified.  See Also -------- prod : equivalent function  Notes ----- Arithmetic is modular when using integer types, and no error is raised on overflow.  Examples -------- >>> np.prod([1.,2.]) 2.0 >>> np.prod([1.,2.], dtype=np.int32) 2 >>> np.prod([[1.,2.],[3.,4.]]) 24.0 >>> np.prod([[1.,2.],[3.,4.]], axis=1) array([  2.,  12.])  '",
            "name": "product_along_axis",
            "type": "array, scalar"
          }
        },
        {
          "description": "'\nReturn the product of the array elements over the given axis.\nMasked elements are set to 1 internally for computation.\n",
          "id": "sklearn.utils.fixes.MaskedArray.prod",
          "name": "prod",
          "parameters": [
            {
              "description": "Axis over which the product is taken. If None is used, then the product is over all the array elements.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Determines the type of the returned array and of the accumulator where the elements are multiplied. If ``dtype`` has the value ``None`` and the type of a is an integer type of precision less than the default platform integer, then the default platform integer precision is used.  Otherwise, the dtype is the same as that of a.",
              "name": "dtype",
              "optional": "true",
              "type": "None, dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "None, array"
            }
          ],
          "returns": {
            "description": "Returns an array whose shape is the same as a with the specified axis removed. Returns a 0d array when a is 1d or axis=None. Returns a reference to the specified output array if specified.  See Also -------- prod : equivalent function  Notes ----- Arithmetic is modular when using integer types, and no error is raised on overflow.  Examples -------- >>> np.prod([1.,2.]) 2.0 >>> np.prod([1.,2.], dtype=np.int32) 2 >>> np.prod([[1.,2.],[3.,4.]]) 24.0 >>> np.prod([[1.,2.],[3.,4.]], axis=1) array([  2.,  12.])  '",
            "name": "product_along_axis",
            "type": "array, scalar"
          }
        },
        {
          "description": "'\nReturn (maximum - minimum) along the the given dimension\n(i.e. peak-to-peak value).\n",
          "id": "sklearn.utils.fixes.MaskedArray.ptp",
          "name": "ptp",
          "parameters": [
            {
              "description": "Axis along which to find the peaks.  If None (default) the flattened array is used.",
              "name": "axis",
              "optional": "true",
              "type": "None, int"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary.",
              "name": "out",
              "optional": "true",
              "type": "None, array_like"
            },
            {
              "description": "Value used to fill in the masked values. ",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            }
          ],
          "returns": {
            "description": "A new array holding the result, unless ``out`` was specified, in which case a reference to ``out`` is returned.  '",
            "name": "ptp",
            "type": "ndarray"
          }
        },
        {
          "description": "\"\nSet storage-indexed locations to corresponding values.\n\nSets self._data.flat[n] = values[n] for each n in indices.\nIf `values` is shorter than `indices` then it will repeat.\nIf `values` has some masked values, the initial mask is updated\nin consequence, else the corresponding values are unmasked.\n",
          "id": "sklearn.utils.fixes.MaskedArray.put",
          "name": "put",
          "parameters": [
            {
              "description": "Target indices, interpreted as integers.",
              "name": "indices",
              "type": ""
            },
            {
              "description": "Values to place in self._data copy at target indices.",
              "name": "values",
              "type": "array"
            },
            {
              "description": "Specifies how out-of-bounds indices will behave. 'raise' : raise an error. 'wrap' : wrap around. 'clip' : clip to the range.  Notes ----- `values` can be a scalar or length 1 array.  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> x.put([0,4,8],[10,20,30]) >>> print(x) [[10 -- 3] [-- 20 --] [7 -- 30]]  >>> x.put(4,999) >>> print(x) [[10 -- 3] [-- 999 --] [7 -- 30]]  \"",
              "name": "mode",
              "optional": "true",
              "type": "'raise', 'wrap', 'clip'"
            }
          ]
        },
        {
          "description": "\"\nReturns a 1D version of self, as a view.\n",
          "id": "sklearn.utils.fixes.MaskedArray.ravel",
          "name": "ravel",
          "parameters": [
            {
              "description": "The elements of `a` are read using this index order. 'C' means to index the elements in C-like order, with the last axis index changing fastest, back to the first axis index changing slowest. 'F' means to index the elements in Fortran-like index order, with the first index changing fastest, and the last index changing slowest. Note that the 'C' and 'F' options take no account of the memory layout of the underlying array, and only refer to the order of axis indexing.  'A' means to read the elements in Fortran-like index order if `m` is Fortran *contiguous* in memory, C-like order otherwise.  'K' means to read the elements in the order they occur in memory, except for reversing the data when strides are negative. By default, 'C' index order is used. ",
              "name": "order",
              "optional": "true",
              "type": "'C', 'F', 'A', 'K'"
            }
          ],
          "returns": {
            "description": "Output view is of shape ``(self.size,)`` (or ``(np.ma.product(self.shape),)``).  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> print(x.ravel()) [1 -- 3 -- 5 -- 7 -- 9]  \"",
            "name": "MaskedArray"
          }
        },
        {
          "description": "'a.repeat(repeats, axis=None)\n\nRepeat elements of an array.\n\nRefer to `numpy.repeat` for full documentation.\n\nSee Also\n--------\nnumpy.repeat : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.repeat",
          "name": "repeat",
          "parameters": []
        },
        {
          "description": "\"\nGive a new shape to the array without changing its data.\n\nReturns a masked array containing the same data, but with a new shape.\nThe result is a view on the original array; if this is not possible, a\nValueError is raised.\n",
          "id": "sklearn.utils.fixes.MaskedArray.reshape",
          "name": "reshape",
          "parameters": [
            {
              "description": "The new shape should be compatible with the original shape. If an integer is supplied, then the result will be a 1-D array of that length.",
              "name": "shape",
              "type": "int"
            },
            {
              "description": "Determines whether the array data should be viewed as in C (row-major) or FORTRAN (column-major) order. ",
              "name": "order",
              "optional": "true",
              "type": "'C', 'F'"
            }
          ],
          "returns": {
            "description": "A new view on the array.  See Also -------- reshape : Equivalent function in the masked array module. numpy.ndarray.reshape : Equivalent method on ndarray object. numpy.reshape : Equivalent function in the NumPy module.  Notes ----- The reshaping operation cannot guarantee that a copy will not be made, to modify the shape in place, use ``a.shape = s``  Examples -------- >>> x = np.ma.array([[1,2],[3,4]], mask=[1,0,0,1]) >>> print(x) [[-- 2] [3 --]] >>> x = x.reshape((4,1)) >>> print(x) [[--] [2] [3] [--]]  \"",
            "name": "reshaped_array",
            "type": "array"
          }
        },
        {
          "description": "'\n.. warning::\n\nThis method does nothing, except raise a ValueError exception. A\nmasked array does not own its data and therefore cannot safely be\nresized in place. Use the `numpy.ma.resize` function instead.\n\nThis method is difficult to implement safely and may be deprecated in\nfuture releases of NumPy.\n\n'",
          "id": "sklearn.utils.fixes.MaskedArray.resize",
          "name": "resize",
          "parameters": []
        },
        {
          "description": "'a.round(decimals=0, out=None)\n\nReturn `a` with each element rounded to the given number of decimals.\n\nRefer to `numpy.around` for full documentation.\n\nSee Also\n--------\nnumpy.around : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.round",
          "name": "round",
          "parameters": []
        },
        {
          "description": "'\nSet the filling value of the masked array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.set_fill_value",
          "name": "set_fill_value",
          "parameters": [
            {
              "description": "The new filling value. Default is None, in which case a default based on the data type is used.  See Also -------- ma.set_fill_value : Equivalent function.  Examples -------- >>> x = np.ma.array([0, 1.], fill_value=-np.inf) >>> x.fill_value -inf >>> x.set_fill_value(np.pi) >>> x.fill_value 3.1415926535897931  Reset to default:  >>> x.set_fill_value() >>> x.fill_value 1e+20  '",
              "name": "value",
              "optional": "true",
              "type": "scalar"
            }
          ]
        },
        {
          "description": "'\nReduce a mask to nomask when possible.\n",
          "id": "sklearn.utils.fixes.MaskedArray.shrink_mask",
          "name": "shrink_mask",
          "parameters": [],
          "returns": {
            "description": " Examples -------- >>> x = np.ma.array([[1,2 ], [3, 4]], mask=[0]*4) >>> x.mask array([[False, False], [False, False]], dtype=bool) >>> x.shrink_mask() >>> x.mask False  '",
            "name": "None"
          }
        },
        {
          "description": "'\nForce the mask to soft.\n\nWhether the mask of a masked array is hard or soft is determined by\nits `hardmask` property. `soften_mask` sets `hardmask` to False.\n\nSee Also\n--------\nhardmask\n\n'",
          "id": "sklearn.utils.fixes.MaskedArray.soften_mask",
          "name": "soften_mask",
          "parameters": []
        },
        {
          "description": "\"\nSort the array, in-place\n",
          "id": "sklearn.utils.fixes.MaskedArray.sort",
          "name": "sort",
          "parameters": [
            {
              "description": "Array to be sorted.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis along which to sort. If None, the array is flattened before sorting. The default is -1, which sorts along the last axis.",
              "name": "axis",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "Sorting algorithm. Default is 'quicksort'.",
              "name": "kind",
              "optional": "true",
              "type": "'quicksort', 'mergesort', 'heapsort'"
            },
            {
              "description": "When `a` is a structured array, this argument specifies which fields to compare first, second, and so on.  This list does not need to include all of the fields.",
              "name": "order",
              "optional": "true",
              "type": "list"
            },
            {
              "description": "Whether missing values (if any) should be forced in the upper indices (at the end of the array) (True) or lower indices (at the beginning). When the array contains unmasked values of the largest (or smallest if False) representable value of the datatype the ordering of these values and the masked values is undefined.  To enforce the masked values are at the end (beginning) in this case one must sort the mask.",
              "name": "endwith",
              "optional": "true",
              "type": "True, False"
            },
            {
              "description": "Value used internally for the masked values. If ``fill_value`` is not None, it supersedes ``endwith``. ",
              "name": "fill_value",
              "optional": "true",
              "type": "var"
            }
          ],
          "returns": {
            "description": "Array of the same type and shape as `a`.  See Also -------- ndarray.sort : Method to sort an array in-place. argsort : Indirect sort. lexsort : Indirect stable sort on multiple keys. searchsorted : Find elements in a sorted array.  Notes ----- See ``sort`` for notes on the different sorting algorithms.  Examples -------- >>> a = ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0]) >>> # Default >>> a.sort() >>> print(a) [1 3 5 -- --]  >>> a = ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0]) >>> # Put missing values in the front >>> a.sort(endwith=False) >>> print(a) [-- -- 1 3 5]  >>> a = ma.array([1, 2, 5, 4, 3],mask=[0, 1, 0, 1, 0]) >>> # fill_value takes over endwith >>> a.sort(endwith=False, fill_value=3) >>> print(a) [1 -- -- 3 5]  \"",
            "name": "sorted_array",
            "type": "ndarray"
          }
        },
        {
          "description": "'a.squeeze(axis=None)\n\nRemove single-dimensional entries from the shape of `a`.\n\nRefer to `numpy.squeeze` for full documentation.\n\nSee Also\n--------\nnumpy.squeeze : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.squeeze",
          "name": "squeeze",
          "parameters": []
        },
        {
          "description": "'\nCompute the standard deviation along the specified axis.\n\nReturns the standard deviation, a measure of the spread of a distribution,\nof the array elements. The standard deviation is computed for the\nflattened array by default, otherwise over the specified axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.std",
          "name": "std",
          "parameters": [
            {
              "description": "Calculate the standard deviation of these values.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis or axes along which the standard deviation is computed. The default is to compute the standard deviation of the flattened array.  .. versionadded: 1.7.0  If this is a tuple of ints, a standard deviation is performed over multiple axes, instead of a single axis or all the axes as before.",
              "name": "axis",
              "optional": "true",
              "type": ""
            },
            {
              "description": "Type to use in computing the standard deviation. For arrays of integer type the default is float64, for arrays of float types it is the same as the array type.",
              "name": "dtype",
              "optional": "true",
              "type": "dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape as the expected output but the type (of the calculated values) will be cast if necessary.",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            },
            {
              "description": "Means Delta Degrees of Freedom.  The divisor used in calculations is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero.",
              "name": "ddof",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. ",
              "name": "keepdims",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "If `out` is None, return a new array containing the standard deviation, otherwise return a reference to the output array.  See Also -------- var, mean, nanmean, nanstd, nanvar numpy.doc.ufuncs : Section \"Output arguments\"  Notes ----- The standard deviation is the square root of the average of the squared deviations from the mean, i.e., ``std = sqrt(mean(abs(x - x.mean())**2))``.  The average squared deviation is normally calculated as ``x.sum() / N``, where ``N = len(x)``.  If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead. In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of the infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables. The standard deviation computed in this function is the square root of the estimated variance, so even with ``ddof=1``, it will not be an unbiased estimate of the standard deviation per se.  Note that, for complex numbers, `std` takes the absolute value before squaring, so that the result is always real and nonnegative.  For floating-point input, the *std* is computed using the same precision the input has. Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below). Specifying a higher-accuracy accumulator using the `dtype` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.std(a) 1.1180339887498949 >>> np.std(a, axis=0) array([ 1.,  1.]) >>> np.std(a, axis=1) array([ 0.5,  0.5])  In single precision, std() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.std(a) 0.45000005  Computing the standard deviation in float64 is more accurate:  >>> np.std(a, dtype=np.float64) 0.44999999925494177  '",
            "name": "standard_deviation",
            "type": "ndarray"
          }
        },
        {
          "description": "\"\nReturn the sum of the array elements over the given axis.\nMasked elements are set to 0 internally.\n",
          "id": "sklearn.utils.fixes.MaskedArray.sum",
          "name": "sum",
          "parameters": [
            {
              "description": "Axis along which the sum is computed. The default (`axis` = None) is to compute over the flattened array.",
              "name": "axis",
              "optional": "true",
              "type": "None, -1, int"
            },
            {
              "description": "Determines the type of the returned array and of the accumulator where the elements are summed. If dtype has the value None and the type of a is an integer type of precision less than the default platform integer, then the default platform integer precision is used.  Otherwise, the dtype is the same as that of a.",
              "name": "dtype",
              "optional": "true",
              "type": "None, dtype"
            },
            {
              "description": "Alternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. ",
              "name": "out",
              "optional": "true",
              "type": "None, ndarray"
            }
          ],
          "returns": {
            "description": "An array with the same shape as self, with the specified axis removed.   If self is a 0-d array, or if `axis` is None, a scalar is returned.  If an output array is specified, a reference to `out` is returned.  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> print(x.sum()) 25 >>> print(x.sum(axis=1)) [4 5 16] >>> print(x.sum(axis=0)) [8 5 12] >>> print(type(x.sum(axis=0, dtype=np.int64)[0])) <type 'numpy.int64'>  \"",
            "name": "sum_along_axis",
            "type": ""
          }
        },
        {
          "description": "'a.swapaxes(axis1, axis2)\n\nReturn a view of the array with `axis1` and `axis2` interchanged.\n\nRefer to `numpy.swapaxes` for full documentation.\n\nSee Also\n--------\nnumpy.swapaxes : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.swapaxes",
          "name": "swapaxes",
          "parameters": []
        },
        {
          "description": "'\n'",
          "id": "sklearn.utils.fixes.MaskedArray.take",
          "name": "take",
          "parameters": []
        },
        {
          "description": "\"\nReturn the array data as a string containing the raw bytes in the array.\n\nThe array is filled with a fill value before the string conversion.\n\n.. versionadded:: 1.9.0\n",
          "id": "sklearn.utils.fixes.MaskedArray.tobytes",
          "name": "tobytes",
          "parameters": [
            {
              "description": "Value used to fill in the masked values. Deafult is None, in which case `MaskedArray.fill_value` is used.",
              "name": "fill_value",
              "optional": "true",
              "type": "scalar"
            },
            {
              "description": "Order of the data item in the copy. Default is 'C'.  - 'C'   -- C order (row major). - 'F'   -- Fortran order (column major). - 'A'   -- Any, current order of array. - None  -- Same as 'A'.  See Also -------- ndarray.tobytes tolist, tofile  Notes ----- As for `ndarray.tobytes`, information about the shape, dtype, etc., but also about `fill_value`, will be lost.  Examples -------- >>> x = np.ma.array(np.array([[1, 2], [3, 4]]), mask=[[0, 1], [1, 0]]) >>> x.tobytes() '\\\\x01\\\\x00\\\\x00\\\\x00?B\\\\x0f\\\\x00?B\\\\x0f\\\\x00\\\\x04\\\\x00\\\\x00\\\\x00'  \"",
              "name": "order",
              "optional": "true",
              "type": "'C','F','A'"
            }
          ]
        },
        {
          "description": "'\nSave a masked array to a file in binary format.\n\n.. warning::\nThis function is not implemented yet.\n\nRaises\n------\nNotImplementedError\nWhen `tofile` is called.\n\n'",
          "id": "sklearn.utils.fixes.MaskedArray.tofile",
          "name": "tofile",
          "parameters": []
        },
        {
          "description": "'\nTransforms a masked array into a flexible-type array.\n\nThe flexible type array that is returned will have two fields:\n\n* the ``_data`` field stores the ``_data`` part of the array.\n* the ``_mask`` field stores the ``_mask`` part of the array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.toflex",
          "name": "toflex",
          "parameters": [],
          "returns": {
            "description": "A new flexible-type `ndarray` with two fields: the first element containing a value, the second element containing the corresponding mask boolean. The returned record shape matches self.shape.  Notes ----- A side-effect of transforming a masked array into a flexible `ndarray` is that meta information (``fill_value``, ...) will be lost.  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> print(x.toflex()) [[(1, False) (2, True) (3, False)] [(4, True) (5, False) (6, True)] [(7, False) (8, True) (9, False)]]  '",
            "name": "record",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nReturn the data portion of the masked array as a hierarchical Python list.\n\nData items are converted to the nearest compatible Python type.\nMasked values are converted to `fill_value`. If `fill_value` is None,\nthe corresponding entries in the output list will be ``None``.\n",
          "id": "sklearn.utils.fixes.MaskedArray.tolist",
          "name": "tolist",
          "parameters": [
            {
              "description": "The value to use for invalid entries. Default is None. ",
              "name": "fill_value",
              "optional": "true",
              "type": "scalar"
            }
          ],
          "returns": {
            "description": "The Python list representation of the masked array.  Examples -------- >>> x = np.ma.array([[1,2,3], [4,5,6], [7,8,9]], mask=[0] + [1,0]*4) >>> x.tolist() [[1, None, 3], [None, 5, None], [7, None, 9]] >>> x.tolist(-999) [[1, -999, 3], [-999, 5, -999], [7, -999, 9]]  '",
            "name": "result",
            "type": "list"
          }
        },
        {
          "description": "'\nTransforms a masked array into a flexible-type array.\n\nThe flexible type array that is returned will have two fields:\n\n* the ``_data`` field stores the ``_data`` part of the array.\n* the ``_mask`` field stores the ``_mask`` part of the array.\n",
          "id": "sklearn.utils.fixes.MaskedArray.toflex",
          "name": "toflex",
          "parameters": [],
          "returns": {
            "description": "A new flexible-type `ndarray` with two fields: the first element containing a value, the second element containing the corresponding mask boolean. The returned record shape matches self.shape.  Notes ----- A side-effect of transforming a masked array into a flexible `ndarray` is that meta information (``fill_value``, ...) will be lost.  Examples -------- >>> x = np.ma.array([[1,2,3],[4,5,6],[7,8,9]], mask=[0] + [1,0]*4) >>> print(x) [[1 -- 3] [-- 5 --] [7 -- 9]] >>> print(x.toflex()) [[(1, False) (2, True) (3, False)] [(4, True) (5, False) (6, True)] [(7, False) (8, True) (9, False)]]  '",
            "name": "record",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nThis function is a compatibility alias for tobytes. Despite its name it\nreturns bytes not strings.\n'",
          "id": "sklearn.utils.fixes.MaskedArray.tostring",
          "name": "tostring",
          "parameters": []
        },
        {
          "description": "'a.trace(offset=0, axis1=0, axis2=1, dtype=None, out=None)\n\nReturn the sum along diagonals of the array.\n\nRefer to `numpy.trace` for full documentation.\n\nSee Also\n--------\nnumpy.trace : equivalent function'",
          "id": "sklearn.utils.fixes.MaskedArray.trace",
          "name": "trace",
          "parameters": []
        },
        {
          "description": "'a.transpose(*axes)\n\nReturns a view of the array with axes transposed.\n\nFor a 1-D array, this has no effect. (To change between column and\nrow vectors, first cast the 1-D array into a matrix object.)\nFor a 2-D array, this is the usual matrix transpose.\nFor an n-D array, if axes are given, their order indicates how the\naxes are permuted (see Examples). If axes are not provided and\n``a.shape = (i[0], i[1], ... i[n-2], i[n-1])``, then\n``a.transpose().shape = (i[n-1], i[n-2], ... i[1], i[0])``.\n",
          "id": "sklearn.utils.fixes.MaskedArray.transpose",
          "name": "transpose",
          "parameters": [
            {
              "description": " * None or no argument: reverses the order of the axes.  * tuple of ints: `i` in the `j`-th place in the tuple means `a`\\'s `i`-th axis becomes `a.transpose()`\\'s `j`-th axis.  * `n` ints: same as an n-tuple of the same ints (this form is intended simply as a \"convenience\" alternative to the tuple form) ",
              "name": "axes",
              "type": ""
            }
          ],
          "returns": {
            "description": "View of `a`, with axes suitably permuted.  See Also -------- ndarray.T : Array property returning the array transposed.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> a array([[1, 2], [3, 4]]) >>> a.transpose() array([[1, 3], [2, 4]]) >>> a.transpose((1, 0)) array([[1, 3], [2, 4]]) >>> a.transpose(1, 0) array([[1, 3], [2, 4]])'",
            "name": "out",
            "type": "ndarray"
          }
        },
        {
          "description": "'\nCopy the mask and set the sharedmask flag to False.\n\nWhether the mask is shared between masked arrays can be seen from\nthe `sharedmask` property. `unshare_mask` ensures the mask is not shared.\nA copy of the mask is only made if it was shared.\n\nSee Also\n--------\nsharedmask\n\n'",
          "id": "sklearn.utils.fixes.MaskedArray.unshare_mask",
          "name": "unshare_mask",
          "parameters": []
        },
        {
          "description": "'\nCompute the variance along the specified axis.\n\nReturns the variance of the array elements, a measure of the spread of a\ndistribution.  The variance is computed for the flattened array by\ndefault, otherwise over the specified axis.\n",
          "id": "sklearn.utils.fixes.MaskedArray.var",
          "name": "var",
          "parameters": [
            {
              "description": "Array containing numbers whose variance is desired.  If `a` is not an array, a conversion is attempted.",
              "name": "a",
              "type": "array"
            },
            {
              "description": "Axis or axes along which the variance is computed.  The default is to compute the variance of the flattened array.  .. versionadded: 1.7.0  If this is a tuple of ints, a variance is performed over multiple axes, instead of a single axis or all the axes as before.",
              "name": "axis",
              "optional": "true",
              "type": ""
            },
            {
              "description": "Type to use in computing the variance.  For arrays of integer type the default is `float32`; for arrays of float types it is the same as the array type.",
              "name": "dtype",
              "optional": "true",
              "type": "data-type"
            },
            {
              "description": "Alternate output array in which to place the result.  It must have the same shape as the expected output, but the type is cast if necessary.",
              "name": "out",
              "optional": "true",
              "type": "ndarray"
            },
            {
              "description": "\"Delta Degrees of Freedom\": the divisor used in the calculation is ``N - ddof``, where ``N`` represents the number of elements. By default `ddof` is zero.",
              "name": "ddof",
              "optional": "true",
              "type": "int"
            },
            {
              "description": "If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the original `arr`. ",
              "name": "keepdims",
              "optional": "true",
              "type": "bool"
            }
          ],
          "returns": {
            "description": "If ``out=None``, returns a new array containing the variance; otherwise, a reference to the output array is returned.  See Also -------- std , mean, nanmean, nanstd, nanvar numpy.doc.ufuncs : Section \"Output arguments\"  Notes ----- The variance is the average of the squared deviations from the mean, i.e.,  ``var = mean(abs(x - x.mean())**2)``.  The mean is normally calculated as ``x.sum() / N``, where ``N = len(x)``. If, however, `ddof` is specified, the divisor ``N - ddof`` is used instead.  In standard statistical practice, ``ddof=1`` provides an unbiased estimator of the variance of a hypothetical infinite population. ``ddof=0`` provides a maximum likelihood estimate of the variance for normally distributed variables.  Note that for complex numbers, the absolute value is taken before squaring, so that the result is always real and nonnegative.  For floating-point input, the variance is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for `float32` (see example below).  Specifying a higher-accuracy accumulator using the ``dtype`` keyword can alleviate this issue.  Examples -------- >>> a = np.array([[1, 2], [3, 4]]) >>> np.var(a) 1.25 >>> np.var(a, axis=0) array([ 1.,  1.]) >>> np.var(a, axis=1) array([ 0.25,  0.25])  In single precision, var() can be inaccurate:  >>> a = np.zeros((2, 512*512), dtype=np.float32) >>> a[0, :] = 1.0 >>> a[1, :] = 0.1 >>> np.var(a) 0.20250003  Computing the variance in float64 is more accurate:  >>> np.var(a, dtype=np.float64) 0.20249999932944759 >>> ((1-0.55)**2 + (0.1-0.55)**2)/2 0.2025  '",
            "name": "variance",
            "type": "ndarray"
          }
        },
        {
          "description": "'a.view(dtype=None, type=None)\n\nNew view of array with the same data.\n",
          "id": "sklearn.utils.fixes.MaskedArray.view",
          "name": "view",
          "parameters": [
            {
              "description": "Data-type descriptor of the returned view, e.g., float32 or int16. The default, None, results in the view having the same data-type as `a`. This argument can also be specified as an ndarray sub-class, which then specifies the type of the returned object (this is equivalent to setting the ``type`` parameter).",
              "name": "dtype",
              "optional": "true",
              "type": "data-type"
            },
            {
              "description": "Type of the returned view, e.g., ndarray or matrix.  Again, the default None results in type preservation.  Notes ----- ``a.view()`` is used two different ways:  ``a.view(some_dtype)`` or ``a.view(dtype=some_dtype)`` constructs a view of the array\\'s memory with a different data-type.  This can cause a reinterpretation of the bytes of memory.  ``a.view(ndarray_subclass)`` or ``a.view(type=ndarray_subclass)`` just returns an instance of `ndarray_subclass` that looks at the same array (same shape, dtype, etc.)  This does not cause a reinterpretation of the memory.  For ``a.view(some_dtype)``, if ``some_dtype`` has a different number of bytes per entry than the previous dtype (for example, converting a regular array to a structured array), then the behavior of the view cannot be predicted just from the superficial appearance of ``a`` (shown by ``print(a)``). It also depends on exactly how ``a`` is stored in memory. Therefore if ``a`` is C-ordered versus fortran-ordered, versus defined as a slice or transpose, etc., the view may give different results.   Examples -------- >>> x = np.array([(1, 2)], dtype=[(\\'a\\', np.int8), (\\'b\\', np.int8)])  Viewing array data using a different type and dtype:  >>> y = x.view(dtype=np.int16, type=np.matrix) >>> y matrix([[513]], dtype=int16) >>> print(type(y)) <class \\'numpy.matrixlib.defmatrix.matrix\\'>  Creating a view on a structured array so it can be used in calculations  >>> x = np.array([(1, 2),(3,4)], dtype=[(\\'a\\', np.int8), (\\'b\\', np.int8)]) >>> xv = x.view(dtype=np.int8).reshape(-1,2) >>> xv array([[1, 2], [3, 4]], dtype=int8) >>> xv.mean(0) array([ 2.,  3.])  Making changes to the view changes the underlying array  >>> xv[0,1] = 20 >>> print(x) [(1, 20) (3, 4)]  Using a view to convert an array to a recarray:  >>> z = x.view(np.recarray) >>> z.a array([1], dtype=int8)  Views share data:  >>> x[0] = (9, 10) >>> z[0] (9, 10)  Views that change the dtype size (bytes per entry) should normally be avoided on arrays defined by slices, transposes, fortran-ordering, etc.:  >>> x = np.array([[1,2,3],[4,5,6]], dtype=np.int16) >>> y = x[:, 0:2] >>> y array([[1, 2], [4, 5]], dtype=int16) >>> y.view(dtype=[(\\'width\\', np.int16), (\\'length\\', np.int16)]) Traceback (most recent call last): File \"<stdin>\", line 1, in <module> ValueError: new type not compatible with array. >>> z = y.copy() >>> z.view(dtype=[(\\'width\\', np.int16), (\\'length\\', np.int16)]) array([[(1, 2)], [(4, 5)]], dtype=[(\\'width\\', \\'<i2\\'), (\\'length\\', \\'<i2\\')])'",
              "name": "type",
              "optional": "true",
              "type": ""
            }
          ]
        }
      ],
      "name": "sklearn.utils.fixes.MaskedArray",
      "parameters": [],
      "source_code": "/Users/shah/anaconda/lib/python2.7/site-packages/sklearn/utils/fixes.pyc:407",
      "tags": [
        "utils",
        "fixes"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "misc"
      ],
      "attributes": [ ],
      "category": "feature_extraction.text",
      "common_name": "CorexText",
      "description": "",
      "id": "corex_text.CorexText",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "corex",
      "methods_available": [],
      "name": "corex_text.CorexText",
      "parameters": [],
      "source_code": "",
      "tags": [
        "feature_extraction",
        "text"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "misc"
      ],
      "attributes": [ ],
      "category": "feature_extraction.image",
      "common_name": "ResNet50ImageFeature",
      "description": "",
      "id": "dsbox.custom_primitives.feature.net_image_feature.ResNet50ImageFeature",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "image",
      "methods_available": [],
      "name": "dsbox.custom_primitives.feature.net_image_feature.ResNet50ImageFeature",
      "parameters": [],
      "source_code": "",
      "tags": [
        "feature_extraction",
        "image"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    },
    {
      "algorithm_type": [
        "misc"
      ],
      "attributes": [ ],
      "category": "feature_extraction.image",
      "common_name": "Vgg16ImageFeature",
      "description": "",
      "id": "dsbox.custom_primitives.feature.net_image_feature.Vgg16ImageFeature",
      "is_class": true,
      "language": "python",
      "learning_type": [
        "supervised"
      ],
      "library": "image",
      "methods_available": [],
      "name": "dsbox.custom_primitives.feature.net_image_feature.Vgg16ImageFeature",
      "parameters": [],
      "source_code": "",
      "tags": [
        "feature_extraction",
        "image"
      ],
      "task_type": [
        "modeling"
      ],
      "version": "0.18.1"
    }
  ]
}
